---
layout: distill
title: Independent Component Analysis
date: 2025-10-08 11:59:00 +0000
description: Cocktail parties make me anxious
tags:
  - linear_algebra
  - statistics
  - machine_learning
giscus_comments: true
related_posts: false
---

Among spies, hackers and tabloids, **independent component analysis (ICA)** immediately brings to mind cocktail parties. That's because ICA solves a famous problem in signal processing: the cocktail party problem.

{% include figure.liquid loading="eager" path="assets/img/posts/2025-09-22-ica/cocktail_party.webp" class="img-fluid" %}

The problem reads as follows. A spy wants to snoop on all the conversations happening in a cocktail party. To that end, they strategically place multiple microphones across the room.

{% include figure.liquid loading="eager" path="assets/python/2025-09-22-ica/img/cocktail_source_signals.webp" class="img-fluid" %}

However, the party is quite busy: many conversations are ongoing, a piano plays in the background, guests gulp, waiters trot, glasses clink. Each microphone only captures a mixture of all these signals, which results in mostly incomprehensible gibberish.

{% include figure.liquid loading="eager" path="assets/python/2025-09-22-ica/img/cocktail_microphone_signals.webp" class="img-fluid" %}

The spy, thoroughly motivated to keep the spying going, invents ICA and succesfully recovers the original signals.

{% include figure.liquid loading="eager" path="assets/python/2025-09-22-ica/img/cocktail_ica_signals.webp" class="img-fluid" %}

# The geometry of ICA

More generally, when dealing with multidimensional datasets, we often observe that many variables are somewhat redundant. A common assumption is that a relatively small number of latent factors explain the data, which then manifests as the variables that we actually measure. Hence, simplifying multidimensional datasets into a reduced number of factors often helps us understand the data better.

One approach to this problem is **matrix factorization**, which aims to decompose our original matrix $$X$$ into a product of matrices. Often the matrix is decomposed into two matrices: $$X = A S$$. $$A$$, the mixing matrix, represents a new vector basis. Since it's a good descriptor of the geometry of the data, studying $$A$$ can teach us something about it. $$S$$, the score matrix, represents the coordinates of each datapoint in that new basis.

{% include figure.liquid path="assets/img/posts/2025-09-22-ica/change_of_basis.webp" class="img-fluid" %}

<div class="caption">
    Illustration of a change of basis. The new basis facilitates identifying which points belong to which subgroup. Note that the underlying data remains unchanged.
</div>

As you would have guessed, what "better capturing the geometry of the data" means exactly is up for debate. But, in general, it involves improvements on the interpretability, data compression, or parsimony. Unsurprisingly, many solutions have been proposed to this problem. A well-known one is the principal component analysis, or PCA. Geometrically speaking, PCA fits an ellipsoid to the data, and uses the axes of this ellipsoid as the basis for the new space. This is equivalent to PCA's traditional interpretation: capturing the main axes of variation in the data. However, when our data has substructures that are not captured by the variance, PCA performs pretty poorly.

{% include figure.liquid path="assets/img/posts/2025-09-22-ica/pca_overview.webp" class="img-fluid" %}

<div class="caption">
    Illustration of the principal component analysis on well-behaved data (A) and on data with substructures (B). Note that in the latter case, the new basis is not particularly well-suited to represent the data.
</div>

This is where ICA shines.

{% include figure.liquid loading="eager" path="assets/python/2025-09-22-ica/img/ica_pca_sklearn_example.webp" class="img-fluid" %}

<div class="caption">
    ICA and PCA applied to a synthetic dataset. The underlying, independent signals (A) are combined into the observed signal (B) as \(x = c_1 + c_2\), \(y = 2 c_2\). While ICA finds the best basis (B) and recovers the true components (C), PCA cannot (D). Adapted from the <a href="https://scikit-learn.org/stable/auto_examples/decomposition/plot_ica_vs_pca.html#sphx-glr-auto-examples-decomposition-plot-ica-vs-pca-py">scikit-learn documentation</a>.
</div>

# How does ICA work?

PCA seeks a basis of uncorrelated components, since the axes of the ellipsoid need to be orthogonal. ICA goes one step further: it seeks a **statistically independent** basis.

More formally, ICA assumes the observed data is a mixture of the individual components. For instance, in the cocktail party, each component (e.g., the piano playing) is a vector indexed by time ($$s_j(t)$$). The components must meet two properties: _non-Gaussianity_ and _statistical independence_. There is also an assumption that they are combined _linearly_ to generate the observed data:

$$
x_i(t) = \sum_j a_{ij} s_j(t)
$$

where $$a_{ij}$$ is a constant, representing the relative contribution of each component. This linearity is key: as per the central limit theorem, the sum of independent random variables converges in distribution to a Gaussian. And here is where the magic lies: our signal needs to be _more_ Gaussian than any of the individual components, and we will unmix them by seeking highly non-Gaussian components.

We can arrange the observation vectors into a matrix, and think of it as a matrix factorization problem:

$$
X = AS
$$

where $$A$$ is called the mixing matrix and $$S$$ the independent component matrix. The goal is to estimate both $$A$$ and $$S$$ from $$X$$. To that end, ICA seeks a de-mixing matrix $$W$$ such that:

$$
S = W X
$$

Specifically, ICA's goal is to find the demixing matrix $$W$$ that maximizes the non-Gaussianity of the recovered signals in $$S$$.

{% details Quantifying non-Gaussianity %}

There are two popular ways of quantifying the non-Gaussianity of a distribution.

The first way consists in comparing the properties of the distribution called the [**moments**](https://gregorygundersen.com/blog/2020/04/11/moments/). The first two moments, the mean and the variance, are not useful, since the data is usually standardized to have 0 mean and 1 variance. But the third and fourth moments (skewness and kurtosis, respectively) are frequently used:

- The **skewness** measures the asymmetry of the distribution. Gaussian distributions are famously symmetrical, and have a skewness of 0.

  {% include figure.liquid loading="eager" path="assets/img/posts/2025-09-22-ica/skewness.webp" class="img-fluid" %}

- The **kurtosis** measures the tailedness of the distribution. By comparing the kurtosis of our distribution relative to that of a normal distribution (_excess kurtosis_), we can estimate how non-Gaussian it is. The kurtosis is popular because it can be easily estimated from the data.

  {% include figure.liquid loading="eager" path="assets/img/posts/2025-09-22-ica/kurtosis.webp" class="img-fluid" %}

The second family involves using the **entropy** of the distribution. A fundamental theorem in information theory states that a gaussian variable has the largest entropy among all random variables of equal variance. By computing the difference between the entropy of our distribution and that of a gaussian distribution of equal variance, we can estimate how non-Gaussian it is. This difference is called the **negentropy**.

{% enddetails %}

## Implementation

Let's put the above into practice by aiming for the simplest, possibly most inefficient, implementation, seeking the first two independent components. We will work on an $$m \times n$$ matrix $$X$$, where $$m$$ is the number of features and $$n$$ the number of samples. Before we can apply ICA, we need to apply two preprocessing steps to $$X$$:

- **Centering**, ensuring that each feature has a zero mean
- **Whitening**, ensuring that the features are uncorrelated and have unit variance

```python
X_centered = X - np.mean(X, axis=0)
pca = PCA(whiten=True)
X_whitened = pca.fit_transform(X_centered)
```

After preprocessing, we will seek an $$m$$-dimensional vector $$\mathbf{w}_1$$ that maximizes the non-Gaussianity of the independent component $$\mathbf{s}_{(1)} = X \mathbf{w}_1$$. We will quantify non-Gaussianity using the excess kurtosis:

$$
\operatorname{Kurtosis}(Y) = E\left[\left(\frac{Y - \mu}{\sigma}\right)^4\right] - 3.
$$

Since our data is centered and whitened, this simplifies to $$\operatorname{Kurtosis}(Y) = E\left[\left(Y\right)^4\right] - 3$$.

A biased sample estimator of excess kurtosis is:

$$
\hat{K}(\mathbf{s}_{(1)}) = \frac 1 n \sum_{i=1}^n s_{(1)i}^4 - 3  = \frac 1 n \sum_{i=1}^n (\mathbf{x_{i*}}^\intercal \mathbf{w})^4 - 3.
$$

We will maximize the kurtosis by gradient ascent. Hence, we need the first derivative of the kurtosis:

$$
\frac { d \hat{K}(\mathbf{s}_{(1)}) } { d \mathbf{w}_1 } = \frac 1 n \sum_{i=1}^n 4 (\mathbf{x_{i*}}^\intercal \mathbf{w})^3 \mathbf{x_{i*}} = \frac 4 n \sum_{i=1}^n s_{(1)i}^3 \mathbf{x_{i*}}.
$$

Let's implement this in Python:

```python
STEP_SIZE = 1e-3
N_ITERATIONS = 50

# n = number of samples, m = number of features
n, m = X_whitened.shape

# Random initial weight vector
w1 = rng.rand(m)
w1 /= np.linalg.norm(w1) + 1e-10

for i in range(N_ITERATIONS):
    # Project data onto weight vector
    s = np.dot(X_whitened, w1)

    # Compute the gradient
    gradient = 4 / n * np.dot(np.pow(s, 3), X_whitened)

    # Update the weight vector
    w1 += STEP_SIZE * gradient

    # Normalize the weight vector
    w1 /= np.linalg.norm(w1) + 1e-10
```

The algorithm converges pretty quickly:

{% include figure.liquid loading="eager" path="assets/python/2025-09-22-ica/img/ica_kurtosis_gd_component_1.gif" class="img-fluid" %}

Before finding the second component, we need to ensure that it is statistically independent from the first component. We can achieve this through a **deflation** step, which removes the influence of already-found components from the data before searching for the next component.

```python
X_deflated = X_whitened - np.outer(np.dot(X_whitened, w1), w1)
```

Then, we repeat the gradient ascent to find the second component:

```python
# Random initial weight vector
w2 = rng.rand(m)
w2 /= np.linalg.norm(w2) + 1e-10

# Repeat gradient ascent to find second component
for i in range(N_ITERATIONS):
    s = np.dot(X_deflated, w2)
    gradient = 4 / n * np.dot(np.pow(s, 3), X_deflated)
    w2 += STEP_SIZE * gradient
    w2 /= np.linalg.norm(w2) + 1e-10
```

{% include figure.liquid loading="eager" path="assets/python/2025-09-22-ica/img/ica_kurtosis_gd_component_2.gif" class="img-fluid" %}

If we wanted to keep going, we would repeat the deflation step to remove the influence of both the first and second components from the data, and then run gradient ascent again to find the third component, and so on.

> Our clunky ICA gets the job done. Sort of. But it's definitely not production-ready. The results are clearly not optimal, as we can see by comparing them with the results obtained above. If you want to play with ICA, [FastICA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html) is the way to go. It offers multiple improvements over my toy implementation, including a fast optimization process (fixed-point algorithm), solves all the components at once, and will even whiten the data for you.

# Application: gene expression analysis

Ultimately, the reason I went down the ICA rabbit hole was not to eavesdropping on parties, but because of its applications in computational biology. ICA is [one of the best approaches to disentangle complex gene expression signals](https://www.nature.com/articles/s41467-018-03424-4). Specifically, ICA is used to factorize gene expression matrices into a matrix containing groups of genes that work together ($$A$$, the _metagenes_) and a matrix containing groups of similar samples ($$S$$, the _metasamples_). Studying the pathways and the properties of the genes within each metagene can give us insights about the biology. Studying commonalities within a metasample can inform about clinical subtypes or link them to other phenotypes.
