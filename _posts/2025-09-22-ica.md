---
layout: distill
title: Independent Component Analysis
date: 2025-09-22 11:59:00 +0000
description: Cocktail parties make me anxious
tags:
  - linear_algebra
  - statistics
  - machine_learning
giscus_comments: true
related_posts: false
---

When dealing with multidimensional datasets, we often observe that many variables are somewhat redundant. A common assumption is that a relatively small number of latent factors explain the data, which then manifest themselves into the variables that we actually measure. Hence, simplifying multidimensional datasets into a reduced number of factors often help us understand the data better.

One approach to this problem are **matrix factorization**, which aim to decompose our original matrix $$X$$ into a product of matrices. Often the matrix is decomposed into two matrices: $$X = W S$$. $$W$$, the weighting matrix, represents the new basis vectors. Since it's a good descriptor of the geometry of the data, studying $$W$$ can teach us something about it. $$S$$, the score matrix, represents the coordinates of each datapoint in that new basis.

{% include figure.liquid path="assets/img/posts/2025-09-22-ica/change_of_basis.webp" class="img-fluid" %}

<div class="caption">
    Illustration of a change of basis. The new basis facilitates identifying which point belong to which subgroup. Note that the underlying data remains unchanged.
</div>

As you would have guessed, what "better capturing the geometry of the data" means exactly is up for debate. But, in general, it involves helping with interpretability, data compression, or parsimony. Unsurprisingly, many solutions have been proposed to this problem. A well-known one is the principal component analysis, or PCA. Geometrically speaking, PCA fits an ellipsoid to the data, and uses the axis of this ellipsoid as the basis for the new space. This is equivalent to PCA's traditional interpretation: capturing the main axes of variation on the data. However, PCA doesn't account for potential substructures of the data. When these are present, if will fail miserably:

{% include figure.liquid path="assets/img/posts/2025-09-22-ica/pca_overview.webp" class="img-fluid" %}

<div class="caption">
    Illustration of the principal component analysis on well-behaved data (A) and on data with substructures (B). Note that in the latter case, the new basis is not particularly well-suited to represent the data.
</div>

This is the case in which **independent component analysis (ICA)** shines.

{% include figure.liquid loading="eager" path="assets/python/2025-09-22-ica/img/ica_pca_sklearn_example.webp" class="img-fluid" %}

<div class="caption">
    ICA and PCA applied to a synthetic dataset. The underlying, independent signals (A) are combined into the observed signal (B) as \(x = c_1 + c_2\), \(y = 2 c_2\). While ICA finds the best basis (B) and recovers the true components (C), PCA cannot (D). Adapted from the <a href="https://scikit-learn.org/stable/auto_examples/decomposition/plot_ica_vs_pca.html#sphx-glr-auto-examples-decomposition-plot-ica-vs-pca-py">scikit-learn documentation</a>.
</div>

{% details The cocktail party problem %}

The ICA is the paradigmatic solution to the cocktail party problem, a famous problem in signal processing. And also, I suspect, during the Cold War.

{% include figure.liquid loading="eager" path="assets/img/posts/2025-09-22-ica/cocktail_party.webp" class="img-fluid" %}

The problem reads as follows. A spy wants to snoop on all the conversations happening in a cocktail party.To that end, they strategically place multiple microphones across the room. However, the party is quite busy: many conversations are ongoing, a piano plays in the background, guests gulp, waiters trot, glasses clink.

{% include figure.liquid loading="eager" path="assets/python/2025-09-22-ica/img/cocktail_source_signals.webp" class="img-fluid" %}

Each microphone only captures a mixture of all these signals, which results in mostly incomprehensible gibberish.

{% include figure.liquid loading="eager" path="assets/python/2025-09-22-ica/img/cocktail_microphone_signals.webp" class="img-fluid" %}

The spy, thoroughly motivated to separate each of the signals into its individual components, invents ICA.

{% include figure.liquid loading="eager" path="assets/python/2025-09-22-ica/img/cocktail_ica_signals.webp" class="img-fluid" %}

{% enddetails %}

# How does ICA work?

PCA seeks an uncorrelated basis, since the axes of the ellipsoid need to be orthogonal. ICA goes one step further: it seeks a **statistically independent** basis.

More formally, ICA assumes the observed data is mixture of the individual components (e.g., the piano playing in the cocktail party). Each independent component $$j$$ is a vector indexed by time ($$s_j(t)$$). They must meet two properties: _non-gaussianity_ and _statistically independence_. There is also the assumption that they are combined _linearly_ to generate the observed data:

$$
x_i(t) = \sum_j a_{ij} s_j(t)
$$

where $$a_{ij}$$ is a constant, representing the relative contribution of each component. And this is key: as per the central limit theorem, the sum of independent random variables converges in distribution to a Gaussian.

We can arrange the observation vectors into a matrix, and think of it as a matrix factorization problem:

$$
X = AS
$$

where $$A$$ is called the mixing matrix and $$S$$ the independent component matrix. The goal is to estimate both $$A$$ and $$S$$ from $$X$$. To that end, ICA seeks a de-mixing matrix $$W$$ such that:

$$
S = W X
$$

Specifically, ICA will seek the $$W$$ that minimizes the gaussianity of each column of $$S$$.

{% details Quantifying non-Gaussianity %}

There are two popular ways of quantifying the non-Gaussianity of a distribution.

The first way consists in comparing the properties of the distribution called the [**moments**](https://gregorygundersen.com/blog/2020/04/11/moments/). The first two moments, the mean and the variance, are not useful, since the data is usually standardized to have 0 mean and 1 variance. But the third and the forth moments (skewness and kurtosis, respectively) are frequently used:

- The **skewness** measures the asymmetry of the distribution. Gaussian distributions are famously symmetrical, and have a skewness of 0.

    {% include figure.liquid loading="eager" path="assets/img/posts/2025-09-22-ica/skewness.webp" class="img-fluid" %}

- The **kurtosis** measures the tailedness of the distribution. By comparing the kurtosis of our distribution relative to that of a normal distribution (_excess kurtosis_), we can estimate how non-Gaussian it is. The kurtosis is popular because it can be easily estimated from the data.

    {% include figure.liquid loading="eager" path="assets/img/posts/2025-09-22-ica/kurtosis.webp" class="img-fluid" %}

The second family involves using the **entropy** of the distribution. A fundamental theorem in information theory states that a gaussian variable has the largest entropy among all random variables of equal variance.

{% enddetails %}

## Implementation: FastICA

[FastICA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html) is one of the most popular algorithms to conduct ICA. Steps:

1. Define data dimensionality *k*.
2. Whitening of the data: remove all gaussian signals. The variance in all dimensions become unity. We are assuming that everything that is Gaussian is noise of some kind
3. ...

We rank the independent components according to their consistency (apparently there is a random step?).

# Case: gene expression analysis

X is an approximation of the data.

X = AS

A table of metagenes.
S table of metasamples.

Metagene interpretation: geneset enrichment analysis, hypergeometric test, correlations with properties of the genes...
Metasample interpretation: look at data of the patients e.g. match known cancer subtypes, study other tests on the same patient e.g. histopathological samples (e.g. rank them by the weight and checking if there is any trend).
