---
layout: post
title: SHAP values
date: 2025-04-01 11:59:00-0000
description: A model-agnostic explainable framework
tags: feature_selection machine_learning feature_importance
giscus_comments: true
related_posts: false
toc:
  sidebar: left
---

SHAP values are a model-agnostic method to quantify the contribution of any given feature to the output. They provide both local and global interpretations.

# Shapley values

SHAP values have their roots in game theory, specifically in the context of **Shapley** values. Imagine a group of players collaborate to achieve an payout. The Shapley value is a method to find out how to fairly distribute the total earnings among the players. Or the blame, if the payout was negative!

A core concept of Shapley values are **coalitions**: given $$n$$ players, a coalition is a subset of the players that can collaborate to achieve the payout. Another concept is the **characteristic function** $$v: 2^n \rightarrow R $$ which returns the total payout for any given coalition (its *worth*). The last concept is the Shapley value itself, the amount $$ \phi_i $$ that player $$i$$ is given. It is computed as the average difference in the value when $$i$$ is added to all possible coalitions that do not include it. Or more formally:

$$
\phi_i(v) = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|!\; (n-|S|-1)!}{n!} (v(S\cup\{i\})-v(S))
$$

where $$S$$ is a subset of the players.

We are now equipped to describe four properties of Shapley values: efficiency, symmetry, dummy player and additivity. Notice how they all relate to _fair_ attribution.

## Efficiency

The grand coalition is the coalition of all players. Efficiency means that the sum of all Shapley values equals the value of the grand coalition, i.e.,

$$
\sum_i \phi_i(v) = v(N)
$$

In other words, the payout has to be distributed among all players.

## Symmetry

Two players $$i$$ and $$j$$ contribute equally if, for any $$S$$, $$v(S \cup \{i\}) = v(S \cup \{j\})$$. Symmetry implies that players that contribute equally receive the same Shapley value.

## Dummy player

If a player does not change the value of any coalition they join (_dummy_), their Shapley value is 0.

## Additivity

If the payout of two games with different characteristic functions, $$u$$ and $$v$$, are combined, the total gains can be linearly decomposed:

$$
\phi_i(u+v) = \phi_i(u) + \phi_i(v)
$$

# SHAP values

SHAP values are a way of quantifying the contribution of a feature to a model's prediction. SHAP stands for SHapley Additive exPlanations, which shows their roots on Shapley values. Let's start by recasting the Shapley value problem transposing some concepts:

- The players are _model features_
- The game is the _model_
- The payout is the _prediction_

With these modifications, the Shapley value looks as follows:

$$
\phi_i(f, x) = \sum_{S \subseteq F \ {i}} \frac{|S|!\; (n-|S|-1)!}{n!} (f(x_{S\cup\{i\}})-f(x_S))
$$

where $$f$$ is our model; $$x$$ is the input datapoint; $$F$$ is the set of all features; and $$n$$ is the total number of features.

> **Note**: Shapley values provide **local** explanations, i.e., the contribution of each feature to a particular prediction. However global explanations can also be computed, by aggregating the local explanations over the whole dataset.

However most machine learning models cannot deal with missing data. This raises the question, what does it mean to "remove" a feature? SHAP solves this by sampling from all the values for that feature available in the dataset. This ensures that the impact of that feature is averaged out over all possible coalitions.

This naive implementation is also very computationally intensive. Exploring all possible coalitions is equivalent to exploring all $$2^n$$ subsets of features. Kernel SHAP approximates the solution by sampling.

<!--
Limitations & Caveats:

    Sensitivity to Feature Correlations: Discuss how SHAP values might behave when features are highly correlated. This is a hot topic in model interpretability, as collinearity can distort attributions.

    Computational Complexity: While you mention the naive approach is intensive, you might add a brief discussion on when approximation methods (like Kernel SHAP and Tree SHAP) should be preferred and any trade-offs associated with them.

Comparison with Other Methods:

    Adding a short section comparing SHAP values with other interpretability methods like LIME or Integrated Gradients might help readers understand the broader landscape of explainable AI tools.

Use Cases and Practical Considerations:

    Consider including a section that outlines common applications of SHAP values in model debugging, feature selection, and fairness assessments.

    A small example or a real-world case study could enhance the readerâ€™s grasp of the concept. For instance, showing a simple visualization or discussing how SHAP explanations can uncover biases in predictions would be insightful.

Further Technical Details:

    Handling Missing Data: Expand a bit on how SHAP uses background sampling to manage missing features, possibly with an illustrative diagram or pseudo-code.

    Mathematical Derivation or Intuition: For readers interested in deeper dives, you might provide a link or a brief explanation of how the weighting factors in the Shapley formula are derived.

References to Recent Research:

    While you already included some further reading, consider adding citations or links to recent studies or blog posts that address both the strengths and the ongoing challenges associated with SHAP values.
 -->

# Further reading

- [Interpretable Machine Learning: Shapley values](https://christophm.github.io/interpretable-ml-book/shapley.html)
- [Interpretable Machine Learning: SHAP](https://christophm.github.io/interpretable-ml-book/shap.html)
- [Python's SHAP documentation](https://shap.readthedocs.io)
