---
layout: post
title: SHAP values
date: 2025-04-01 11:59:00-0000
description: A model-agnostic explainable framework
tags: feature_selection machine_learning feature_importance
giscus_comments: true
related_posts: false
toc:
  sidebar: left
---

SHAP values are a model-agnostic method to quantify the contribution of any given feature to the output. They offer both local (per prediction) and global (overall) interpretations.

# Shapley values

SHAP values have their roots in game theory, specifically in the context of **Shapley** values. Imagine a group of players collaborating to achieve a payout. The Shapley value is a method to find out how to fairly distribute the total earnings among the players. Or the blame, if the payout was negative!

A core concept of Shapley values is **coalitions**: given $$n$$ players, a coalition is a subset of the players that can collaborate to achieve the payout. Another concept is the **characteristic function**, $$v: 2^n \rightarrow R $$, which returns the total payout for any given coalition (its *worth*). The last concept is the Shapley value itself, the amount $$ \phi_i $$ that player $$i$$ is given. It is computed as the average difference in the value when $$i$$ is added to all possible coalitions that do not include it. Or more formally:

$$
\phi_i(v) = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|!\; (n-|S|-1)!}{n!} (v(S\cup\{i\})-v(S))
$$

where $$S$$ is a subset of the players.

We are now equipped to describe four properties of Shapley values: efficiency, symmetry, dummy player and additivity. Notice how they all relate to _fair_ attribution.

## Efficiency

The grand coalition is the coalition of all players. Efficiency means that the sum of all Shapley values equals the value of the grand coalition, i.e.,

$$
\sum_i \phi_i(v) = v(N)
$$

In other words, the payout has to be distributed among all players.

## Symmetry

Two players $$i$$ and $$j$$ contribute equally if, for any $$S$$, $$v(S \cup \{i\}) = v(S \cup \{j\})$$. Symmetry implies that players that contribute equally receive the same Shapley value.

## Dummy player

If a player does not change the value of any coalition they join (_dummy_), their Shapley value is 0.

## Additivity

If the payout of two games with different characteristic functions, $$u$$ and $$v$$, are combined, the total gains can be linearly decomposed:

$$
\phi_i(u+v) = \phi_i(u) + \phi_i(v)
$$

# SHAP values

SHAP values are a way of quantifying the contribution of a feature to a model's prediction. SHAP stands for SHapley Additive exPlanations, which highlights their roots on Shapley values. Let’s start by recasting the Shapley value problem by mapping the original concepts to the machine learning context:

- The players are _model features_
- The game is the _model_
- The payout is the _prediction_

With these modifications, the Shapley value looks as follows:

$$
\phi_i(f, x) = \sum_{S \subseteq F \ {i}} \frac{|S|!\; (n-|S|-1)!}{n!} (f(x_{S\cup\{i\}})-f(x_S))
$$

where $$f$$ is our model; $$x$$ is the input datapoint; $$F$$ is the set of all features; and $$n$$ is the total number of features.

> **Note**: Shapley values provide **local** explanations, i.e., the contribution of each feature to a particular prediction. However global explanations can also be computed, by aggregating the local explanations over the whole dataset.

However, most machine learning models cannot deal with missing data. This raises the question, what does it mean to "remove" a feature? SHAP solves this by sampling from all the values for that feature available in the dataset. This ensures that the impact of that feature is averaged out over all possible coalitions.

This naïve approach is also computationally intensive. Exploring all possible coalitions is equivalent to exploring all $$2^n$$ subsets of features. Kernel SHAP approximates the solution by sampling subsets of features and estimating the contributions via a weighted linear regression.

# A visual example

In this example, I train a model that uses 10 clinical features (body mass index, cholesterol, age, and a few others) to predict a continuous measure of disease progression one year after baseline. (No, seriously, [that's its description](https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html).) For the purposes of this example, the model is just a black box whose input are the 10 features, and the output a real number.

{% details The `shap` package %}

SHAP values are implemented in Python via the [`shap`](https://shap.readthedocs.io/en/latest/index.html) package. While I won't be showing any code here, you can see the code that generated the figures [here]({{ "assets/python/2025-04-01-shapley/main.py" | relative_url }}).

{% enddetails %}

I computed the SHAP values describing the importance of each of the 10 variables over all 442 patients. In particular, the SHAP values represent the estimated impact of each feature on a prediction. We can start by looking at the SHAP values for one patient, using a _waterfall_ plot:

{% include figure.liquid loading="eager" path="assets/python/2025-04-01-shapley/img/waterfall_diabetes.jpg" class="img-fluid rounded z-depth-1" %}

The waterfall plot shows how the prediction from this patient (186.53) departed from the mean for the whole training set (152.132). The payout is 186.53 - 152.132 = 34.398, and the SHAP values will quantify how each feature moved the needle. The features are sorted by their absolute SHAP value: less important variables are at the bottom, and the most important ones on top. This ordering is reflected in the length of each bar. Features colored in pink push the prediction toward higher values, and variables in blue toward lower values. We can see that, for this patient, the body mass index was the most important feature, and moved the prediction upwards by 22.25.

We can nicely visualize all 442 patients using a _swarmplot_:

{% include figure.liquid loading="eager" path="assets/python/2025-04-01-shapley/img/beeswarm_diabetes.jpg" class="img-fluid rounded z-depth-1" %}

In the swarmplot, every point represents the SHAP value for a patient: features are shown on the y-axis, and SHAP values on the x-axis. As in the waterfall plot the features are sorted by mean absolute SHAP value; and pink and blue points represent high and low values, respectively.

<!--
Limitations & Caveats:

    Sensitivity to Feature Correlations: Discuss how SHAP values might behave when features are highly correlated. This is a hot topic in model interpretability, as collinearity can distort attributions.

    Computational Complexity: While you mention the naive approach is intensive, you might add a brief discussion on when approximation methods (like Kernel SHAP and Tree SHAP) should be preferred and any trade-offs associated with them.

Comparison with Other Methods:

    Adding a short section comparing SHAP values with other interpretability methods like LIME or Integrated Gradients might help readers understand the broader landscape of explainable AI tools.

Use Cases and Practical Considerations:

    Consider including a section that outlines common applications of SHAP values in model debugging, feature selection, and fairness assessments.

    A small example or a real-world case study could enhance the reader’s grasp of the concept. For instance, showing a simple visualization or discussing how SHAP explanations can uncover biases in predictions would be insightful.

Further Technical Details:

    Handling Missing Data: Expand a bit on how SHAP uses background sampling to manage missing features, possibly with an illustrative diagram or pseudo-code.

    Mathematical Derivation or Intuition: For readers interested in deeper dives, you might provide a link or a brief explanation of how the weighting factors in the Shapley formula are derived.

References to Recent Research:

    While you already included some further reading, consider adding citations or links to recent studies or blog posts that address both the strengths and the ongoing challenges associated with SHAP values.
 -->

# Further reading

- [Interpretable Machine Learning: Shapley values](https://christophm.github.io/interpretable-ml-book/shapley.html)
- [Interpretable Machine Learning: SHAP](https://christophm.github.io/interpretable-ml-book/shap.html)
- [Python's SHAP documentation](https://shap.readthedocs.io)
