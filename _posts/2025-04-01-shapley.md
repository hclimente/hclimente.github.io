---
layout: post
title: SHAP values
date: 2025-04-01 11:59:00-0000
description: A model-agnostic framework for explaining predictions
tags: feature_selection machine_learning feature_importance
giscus_comments: true
related_posts: false
toc:
  sidebar: left
images:
  compare: true
  slider: true
---

SHAP values are a model-agnostic method to quantify the contribution of any given feature to the output. They offer both local (per prediction) and global (overall) interpretations.

# Shapley values

SHAP values have their roots in game theory, specifically in **Shapley** values. Imagine a group of players collaborating to achieve a payout. The Shapley value is a method to find out how to fairly distribute the total earnings among the players. Or the blame, if the payout was negative!

A core concept of Shapley values is **coalitions**: given $$n$$ players, a coalition is a subset of the players that can collaborate to achieve the payout. Another concept is the **characteristic function**, $$v: 2^n \rightarrow R $$, which returns the total payout for any given coalition (its *worth*). The last concept is the Shapley value itself, the amount $$ \phi_i $$ that player $$i$$ receives. It is computed as the average difference in the value when $$i$$ is added to all possible coalitions that do not include it. Or more formally:

$$
\phi_i(v) = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|!\; (n-|S|-1)!}{n!} (v(S\cup\{i\})-v(S))
$$

where $$S$$ is a subset of the players.

These values satisfy four key properties: efficiency, symmetry, dummy player and additivity. Notice how they all relate to _fair_ attribution.

## Efficiency

The grand coalition is the coalition of all players. Efficiency means that the sum of all Shapley values equals the value of the grand coalition, i.e.:

$$
\sum_i \phi_i(v) = v(N)
$$

In other words, the entire payout is distributed among the players.

## Symmetry

Two players $$i$$ and $$j$$ contribute equally if, for any $$S$$, $$v(S \cup \{i\}) = v(S \cup \{j\})$$. Symmetry implies that players that contribute equally receive the same Shapley value.

## Dummy player

If a player does not change the value of any coalition they join (_dummy_), their Shapley value is 0.

## Additivity

If two games with characteristic functions $$u$$ and $$v$$ are combined into a new game $$u + v$$, the Shapley values are additive:

$$
\phi_i(u+v) = \phi_i(u) + \phi_i(v)
$$

# SHAP values

Machine learning models like linear regression are _interpretable_, that is, the model parameters indicate how each input feature contributes to the prediction. However, many models like neural networks or random forests are less interpretable: their output is a complex combination of all the input features. SHAP values ([Lundberg and Lee, 2017](https://arxiv.org/abs/1705.07874)) are a way of quantifying the contribution of a feature to a model's prediction. SHAP stands for SHapley Additive exPlanations, which highlights their roots in _Shapley_ values. Intuitively, SHAP values quantify how much each feature is responsible for how the prediction differs from the average prediction $$\mathbb{E}[f(X)]$$. Some features will have a negative contribution (they'll push the prediction to lower values) and others will have a positive contribution, pushing the prediction to higher values.

To establish this connection more formally, letâ€™s start by recasting the Shapley value problem by mapping the original concepts to the machine learning context:

- The $$n$$ players become $$n$$ _predictive features_
- The game is the _model_
- The payout is the _prediction_; or rather, the departure of the prediction from its expected value

With these modifications, the Shapley value of feature $$i$$ could be computed as follows:

$$
\phi_i = \sum_{S \subseteq F \setminus \{i\}} \frac{|S|!\; (n-|S|-1)!}{n!} (f(\mathbf{x}_{S\cup\{i\}})-f(\mathbf{x}_S))
$$

where $$\mathbf{x}_S$$ is the input datapoint including only the features in $$S$$; $$F$$ is the set of all features; $$n$$ is the total number of features; and $$f_S$$ is the model trained only on the features in set $$S$$. However, this approach is very computationally intensive, since it requires retraining $$2^n$$ models, one per possible coalition. (See a worked out example [here](https://www.aidancooper.co.uk/how-shapley-values-work/).) SHAP values get around re-training models by approximating the effect of feature subsets using conditional expectations: $$f(\mathbf{x}_S) = \mathbb{E}[f(X) \mid X_S = \mathbf{x}_S]$$. In other words, we fix the features that are in $$S$$ to the sample values, and average over the predictions when sampling the remaining features from the dataset.

> **_Simplified_ features:** we use $$\mathbf{x}$$ to represent the features in the original space $$\chi$$; it's a vector of length $$n$$. Instead, SHAP operates on a simplified feature vector, $$\mathbf{x}' \in \{0,1\}^m$$ representing the coalition: $$x'_j = 1$$ if and only if _simplified_ feature $$j$$ was included in the coalition. $$\mathbf{x}'$$ has length $$m$$, which can be different from $$n$$. That's because $$\mathbf{x}'$$ represents simplified features that are useful for interpretation. For instance, if $$\mathbf{x}$$ represented the individual pixels of an image, $$\mathbf{x}'$$ could represent the "super pixels" that form the cat, the grass or the sky. They are linked by a function $$h_\mathbf{x}: \{0,1\}^m \rightarrow \chi$$. If $$\mathbf{x}' = \mathbf{1}$$ we retrieve the original feature vector: $$h_\mathbf{x}(\mathbf{x}') = \mathbf{x}$$. Otherwise $$h_\mathbf{x}(\mathbf{x}')$$ equals a variation of $$\mathbf{x}$$ with some features removed. Note that $$h_\mathbf{x}$$ is specific to a particular input instance.

That covers the _Shapley_ part of SHAP; let's now focus on the _Additive exPlanation_ bit. SHAP aims to obtain a _local_ explanation function $$g$$ to explain each prediction $$f(\mathbf{x})$$:

$$
g(\mathbf{x}') = \phi_0 + \sum_{j = 1}^n \phi_j \mathbf{x}'_j
$$

where $$\phi_0$$ is the expectation over all training examples $$\mathbb{E}[f(X)]$$. $$g(\mathbf{x}')$$ is a very easy to interpret function that we'll use to explain $$f(\mathbf{x})$$.

Since SHAP values are Shapley values, they meet all the properties specified above. But they also satisfy three additional properties that are desirable for model explainers.

## Local accuracy

If we transpose the **efficiency** property, the explanation needs to cover all the features and equal the prediction:

$$
f(\mathbf{x}) = g(\mathbf{x}') = \phi_0 + \sum_{j = 1}^m \phi_j \mathbf{x}'_j.
$$

Since we use all the features, $$\mathbf{x}' = \mathbf{1}$$ and $$\phi_0 = \mathbb{E}[f(X)]$$:

$$
f(\mathbf{x}) = \mathbb{E}[f(X)] + \sum_{j = 1}^m \phi_j.
$$

## Missingness

If a feature is missing, it deserves 0 attribution. However, most machine learning models cannot deal with missing data. This raises the question, what does it mean to "remove" a feature? SHAP solves this by sampling from all the values for that feature available in the dataset. This ensures that the impact of that feature is averaged out over all possible coalitions.

## Consistency

# A visual example

Let's understand SHAP values better by looking at an example. I trained a model that uses 10 clinical features (body mass index, cholesterol, age, and a few others) to predict a continuous measure of disease progression one year after baseline. (No, seriously, [that's its description](https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html).) For the purposes of this example, the model is just a black box whose input are the 10 aforementioned features, and the output a real number.

> SHAP values can be computed on the train set, used to train the model, as well as in a holdout set. Usually train sets are much larger, and in that sense using them will paint a more accurate picture of what the model learned. However, if there is a shift between their data-generating distributions, the results can differ significantly.

{% details The `shap` package %}

SHAP values are implemented in Python via the [`shap`](https://shap.readthedocs.io/en/latest/index.html) package. While I won't be showing any code here, you can see the code that generated the figures [here]({{ "assets/python/2025-04-01-shapley/main.py" | relative_url }}).

{% enddetails %}

SHAP values provide **local** explanations, i.e., the contribution of each feature to a particular prediction. I computed the SHAP values describing the importance of each of the 10 variables over all 442 patients. In particular, the SHAP values represent the estimated impact of each feature on a prediction. We can start by looking at the SHAP values for one patient, using a _waterfall_ plot:

{% include figure.liquid loading="eager" path="assets/python/2025-04-01-shapley/img/waterfall_diabetes.jpg" class="img-fluid rounded z-depth-1" %}

The waterfall plot shows how the prediction for this patient (186.53) departed from the average prediction over the training set (152.132). The payout is 186.53 - 152.132 = 34.398. As per the local accuracy property the SHAP values sum up to this difference. The features are sorted by their absolute SHAP value: less important variables are at the bottom, and the most important ones on top. This ordering is reflected in the length of each bar. Features colored in pink push the prediction toward higher values, and features in blue toward lower values. We can see that, for this patient, the body mass index was the most important feature, and moved the prediction upwards by 22.25.

We can nicely visualize all 442 patients using a _swarmplot_:

{% include figure.liquid loading="eager" path="assets/python/2025-04-01-shapley/img/beeswarm_diabetes.jpg" class="img-fluid rounded z-depth-1" %}

In the swarmplot, every point represents the SHAP value for a patient: features are shown on the y-axis, and SHAP values on the x-axis. As in the waterfall plot the features are sorted by mean absolute SHAP value; and pink and blue points represent high and low feature values, respectively.

Global explanations can also be computed, by aggregating the local explanations over the whole dataset. For instance, we can display the average absolute SHAP value for each feature:

{% include figure.liquid loading="eager" path="assets/python/2025-04-01-shapley/img/global_diabetes.jpg" class="img-fluid rounded z-depth-1" %}

The features that have consistently the largest absolute SHAP values will be at the top, highlighting their importance in the model.

Lastly, SHAP values can be used for clustering, i.e., to look for similar groups of datapoints within our dataset by identifying common patterns among the features. Typically clustering is carried out in the original feature space. This is problematic because often features have different units and some scaling or normalization is needed in order to meaningfully compare them. Instead of performing clustering in the original feature space, we can do it in the SHAP value space. This is a type of _supervised_ clustering, since we are indirectly leveraging the model's predictions (and thus indirectly the outcome) in the clustering. Supervised clustering looks for similarities in how different features contribute to the outcome, which can be useful to identify groups in which different features contribute to the outcome. Let's see how it plays out in our example:

<style>
    .colored-slider {
        --divider-color: rgba(0, 0, 0, 0.5);
        --default-handle-color: rgba(0, 0, 0, 0.5);
        --default-handle-width: clamp(40px, 10vw, 200px);
  }
</style>
<img-comparison-slider class="colored-slider">
    {% include figure.liquid path="assets/python/2025-04-01-shapley/img/supervised_pca.jpg" class="img-fluid rounded z-depth-1" slot="first" %}
    {% include figure.liquid path="assets/python/2025-04-01-shapley/img/unsupervised_pca.jpg" class="img-fluid rounded z-depth-1" slot="second" %}
</img-comparison-slider>

While the PCA does not reveal clearly identifiable groups, the patients are more grouped together by their outcome when using SHAP values.

# Flavors of SHAP

[Above](#shap-values) I described a naÃ¯ve approach to compute SHAP values. Unfortunately, it is very computationally intensive: exploring all possible coalitions is equivalent to exploring all $$2^n$$ subsets of features. For that reason, different flavors of SHAP values have been proposed to make computations more efficient.

## Kernel SHAP

**Kernel SHAP** approximates the solution by sampling subsets of features and estimating the contributions via a weighted linear regression.

## Tree SHAP

## DeepLift

<!--
Limitations & Caveats:

    Sensitivity to Feature Correlations: Discuss how SHAP values might behave when features are highly correlated. This is a hot topic in model interpretability, as collinearity can distort attributions.

    Computational Complexity: While you mention the naive approach is intensive, you might add a brief discussion on when approximation methods (like Kernel SHAP and Tree SHAP) should be preferred and any trade-offs associated with them.

Use Cases and Practical Considerations:

    Consider including a section that outlines common applications of SHAP values in model debugging, feature selection, and fairness assessments.

    A small example or a real-world case study could enhance the readerâ€™s grasp of the concept. For instance, showing a simple visualization or discussing how SHAP explanations can uncover biases in predictions would be insightful.

Further Technical Details:

    Handling Missing Data: Expand a bit on how SHAP uses background sampling to manage missing features, possibly with an illustrative diagram or pseudo-code.

    Mathematical Derivation or Intuition: For readers interested in deeper dives, you might provide a link or a brief explanation of how the weighting factors in the Shapley formula are derived.

 -->

# Further reading

- [Interpretable Machine Learning: Shapley values](https://christophm.github.io/interpretable-ml-book/shapley.html)
- [Interpretable Machine Learning: SHAP](https://christophm.github.io/interpretable-ml-book/shap.html)
- [Python's `shap` documentation](https://shap.readthedocs.io)
- [Supervised Clustering: How to Use SHAP Values for Better Cluster Analysis](https://www.aidancooper.co.uk/supervised-clustering-shap-values/)
