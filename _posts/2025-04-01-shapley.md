---
layout: post
title: SHAP values
date: 2025-04-01 11:59:00-0000
description: A model-agnostic explainable framework
tags: feature_selection machine_learning feature_importance
giscus_comments: true
related_posts: false
toc:
  sidebar: left
images:
  compare: true
  slider: true
---

SHAP values are a model-agnostic method to quantify the contribution of any given feature to the output. They offer both local (per prediction) and global (overall) interpretations.

# Shapley values

SHAP values have their roots in game theory, specifically in **Shapley** values. Imagine a group of players collaborating to achieve a payout. The Shapley value is a method to find out how to fairly distribute the total earnings among the players. Or the blame, if the payout was negative!

A core concept of Shapley values is **coalitions**: given $$n$$ players, a coalition is a subset of the players that can collaborate to achieve the payout. Another concept is the **characteristic function**, $$v: 2^n \rightarrow R $$, which returns the total payout for any given coalition (its *worth*). The last concept is the Shapley value itself, the amount $$ \phi_i $$ that player $$i$$ is given. It is computed as the average difference in the value when $$i$$ is added to all possible coalitions that do not include it. Or more formally:

$$
\phi_i(v) = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|!\; (n-|S|-1)!}{n!} (v(S\cup\{i\})-v(S))
$$

where $$S$$ is a subset of the players.

We are now equipped to describe four properties of Shapley values: efficiency, symmetry, dummy player and additivity. Notice how they all relate to _fair_ attribution.

## Efficiency

The grand coalition is the coalition of all players. Efficiency means that the sum of all Shapley values equals the value of the grand coalition, i.e.,

$$
\sum_i \phi_i(v) = v(N)
$$

In other words, the payout has to be distributed among all players.

## Symmetry

Two players $$i$$ and $$j$$ contribute equally if, for any $$S$$, $$v(S \cup \{i\}) = v(S \cup \{j\})$$. Symmetry implies that players that contribute equally receive the same Shapley value.

## Dummy player

If a player does not change the value of any coalition they join (_dummy_), their Shapley value is 0.

## Additivity

If the payout of two games with different characteristic functions, $$u$$ and $$v$$, are combined, the total gains can be linearly decomposed:

$$
\phi_i(u+v) = \phi_i(u) + \phi_i(v)
$$

# SHAP values

SHAP values ([Lundberg and Lee, 2017](https://arxiv.org/abs/1705.07874)) are a way of quantifying the contribution of a feature to a model's prediction. SHAP stands for SHapley Additive exPlanations, which highlights their roots on Shapley values. Let’s start by recasting the Shapley value problem by mapping the original concepts to the machine learning context:

- The $$n$$ players become $$n$$ _model features_
- The game is the _model_
- The payout is the _prediction_; or rather, the departure of the prediction from its expected value

With these modifications, the Shapley value looks as follows:

$$
\phi_i(f, \mathbf{x}) = \sum_{S \subseteq F \setminus \{i\}} \frac{|S|!\; (n-|S|-1)!}{n!} (f(\mathbf{x}_{S\cup\{i\}})-f(\mathbf{x}_S))
$$

where $$f$$ is our model; $$\mathbf{x}$$ is the input datapoint; $$F$$ is the set of all features; and $$n$$ is the total number of features.

Intuitively, SHAP values quantify how much each feature is responsible for how the prediction differs the average prediction $$\mathbb{E}[f(X)]$$. Some features will have a negative contribution (they'll push the prediction to lower values) and others will have a positive contribution, pushing the prediction to higher values. Specifically, SHAP postulates an additive explanation $$g$$ for any given coalition of features:

$$
g(\mathbf{z}') = \phi_0 + \sum_{j = 1}^n \phi_j x'_j
$$

where $$\mathbf{z}'$$ is an indicator vector that describes if $$x_j$$ was included in the coalition. Since SHAP values are Shapley values, they meet all the properties specified above. But also three others.

## Local accuracy

If we transpose the **efficiency** property, the explanation needs to cover all the features and equal the prediction:

$$
f(\mathbf{x}) = g(\mathbf{z}') = \phi_0 + \sum_{j = 1}^n \phi_j x'_j.
$$

Since we use all the features, $$\mathbf{z}' = \mathbf{1}$$ and $$\phi_0 = \mathbb{E}[f(X)]$$:

$$
f(\mathbf{x}) = \mathbb{E}[f(X)] + \sum_{j = 1}^n \phi_j.
$$

## Missingness

If a feature is missing, it deserves 0 attribution. However, most machine learning models cannot deal with missing data. This raises the question, what does it mean to "remove" a feature? SHAP solves this by sampling from all the values for that feature available in the dataset. This ensures that the impact of that feature is averaged out over all possible coalitions.

## Consistency

# A visual example

Let's understand better SHAP values by looking at an example. I trained a model that uses 10 clinical features (body mass index, cholesterol, age, and a few others) to predict a continuous measure of disease progression one year after baseline. (No, seriously, [that's its description](https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html).) For the purposes of this example, the model is just a black box whose input are the 10 features, and the output a real number.

> SHAP values can be computed on the train set, used to train the model, as well as in a holdout set. Usually train sets are much larger, and in that sense using them will paint a more accurate picture of what the model learnt. However, if there is a shift between their data-generating distributions, the results can differ significantly.

{% details The `shap` package %}

SHAP values are implemented in Python via the [`shap`](https://shap.readthedocs.io/en/latest/index.html) package. While I won't be showing any code here, you can see the code that generated the figures [here]({{ "assets/python/2025-04-01-shapley/main.py" | relative_url }}).

{% enddetails %}

SHAP values provide **local** explanations, i.e., the contribution of each feature to a particular prediction. I computed the SHAP values describing the importance of each of the 10 variables over all 442 patients. In particular, the SHAP values represent the estimated impact of each feature on a prediction. We can start by looking at the SHAP values for one patient, using a _waterfall_ plot:

{% include figure.liquid loading="eager" path="assets/python/2025-04-01-shapley/img/waterfall_diabetes.jpg" class="img-fluid rounded z-depth-1" %}

The waterfall plot shows how the prediction from this patient (186.53) departed from the mean for the whole training set (152.132). The payout is 186.53 - 152.132 = 34.398. As per the local accuracy property the SHAP values will distribute the payout among the features. The features are sorted by their absolute SHAP value: less important variables are at the bottom, and the most important ones on top. This ordering is reflected in the length of each bar. Features colored in pink push the prediction toward higher values, and variables in blue toward lower values. We can see that, for this patient, the body mass index was the most important feature, and moved the prediction upwards by 22.25.

We can nicely visualize all 442 patients using a _swarmplot_:

{% include figure.liquid loading="eager" path="assets/python/2025-04-01-shapley/img/beeswarm_diabetes.jpg" class="img-fluid rounded z-depth-1" %}

In the swarmplot, every point represents the SHAP value for a patient: features are shown on the y-axis, and SHAP values on the x-axis. As in the waterfall plot the features are sorted by mean absolute SHAP value; and pink and blue points represent high and low values, respectively.

**Global** explanations can also be computed, by aggregating the local explanations over the whole dataset. For instance, we can display the average absolute SHAP value for each feature:

{% include figure.liquid loading="eager" path="assets/python/2025-04-01-shapley/img/global_diabetes.jpg" class="img-fluid rounded z-depth-1" %}

The features that have consistently the largest SHAP values will be at the top, highlighting their importance in the model.

Last, SHAP values can be used for clustering, i.e., to look for similar groups of datapoints within our dataset. Instead of performing clustering in the original feature space, we do it in the SHAP value space. This is a type of _supervised_ clustering, since we are indirectly leveraging the outcome in the clustering. Let's see one example:

<style>
    .colored-slider {
        --divider-color: rgba(0, 0, 0, 0.5);
        --default-handle-color: rgba(0, 0, 0, 0.5);
        --default-handle-width: clamp(40px, 10vw, 200px);
  }
</style>
<img-comparison-slider class="colored-slider">
    {% include figure.liquid path="assets/python/2025-04-01-shapley/img/supervised_pca.jpg" class="img-fluid rounded z-depth-1" slot="first" %}
    {% include figure.liquid path="assets/python/2025-04-01-shapley/img/unsupervised_pca.jpg" class="img-fluid rounded z-depth-1" slot="second" %}
</img-comparison-slider>

# Flavors of SHAP

[Above](#shap-values) I described a naïve approach to compute SHAP values. Unfortunately, it is very computationally intensive: exploring all possible coalitions is equivalent to exploring all $$2^n$$ subsets of features. For that reason, different flavors of SHAP values have been proposed to make computations more efficient.

## Kernel SHAP

**Kernel SHAP** approximates the solution by sampling subsets of features and estimating the contributions via a weighted linear regression.

## Tree SHAP

## DeepLift

<!--
Limitations & Caveats:

    Sensitivity to Feature Correlations: Discuss how SHAP values might behave when features are highly correlated. This is a hot topic in model interpretability, as collinearity can distort attributions.

    Computational Complexity: While you mention the naive approach is intensive, you might add a brief discussion on when approximation methods (like Kernel SHAP and Tree SHAP) should be preferred and any trade-offs associated with them.

Use Cases and Practical Considerations:

    Consider including a section that outlines common applications of SHAP values in model debugging, feature selection, and fairness assessments.

    A small example or a real-world case study could enhance the reader’s grasp of the concept. For instance, showing a simple visualization or discussing how SHAP explanations can uncover biases in predictions would be insightful.

Further Technical Details:

    Handling Missing Data: Expand a bit on how SHAP uses background sampling to manage missing features, possibly with an illustrative diagram or pseudo-code.

    Mathematical Derivation or Intuition: For readers interested in deeper dives, you might provide a link or a brief explanation of how the weighting factors in the Shapley formula are derived.

 -->

# Further reading

- [Interpretable Machine Learning: Shapley values](https://christophm.github.io/interpretable-ml-book/shapley.html)
- [Interpretable Machine Learning: SHAP](https://christophm.github.io/interpretable-ml-book/shap.html)
- [Python's `shap` documentation](https://shap.readthedocs.io)
- [Supervised Clustering: How to Use SHAP Values for Better Cluster Analysis](https://www.aidancooper.co.uk/supervised-clustering-shap-values/)
