---
layout: distill
title: How do vector databases work?
date: 2025-08-09 11:59:00 +0000
description: Finding similar documents in large collections
tags:
  - machine_learning
  - graphs
giscus_comments: true
related_posts: false
---

A big part of data science revolves around structured data, data that can be neatly organized into tables. We love tables: they can be easily sliced, diced, summarized, and filtered using SQL or another similar language. To check if an item exists in our table, we can do that quickly by checking for equality of all the rows.

However, large swaths of data can't naturally fit in a table. This is the case with corpora of text, DNA sequences, or songs. In such cases, finding _equal_ elements is still easy by, for instance, checking bit-wise equality. If we are feeling fancy, we might even do that efficiently using hash functions. However, in these cases, _quasi-equality_ is even more important. We want all the versions of [our favorite song](https://www.youtube.com/watch?v=dQw4w9WgXcQ); all the pieces of news reporting on the same event; all the pictures containing a cat. How do we go about that?

# Formalizing the problem

Our goal is to find all documents in a collection of $$N$$ items (like documents, images, or songs) that are similar, though not necessarily equal, to our query item. We will represent each item in a $$D$$-dimensional space. Given a query item, our goal is to find the items that are similar to it. This problem can be decomposed into three subproblems:

- **Embedding the items**, that is, finding a vector representation of each item that retains as much information as possible
- **Computing the similarity between items**
- **Finding the nearest neighbors** of the query item in an efficient manner

# The embeddings

**Embeddings** are just vectors that represent a piece of contentâ€”a song, a text, or [a piece of DNA]({% post_url 2025-05-02-hf-transformers %}#embedding-dna-sequences). You can think of embeddings as the coordinates in some an arbitary, very high-dimensional space. Related documents inhabit nearby regions of this space. They are usually the activations of a given internal layer of a pre-trained model. This ensures two things. First, that all the embeddings have the _same dimensions_, i.e., they map to the same space and we can neatly arrange them into a matrix. Second, that the representation is _meaningful_. After all, the model had to learn a good representation of the data during training. This also ensures that semantically similar items are near each other.

Embedding the items provides many advantages: where we had atomic units of unstructured data, we now have numerical representations that can be mathematically operated on. For instance, we can go from the 32 individual posts in my blog to a $$32 \times 384$$ matrix of their embeddings. Then, we can do a dimensionality reduction step:

<div class="l-page">
    <iframe src="{{ '/assets/python/2025-08-16-rags/plotly/posts_umap.html' | relative_url }}" frameborder='0' scrolling='no' height="500px" width="100%"></iframe>
</div>

<div class="caption">
    Scatter plot of the two UMAP dimensions from the 384-dimensional embeddings computed by applying the [`all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) LLM to my 32 blog posts. They are colored by their highest ranking tag. Hover over the post to see the title; click on it to read the post.
</div>

Posts with similar tags are neatly close to each other. But the embeddings offer a bit more nuance than that. Posts about statistical methods live near those about machine learning. The post about [data structures]({% post_url 2025-05-02-hf-transformers %}) live between the posts related to Python and those discussing graphs.

## How to embed well

I just glossed over a very important detail: embedding a whole document _is a bad idea_. We can see an embedding as a lossy compression of our document. But, since the dimensionality of our embeddings is fixed, the more material we compress, the more information we lose. That's why many popular embedders are trained on sentences and short paragraphs, not on long texts. In fact, the model used above only used the first 256 tokens of the post and completely discarded the rest.

Instead of dealing with the whole document, it's better to split each post into semantically coherent chunks (e.g., paragraphs) and then embedded the chunks individually. The recursive character split is a common way of getting nice chunks. This strategy aims to get chunks smaller than a predetermined maximum chunk size. First, it tries to get chunks of the right size splitting by paragraph. For those paragraphs that remain too long, it further splits them into sentences. If they are still too long, it keeps going, splitting them into words and then into individual characters if needed.

TODO: overlap between paragraphs
TODO: embedding models

<div class="l-page">
    <iframe src="{{ '/assets/python/2025-08-16-rags/plotly/paragraphs_umap.html' | relative_url }}" frameborder='0' scrolling='no' height="500px" width="100%"></iframe>
</div>

<div class="caption">
    Scatter plot of the two UMAP dimensions from the 384-dimensional embeddings computed by applying the [`all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) LLM to my 1,151 chunks of text from my 32 blog posts. They are colored by their highest post of origin tag. Hover over the point to see the title of their post of origin and the first words of the chunked text.
</div>

The results are now more nuanced. In general, the points are grouped by their post of origin, which is a good sanity check. On the top right we find chunks coming from my Python series, which are understandably quite similar.

# The distance measure

If we wanted to find equal documents, an efficient solution would be relatively straightforward: hash them and see which ones fall in the same bucket. But that's not the task we've embarked on. If the number of documents is large enough, eye-balling the UMAP plot is not an option either. We need a **distance** measure, a function whose inputs are two items and whose output is a real value telling us how _far_ they are. The distance will be low when the two items are alike, and high when they are not. Analogously, we can define **similarity** measures, which behave just in the opposite way.

Many distance and similarity measures have been defined for different kinds of data. In the vector spaces in which our embeddings live, the most popular ones are:

| Measure                |               Formula               | Meaning                                                        | Range                 |
| ---------------------- | :---------------------------------: | -------------------------------------------------------------- | --------------------- |
| **Cosine** similarity  | $$\frac {u \cdot v} {\|u\| \|v\|}$$ | Extent to which $u$ and $v$ point in the same direction        | $$[-1, 1]$$           |
| **Dot product**        | $$u \cdot v$$                       | Same as cosine, but multiplied by the magnitude of $u$ and $v$ | $$(-\infty, \infty)$$ |
| **Euclidean** distance |   $$\sqrt{\sum_i (u_i - v_i)^2}$$   | Distance between the tips of $u$ and $v$ in Euclidean space    | $$[0, \infty)$$       |

The cosine similarity is a common choice to measure semantic similarity of embeddings. Although I haven't found a satisfying explanation for the why, the lengths of the vectors do not seem to carry interesting information over the direction in which they point.

For instance, here is the similarity matrix between all 1,151 text chunks:

{% include figure.liquid loading="eager" path="assets/python/2025-08-16-rags/img/paragraph_similarity_heatmap.webp" class="img-fluid" %}

<div class="l-page">
    <iframe src="{{ '/assets/python/2025-08-16-rags/plotly/paragraph_similarity_heatmap.html' | relative_url }}" frameborder='0' scrolling='no' height="500px" width="100%"></iframe>
</div>

<div class="caption">
    Pairwise cosine similarities between all 1,151 text chunks. Values with an absolute cosine similarity lower than 0.1 have been set to 0 to facilitate visualization.
</div>

# Nearest neighbor search

Let's fetch the 5 chunks of text that best match this query:

```
Interpretable machine learning
```

To that end, I will embed it and compare that embedding against that of all my chunks. Here are the five chunks with the highest cosine similarity:

| Post title  | Cosine similarity | Text                                              |
|-------------|-------------------|---------------------------------------------------|
| SHAP values | 0.514             | Machine learning models like linear regression... |
| SHAP values | 0.505             | - [Interpretable Machine Learning: Shapley val... |
| SHAP values | 0.403             | To establish the connection to Shapley values,... |
| SHAP values | 0.391             | Let's understand SHAP values better by looking... |
| SHAP values | 0.388             | Global explanations can be derived by aggregat... |

I would say the search went quite well. I was lucky I am not a prolific writer, and I could compute all the distances. But this operation has a time complexity of $$O(MN)$$ for the simple query, where $$N$$ is the number of posts and $$M$$ the average chunk length. In large collections, this would be too expensive. That's why I will focus on approximate methods, which produce good enough estimations of similarity in a reasonable time.

In the past, [I described a method to find similar items using **local-sensitivity hashing** (LSH)]({% post_url 2017-11-04-finding-similar-items %}). The idea is to hash the items in such a way that similar items are more likely to fall in the same bucket. This way, we can quickly scan over the corpus and find the items that are similar to our query. Here I will focus on **hierarchical navigable small world graphs** (HNSW), a method widely used in modern vector databases. They build on two concepts: _skip lists_ and _navigable small world graphs_.

**Skip lists** are a data structure that provides $$O(\log n)$$ search time, same as binary search trees, and also $$O(\log n)$$ insertion time. They consist of a set of [linked lists]({% post_url 2024-02-15-data-structures %}#linked-lists), each one containing a subset of the items in the collection. The topmost list contains only a few items, while the bottommost list contains all the items. Each item in a list points to the next item in the same list, and also to the next item in the lists below it. This allows us to quickly traverse the lists and find the item we are looking for.

{% include figure.liquid path="assets/img/posts/2025-08-16-rags/skip_list_diagram.png" class="img-fluid" %}

<div class="caption">
    4-layered skip list.
</div>

**Navigable graphs** are graphs in which we can find a path between any two nodes via a greedy strategy that chooses the neighbor closest according to a [distance function](#the-distance-measure).

HNSW graphs are _indexing_ structures, i.e., every time we add a new item to our collection, we will update the graph. The graph is built in such a way that it allows us to quickly find the nearest neighbors of a query item.

# Vector databases

Vector databases efficiently encapsulate the above to efficiently find similar items. I will show how to do the above using the [Qdrant vector database](https://qdrant.tech/).

# Why does this matter?

Efficient vector search has traditionally been very used in recommendation algorithms. However, the advent of LLM has infused them with renewed interest.

When an LLM produces an output, it uses two sources of information: its _memory_ and the _query_. The LLM's memory consists of large swaths of patterns learnt during training and stored in its weights. The query is the user request, containing a context and potentially some additional information. Vector databases provide a way to add a third source of information, allowing the LLM to efficiently retrieve relevant items from a corpus and leverage them in its answer. This is called **retrieval augmented generation** or RAG.

# References

- [My previous, pre-LLM write-up on this topic]({% post_url 2017-11-04-finding-similar-items %})
- [Embeddings: What they are and why they matter](https://simonwillison.net/2023/Oct/23/embeddings/)
- [Pinecone: Hierarchical Navigable Small Worlds](https://www.pinecone.io/learn/series/faiss/hnsw/)
