---
layout: distill
title: How do vector databases work?
date: 2025-08-09 11:59:00 +0000
description: Finding similar documents in large collections
tags:
  - machine_learning
  - graphs
giscus_comments: true
related_posts: false
---

A big part of data science revolves around structured data, data that can be neatly organized into tables. We love tables: they can be easily sliced, diced, summarized, and filtered using SQL or another similar language. If we want to check if an item exists in our table, we can do that quicky by checking for equality of all the rows.

However, large swaths of data can't naturally fit in a table. This is the case of corpora of text, DNA sequences or songs. In such cases, finding _equal_ elements is still easy by, for instance, checking bit-wise equality. If we are feeling fancy, we might even do that efficiently using hash functions. However, in these cases, _quasi-equality_ is even more important. We want all the versions of [our favorite song](https://www.youtube.com/watch?v=dQw4w9WgXcQ); all the pieces of news reporting on the same event; all the pictures containing a cat. How do we go about that?

# Formalizing the problem

Our goal is finding all documents in a collection of $$N$$ items (like documents, images or songs) that are similar, though not necessarily equal, to our query document. We will represent each item in a $$D$$-dimensional space. Given a query item, our goal is to find the items that are similar to it. The above problem relies on three concepts:

- **Embeddings**: $$d$$-dimensional numerical representations of the items in our collection
- **Distance function**: a measure of similarity between pairs of items
- **Nearest neighbor search**: an algorithm to find the similar items

## The embeddings

Embeddings are just vectors.

<div class="l-page">
    <iframe src="{{ '/assets/python/2025-08-16-rags/img/posts_umap.html' | relative_url }}" frameborder='0' scrolling='no' width="100%"></iframe>
</div>

<div class="caption">
    Scatter plot of the two UMAP dimensions from the embeddings computed by applying the <em>pre-trained</em> NT to the 6,000 DNA sequences test dataset, containing 1,000 sequences from each species.
</div>

## The distance measure

If we wanted to find equal documents, an efficient solution would be relatively straightforward: hash them and see which ones fall in the same bucket. But that's not the task we embarked ourselves on. We need a **distance** measure, a function whose inputs are two items and whose output is a real value. The distance will be low when the two items are alike, and high when they are not. (Analogously, we could define a **similarity** measures, which behaves just in the opposite way.)

Many distance and similarity measures have been defined for different kinds of data. In the vector spaces in which our embeddings live the most popular ones are:

| Distance        |               Formula               | Meaning                                                     | Range                 |
| --------------- | :---------------------------------: | ----------------------------------------------------------- | --------------------- |
| **Dot product** |            $$u \cdot v$$            | Extent to which $u$ and $v$ point in the same direction     | $$(-\infty, \infty)$$ |
| **Cosine**      | $$\frac {u \cdot v} {\|u\| \|v\|}$$ | Cosine of the angle $\theta$ between $u$ and $v$            | $$[-1, 1]$$           |
| **Euclidean**   |   $$\sqrt{\sum_i (u_i - v_i)^2}$$   | Distance between the tips of $u$ and $v$ in Euclidean space | $$[0, \infty)$$       |

## Nearest neighbor search

In the naive case, we would compare our query item agains all the items in our collection, calculating the distance between each pair. This has a time complexity of $$O(N^2)$$, which makes in unfeasible for large collections. That's why I will focus on approximate methods, which produce good enough estimation of similarity in a reasonable time.

In the past, [I described a method to find similar items using **local-sensitivity hashing** (LSH)]({% post_url 2017-11-04-finding-similar-items %}). The idea is to hash the items in such a way that similar items are more likely to fall in the same bucket. This way, we can quickly scan over the corpus and find the items that are similar to our query. Here I will focus on **hierarchical navigable small world** graphs (HNSW), a method widely used in modern vector databases. They build on two concepts: _skip lists_ and _navigable small world graphs_.

**Skip lists** are a data structure that provides $$O(\log N)$$ search time for search, same as binary search trees, and also $$O(\log N)$$ insertion time. They consist of a set of [linked lists]({% post_url 2024-02-15-data-structures %}#linked-lists), each one containing a subset of the items in the collection. The topmost list contains only a few items, while the bottommost list contains all the items. Each item in a list points to the next item in the same list, and also to the next item in the list above it. This allows us to quickly traverse the lists and find the item we are looking for.

{% include figure.liquid path="assets/img/posts/2025-08-16-rags/skip_list_diagram.png" class="img-fluid" %}

<div class="caption">
    4-layered skip list.
</div>

**Navigable graphs** are graphs in which we can find a path between any two nodes via a greedy strategy that choses the neighbor closest according to a [distance function](#the-distance-measure).


HNSW graphs are _indexing_ structures, i.e., every time we add a new item to our collection, we will update the graph. The graph is built in such a way that it allows us to quickly find the nearest neighbors of a query item.

The pro

# References

- [My previous, pre-LLM write-up on this topic]({% post_url 2017-11-04-finding-similar-items %})
- [Pinecone: Hierarchical Navigable Small Worlds](https://www.pinecone.io/learn/series/faiss/hnsw/)
