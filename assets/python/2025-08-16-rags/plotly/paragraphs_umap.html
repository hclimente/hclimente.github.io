<html>
<head><meta charset="utf-8" /></head>
<body>
    <div>                        <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-3.1.0.min.js" integrity="sha256-Ei4740bWZhaUTQuD6q9yQlgVCMPBz6CZWhevDYPv93A=" crossorigin="anonymous"></script>                <div id="umap_plot" class="plotly-graph-div" style="height:100%; width:100%;"></div>            <script type="text/javascript">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById("umap_plot")) {                    Plotly.newPlot(                        "umap_plot",                        [{"hoverinfo":"text","marker":{"color":["hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(86,70%,50%)","hsl(136,70%,50%)","hsl(136,70%,50%)","hsl(136,70%,50%)","hsl(136,70%,50%)","hsl(136,70%,50%)","hsl(136,70%,50%)","hsl(136,70%,50%)","hsl(136,70%,50%)","hsl(136,70%,50%)","hsl(136,70%,50%)","hsl(136,70%,50%)","hsl(136,70%,50%)","hsl(136,70%,50%)","hsl(136,70%,50%)","hsl(136,70%,50%)","hsl(136,70%,50%)","hsl(136,70%,50%)","hsl(136,70%,50%)","hsl(136,70%,50%)","hsl(136,70%,50%)","hsl(136,70%,50%)","hsl(136,70%,50%)","hsl(136,70%,50%)","hsl(136,70%,50%)","hsl(136,70%,50%)","hsl(136,70%,50%)","hsl(136,70%,50%)","hsl(136,70%,50%)","hsl(99,70%,50%)","hsl(99,70%,50%)","hsl(99,70%,50%)","hsl(99,70%,50%)","hsl(99,70%,50%)","hsl(99,70%,50%)","hsl(99,70%,50%)","hsl(99,70%,50%)","hsl(99,70%,50%)","hsl(99,70%,50%)","hsl(99,70%,50%)","hsl(99,70%,50%)","hsl(99,70%,50%)","hsl(99,70%,50%)","hsl(99,70%,50%)","hsl(99,70%,50%)","hsl(99,70%,50%)","hsl(99,70%,50%)","hsl(99,70%,50%)","hsl(99,70%,50%)","hsl(99,70%,50%)","hsl(99,70%,50%)","hsl(99,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(211,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(74,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(285,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(335,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(248,70%,50%)","hsl(273,70%,50%)","hsl(273,70%,50%)","hsl(273,70%,50%)","hsl(273,70%,50%)","hsl(273,70%,50%)","hsl(273,70%,50%)","hsl(273,70%,50%)","hsl(273,70%,50%)","hsl(273,70%,50%)","hsl(273,70%,50%)","hsl(273,70%,50%)","hsl(273,70%,50%)","hsl(273,70%,50%)","hsl(273,70%,50%)","hsl(273,70%,50%)","hsl(273,70%,50%)","hsl(273,70%,50%)","hsl(273,70%,50%)","hsl(273,70%,50%)","hsl(273,70%,50%)","hsl(273,70%,50%)","hsl(273,70%,50%)","hsl(273,70%,50%)","hsl(273,70%,50%)","hsl(273,70%,50%)","hsl(273,70%,50%)","hsl(273,70%,50%)","hsl(273,70%,50%)","hsl(273,70%,50%)","hsl(260,70%,50%)","hsl(260,70%,50%)","hsl(260,70%,50%)","hsl(260,70%,50%)","hsl(260,70%,50%)","hsl(260,70%,50%)","hsl(260,70%,50%)","hsl(260,70%,50%)","hsl(260,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(347,70%,50%)","hsl(148,70%,50%)","hsl(148,70%,50%)","hsl(148,70%,50%)","hsl(148,70%,50%)","hsl(148,70%,50%)","hsl(148,70%,50%)","hsl(148,70%,50%)","hsl(148,70%,50%)","hsl(148,70%,50%)","hsl(148,70%,50%)","hsl(148,70%,50%)","hsl(148,70%,50%)","hsl(148,70%,50%)","hsl(148,70%,50%)","hsl(148,70%,50%)","hsl(148,70%,50%)","hsl(148,70%,50%)","hsl(148,70%,50%)","hsl(148,70%,50%)","hsl(148,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(297,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(12,70%,50%)","hsl(173,70%,50%)","hsl(173,70%,50%)","hsl(173,70%,50%)","hsl(173,70%,50%)","hsl(173,70%,50%)","hsl(173,70%,50%)","hsl(173,70%,50%)","hsl(173,70%,50%)","hsl(173,70%,50%)","hsl(173,70%,50%)","hsl(173,70%,50%)","hsl(173,70%,50%)","hsl(173,70%,50%)","hsl(173,70%,50%)","hsl(173,70%,50%)","hsl(173,70%,50%)","hsl(173,70%,50%)","hsl(173,70%,50%)","hsl(173,70%,50%)","hsl(173,70%,50%)","hsl(173,70%,50%)","hsl(173,70%,50%)","hsl(173,70%,50%)","hsl(173,70%,50%)","hsl(173,70%,50%)","hsl(173,70%,50%)","hsl(173,70%,50%)","hsl(173,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(24,70%,50%)","hsl(223,70%,50%)","hsl(223,70%,50%)","hsl(223,70%,50%)","hsl(223,70%,50%)","hsl(223,70%,50%)","hsl(223,70%,50%)","hsl(223,70%,50%)","hsl(223,70%,50%)","hsl(223,70%,50%)","hsl(223,70%,50%)","hsl(223,70%,50%)","hsl(223,70%,50%)","hsl(223,70%,50%)","hsl(223,70%,50%)","hsl(223,70%,50%)","hsl(223,70%,50%)","hsl(223,70%,50%)","hsl(223,70%,50%)","hsl(223,70%,50%)","hsl(223,70%,50%)","hsl(186,70%,50%)","hsl(186,70%,50%)","hsl(186,70%,50%)","hsl(186,70%,50%)","hsl(186,70%,50%)","hsl(186,70%,50%)","hsl(186,70%,50%)","hsl(186,70%,50%)","hsl(186,70%,50%)","hsl(186,70%,50%)","hsl(186,70%,50%)","hsl(186,70%,50%)","hsl(186,70%,50%)","hsl(186,70%,50%)","hsl(186,70%,50%)","hsl(186,70%,50%)","hsl(186,70%,50%)","hsl(186,70%,50%)","hsl(186,70%,50%)","hsl(186,70%,50%)","hsl(186,70%,50%)","hsl(186,70%,50%)","hsl(186,70%,50%)","hsl(111,70%,50%)","hsl(111,70%,50%)","hsl(111,70%,50%)","hsl(111,70%,50%)","hsl(111,70%,50%)","hsl(111,70%,50%)","hsl(111,70%,50%)","hsl(111,70%,50%)","hsl(111,70%,50%)","hsl(111,70%,50%)","hsl(111,70%,50%)","hsl(111,70%,50%)","hsl(111,70%,50%)","hsl(111,70%,50%)","hsl(111,70%,50%)","hsl(111,70%,50%)","hsl(111,70%,50%)","hsl(111,70%,50%)","hsl(111,70%,50%)","hsl(111,70%,50%)","hsl(111,70%,50%)","hsl(111,70%,50%)","hsl(111,70%,50%)","hsl(111,70%,50%)","hsl(111,70%,50%)","hsl(111,70%,50%)","hsl(111,70%,50%)","hsl(111,70%,50%)","hsl(111,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(124,70%,50%)","hsl(310,70%,50%)","hsl(310,70%,50%)","hsl(310,70%,50%)","hsl(310,70%,50%)","hsl(310,70%,50%)","hsl(310,70%,50%)","hsl(310,70%,50%)","hsl(310,70%,50%)","hsl(310,70%,50%)","hsl(310,70%,50%)","hsl(310,70%,50%)","hsl(310,70%,50%)","hsl(310,70%,50%)","hsl(310,70%,50%)","hsl(310,70%,50%)","hsl(310,70%,50%)","hsl(310,70%,50%)","hsl(310,70%,50%)","hsl(310,70%,50%)","hsl(310,70%,50%)","hsl(310,70%,50%)","hsl(310,70%,50%)","hsl(310,70%,50%)","hsl(310,70%,50%)","hsl(310,70%,50%)","hsl(310,70%,50%)","hsl(310,70%,50%)","hsl(310,70%,50%)","hsl(310,70%,50%)","hsl(310,70%,50%)","hsl(310,70%,50%)","hsl(310,70%,50%)","hsl(310,70%,50%)","hsl(235,70%,50%)","hsl(235,70%,50%)","hsl(235,70%,50%)","hsl(235,70%,50%)","hsl(235,70%,50%)","hsl(235,70%,50%)","hsl(235,70%,50%)","hsl(235,70%,50%)","hsl(235,70%,50%)","hsl(235,70%,50%)","hsl(235,70%,50%)","hsl(235,70%,50%)","hsl(235,70%,50%)","hsl(235,70%,50%)","hsl(235,70%,50%)","hsl(235,70%,50%)","hsl(235,70%,50%)","hsl(235,70%,50%)","hsl(235,70%,50%)","hsl(235,70%,50%)","hsl(235,70%,50%)","hsl(198,70%,50%)","hsl(198,70%,50%)","hsl(198,70%,50%)","hsl(198,70%,50%)","hsl(198,70%,50%)","hsl(198,70%,50%)","hsl(198,70%,50%)","hsl(198,70%,50%)","hsl(198,70%,50%)","hsl(198,70%,50%)","hsl(198,70%,50%)","hsl(198,70%,50%)","hsl(198,70%,50%)","hsl(198,70%,50%)","hsl(198,70%,50%)","hsl(198,70%,50%)","hsl(198,70%,50%)","hsl(198,70%,50%)","hsl(198,70%,50%)","hsl(198,70%,50%)","hsl(198,70%,50%)","hsl(198,70%,50%)","hsl(198,70%,50%)","hsl(198,70%,50%)","hsl(198,70%,50%)","hsl(198,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(0,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(322,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(62,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(37,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(49,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)","hsl(161,70%,50%)"],"opacity":0.7,"size":8},"mode":"markers","text":["\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eCancer is the name of a collection of related diseases. Specifically, all cancers undergo an uncontrolled proliferation of the patient cells, which spread into surrounding tissues. In a normal organis...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eThis abnormal behavior occurs as consequence of the alteration of crucial genes. These alterations can be inherited from our parents, or acquired during our lifetime due to replication errors or expos...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eBreast cancer occurs when breast cells undergo this uncontrolled proliferation. In most of the cases they begin in the ducts that carry the milk to the nipple. However the tumor can originate in other...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eBreast cancer is the second most commonly diagnosed cancer among women, after non-melanoma skin cancer...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eWe find the highest incidence in Western countries, and it has increased in the last decades...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eTwo proposed explanations are more diagnoses due to mammography screening, or more cases due to the generalization of hormone treatments for menopause...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eEncouragingly, the prognosis is better every year (5y survival rate \u003e 80% in France) thanks to improvements in treatments and better screening...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eAmong the most important risk factors for breast cancer we can highlight age, family history, reproductive history, usage of oral contraceptives, overweight, physical activity and exposure to radiatio...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eBreast cancer is a very heterogeneous disease: while all the tumors appear in the same organ, the tissue where they originate, the molecular mechanism involved, the response to therapy, etc...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eIn general, clinical decisions are based on the expression of 3 molecular markers: the expression of the endocrine receptors for estrogen and progesterone (ER and PgR, respectively) and the overexpres...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eBased on the results, we distinguish three main breast cancer subtypes: hormone receptor positive, HER2 positive and triple negative....","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eHormone receptor positive tumors include the tumors expressing ER and\u002for PR, which respectively depend on estrogen and\u002for progesterone to grow. They happen mostly in postmenopausal women. HR+\u002fHER2- al...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eHER2+ tumors depend on the protein HER2\u002fneu (human epidermal growth factor receptor 2) to proliferate, which they over-express. HR+\u002fHER2+ (also known as LuminalB) constitutes 10% of the cases, while H...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eTriple-negative tumors (also known as basal-like) lack the expression of all three of ER, PgR and HER2. These patients present a worse prognosis than the rest, due to the aggressiveness of the tumor a...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eRoutine screening for breast cancer has been set up in the last decades in many countries. After it was introduced we observed a mortality reduction of 30%, justifying its wide implementation. It cons...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eDespite the aforementioned success, there is a big controversy regarding mammography programs usefulness...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eThat is because, in practical terms, the 30% decrease in mortality means that for every 1000 women examined, we will save 7-9 lives...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eOn the other hand, it will provoke with 4 cases of overdiagnosis, tumors that would have never progressed to symptomatic presentation during the lifetime of the woman...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eThat, and many more cases which will require further, possibly invasive, testing to make the final (negative) diagnosis...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eThe proposed solution to this involves adapting the screening process to different demographic groups, so that high risk groups are closely watched while low risk groups loosely so....","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eIn the mid-19th century a French medical doctor, Pierre Paul Broca, reported for the first time a case of familial breast cancer. Indeed, his wife acquired breast cancer, as many women in her family h...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eIt wasn\u2019t until the late 20th century that two genes involved in DNA repair, BRCA1 and BRCA2, were associated with hereditary breast and ovarian cancer (HBOC). Some mutations in these genes increase t...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eHBOC follows an autosomal dominant inheritance pattern. While approximately 5\u201310% of all patients with breast cancer exhibit a monogenic predisposition to breast and ovarian cancer, only about 25% of ...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eTable 1. Overview of HBOC genes: estimated lifetime risk of breast cancer (age in years) and tumorogenic molecular mechanisms that involves them: homologous recombination repair (HRR), replication for...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e| **Gene** | **Breast cancer estimated lifetime risk (age in years)** | **HRR** | **Replication fork stability** | **Transcription\u2013replication collisions** | **MMR** | **DNA damage signaling, checkpoi...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e| :------: | :------------------------------------------------------: | :-----: | :----------------------------: | :--------------------------------------: | :-----: | :-------------------------------...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e|   ATM    |                      60% by age 80                       |    \u2713    |                                |                                          |         |                          \u2713      ...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e|  BARD1   |                         Unknown                          |    \u2713    |                                |                                          |         |                                 ...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e|   BLM    |                         Unknown                          |         |               \u2713                |                                          |         |                                 ...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e|  BRCA1   |                     57\u201365% by age 70                     |    \u2713    |               \u2713                |                    \u2713                     |         |                          \u2713      ...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e|  BRCA2   |                     45\u201355% by age 70                     |    \u2713    |               \u2713                |                    \u2713                     |         |                          \u2713      ...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e|  BRIP1   |                         OR: \u003c2.0                         |         |                                |                                          |         |                                 ...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e|   CDH1   |                      42% by age 80                       |         |                                |                                          |         |                                 ...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e|  CHEK2   |                      37% by age 70                       |         |                                |                                          |         |                          \u2713      ...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e| FAM175A  |                         Unknown                          |    \u2713    |                                |                                          |         |                                 ...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e|  FANCC   |                         Unknown                          |         |               \u2713                |                                          |         |                                 ...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e|  FANCM   |                         Unknown                          |         |               \u2713                |                                          |         |                                 ...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e|   MLH1   |                      ~19% by age 70                      |         |               \u2713                |                                          |    \u2713    |                                 ...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e|  MRE11   |                         Unknown                          |         |                                |                                          |    \u2713    |                                 ...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e|   MSH2   |                      ~11% by age 70                      |         |                                |                                          |    \u2713    |                                 ...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e|   NBN    |                         OR: 3.0                          |    \u2713    |                                |                                          |         |                                 ...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e|   NF1    |          6.5-fold increase in women aged 30\u201339           |         |                                |                                          |         |                                 ...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e|  PALB2   |                      35% by age 70                       |    \u2713    |               \u2713                |                                          |         |                                 ...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e|   PMS2   |                         SIR: 3.8                         |         |                                |                                          |    \u2713    |                                 ...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e|   PTEN   |                      85% by age 70                       |         |                                |                                          |         |                                 ...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e|  RAD51B  |                         Unknown                          |    \u2713    |                                |                                          |         |                                 ...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e|  RAD51C  |                         Unknown                          |    \u2713    |                                |                                          |         |                                 ...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e|  RAD51D  |                         Unknown                          |    \u2713    |                                |                                          |         |                                 ...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e|  RECQL   |                         Unknown                          |         |               \u2713                |                                          |         |                                 ...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e|  RINT1   |                         Unknown                          |    \u2713    |                                |                                          |         |                                 ...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e|  STK11   |                      32% by age 60                       |         |                                |                                          |         |                                 ...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e|   TP53   |                      25% by age 70                       |         |                                |                                          |         |                          \u2713      ...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eNearly all known HBOC susceptibility genes encode tumor suppressors that participate in genome stability pathways (homologous recombination repair, replication fork stability, transcription\u2013replicatio...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eThe homologous recombination repair pathway (HRR) deals with double strand DNA breaks by using the undamaged chromosome as template for error-free repair. After a DSB occurs, the MRN complex (MRE11, R...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eHRR involves BRCA1, BRCA2 and, actually, most of the HBOC genes. Because of its ability to interact with a wide range of proteins, BRCA1 is hypothetized to act as a recruitment scaffold. A deficiency ...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eBRCA1 and BRCA2 protect newly synthesized DNA and promote the restart of stalled forks in an HRR-independent manner. In the absence of these proteins, newly synthesized DNA in a stalled fork would get...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eCollisions between transcription and replication are emerging as a source of genome instability. In particular, RNA-DNA hybrids called R-loops can form between the nascent transcript and the DNA templ...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eDNA mismatch repair (MMR) corrects base-base mispairs. When MMR is faulty, accumulations point mutations and genetic changes in repeated nucleotide sequences (microsatellite instability) occur. MMR al...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003ePathways involved in genome maintenance, cell cycle checkpoints and cell death usually eliminate cells with damaged DNA. When proteins involved in them are not active, some processes such as cell cycl...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eDespite the identification of HBOC genes, 52% of the heritability or familial breast cancer remains unexplained: 20% is explained by high penetrance loci and an extra 28% by common variants...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eWe can illustrate this point with the largest genetic association in familial breast cancer so far, carried out in 2013 by Michailidou _et al._...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eIn a first stage, 10,052 breast cancer cases and 12,575 controls of European ancestry were genotyped using the iCOGS platform...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eThrough a GWAS, they selected 29,807 SNPs for further examination in a second stage on a larger cohort (45,290 cases and 41,880 controls)...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eDespite estimating that at least 1,000 uncorrelated loci were involved in breast cancer susceptibility, they were only able to get a genome wide significance (p \u003c 5 \u00d7 10\u003csup\u003e\u22128\u003c\u002fsup\u003e) for ~5% of them ...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e\u003e **iCOGS** is a custom Illumina array designed by four consortia that study genetic susceptibility of three hormone-related cancers: breast ([BCAC](http:\u002f\u002fccge.medschl.cam.ac.uk\u002fconsortia\u002fbcac\u002f) and ...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eIts explicit purpose is to facile the genotyping in large case-control studies for these tumors...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eThe criteria to include the SNPs was (i) previously associated with cancer susceptibility or survival; (ii) fine mapping of genomic regions of interest; (iii) associated with cancer-related quantitati...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003eGENESIS (GENE SISter) is a French project that aims to shed some light on familial breast cancer. The index cases are patients with a breast cancer affected sister and no BRCA1\u002f2 mutations. The contro...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e- PDQ\u00ae Cancer Genetics Editorial Board. PDQ Genetics of Breast and Gynecologic Cancers. Bethesda, MD: National Cancer Institute. Updated 30\u002f03\u002f2017. Available at: https:\u002f\u002fwww.cancer.gov\u002ftypes\u002fbreast\u002fh...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e- Foulkes, W. D. (2008). Inherited Susceptibility to Common Cancers. The New England Journal of Medicine, 359(20), 2143\u20132153. https:\u002f\u002fdoi.org\u002f10.1056\u002fNEJMra0802968...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e- Nielsen, F. C., van Overeem Hansen, T., & S\u00f8rensen, C. S. (2016). Hereditary breast and ovarian cancer: new genes in confined pathways. Nature Reviews Cancer, 16(9), 599\u2013612. https:\u002f\u002fdoi.org\u002f10.1038...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e- Michailidou, K., Hall, P., Gonzalez-Neira, A., Ghoussaini, M., Dennis, J., Milne, R. L., \u2026 Easton, D. F. (2013). Large-scale genotyping identifies 41 new loci associated with breast cancer risk. Nat...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e- Rudolph, A., Chang-claude, J., & Schmidt, M. K. (2016). Gene \u2013 environment interaction and risk of breast cancer. British Journal of Cancer, 114(2), 125\u2013133. https:\u002f\u002fdoi.org\u002f10.1038\u002fbjc.2015.439...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e- Sakoda, L. C., Jorgenson, E., & Witte, J. S. (2013). Turning of COGS moves forward findings for hormonally mediated cancers. Nature Genetics, 45(4), 345\u20138. https:\u002f\u002fdoi.org\u002f10.1038\u002fng.2587...","\u003cb\u003eFamilial breast cancer\u003c\u002fb\u003e\u003cbr\u003e- Sinilnikova, O. M., Dondon, M.-G., Eon-Marchais, S., Damiola, F., Barjhoux, L., Marcou, M., \u2026 Andrieu, N. (2016). GENESIS: a French national resource to study the missing heritability of breast canc...","\u003cb\u003eHeritability\u003c\u002fb\u003e\u003cbr\u003eI will try to explain what I know about heritability by using human height as example phenotype...","\u003cb\u003eHeritability\u003c\u002fb\u003e\u003cbr\u003eThis will be helpful not only because it is very tangible, but also because it is considered a \"model\" trait for quantitative genetics because it is easy to measure (Visscher, 2008)...","\u003cb\u003eHeritability\u003c\u002fb\u003e\u003cbr\u003eHeight can be modeled as the sum of the contributions of a unobserved genotype _G_ and an unobserved environment $$E$$: $$P = G + E$$ (Visscher et al., 2008)...","\u003cb\u003eHeritability\u003c\u002fb\u003e\u003cbr\u003eThis definition ignores both the genotype-environment covariation as well as genotype-environment interactions, which usually cannot be estimated...","\u003cb\u003eHeritability\u003c\u002fb\u003e\u003cbr\u003eAs before, we can decompose the variance in height $$\\sigma^2_P$$ as the fraction attributable to genetic factors and the fraction attributable to the environment:...","\u003cb\u003eHeritability\u003c\u002fb\u003e\u003cbr\u003e$$\\sigma^2_P$$ includes the total observed variation, usually excluding variation due to known fixed factors and covariates (age, sex, cohort...)....","\u003cb\u003eHeritability\u003c\u002fb\u003e\u003cbr\u003eHeritability is a term that aims to describe the proportion of the phenotypic variance that is explained by the genotype. The heritability of height was assessed more than a century ago, when Galton a...","\u003cb\u003eHeritability\u003c\u002fb\u003e\u003cbr\u003e* Narrow-sense heritability $$h^2$$: heritability is the proportion of the phenotypic variance that is explained by additive genetic effects:...","\u003cb\u003eHeritability\u003c\u002fb\u003e\u003cbr\u003e  which accounts for the genetic variability of additive genetic effects (A, also referred to as the breeding value); of dominance genetic effects (D, interactions between alleles at the same locus); ...","\u003cb\u003eHeritability\u003c\u002fb\u003e\u003cbr\u003eNarrow-sense heritability is the estimate that applies in most of situations, and hence, the most widely used. This is because non-additive effects such as dominance do not contribute to genotypic res...","\u003cb\u003eHeritability\u003c\u002fb\u003e\u003cbr\u003eHeritability is calculated from empirical data of related individuals, where we compare the expected and the observed resemblance...","\u003cb\u003eHeritability\u003c\u002fb\u003e\u003cbr\u003eTo estimate it, we need to calculate both the variance in additive genetic effects, and the phenotypic variance...","\u003cb\u003eHeritability\u003c\u002fb\u003e\u003cbr\u003eIt also must be taken into account that heritability is population specific, as both the variance in genetic factors and in environment are population-dependent...","\u003cb\u003eHeritability\u003c\u002fb\u003e\u003cbr\u003eHowever, we have found heritabilities in similar traits to be similar across populations and even species....","\u003cb\u003eHeritability\u003c\u002fb\u003e\u003cbr\u003eWhen it is possible, it is estimated in simple and balanced experiments. For example, by regressing the offspring on parental phenotypes, the correlation of full or half siblings, and the difference i...","\u003cb\u003eHeritability\u003c\u002fb\u003e\u003cbr\u003eIn the case of height, 80% of the intra-population variability in height is due to genetic factors ($$h^2$$ = 0.8) (Visscher, 2008)...","\u003cb\u003eHeritability\u003c\u002fb\u003e\u003cbr\u003eThree studies with a total sample size of 63k individuals (14k + 16k + 34k) identified 54 variants that are reliably associated with height...","\u003cb\u003eHeritability\u003c\u002fb\u003e\u003cbr\u003eHowever, they only get to explain 5% of the phenotypic variance (Manolio et al., 2009)...","\u003cb\u003eHeritability\u003c\u002fb\u003e\u003cbr\u003eThe rest of the estimated genetic component is something alike \"dark matter\": we know it is there, as we can see its effects, we just cannot see it nor know what it is exactly...","\u003cb\u003eHeritability\u003c\u002fb\u003e\u003cbr\u003eThis situation is actually common to many complex phenotypes, and has been called the missing heritability problem...","\u003cb\u003eHeritability\u003c\u002fb\u003e\u003cbr\u003eThere are several candidates out there to explain this dark matter: a larger than expected number of variants with mild effects (difficult to find in underpowered setting of GWAS); rarer variants with...","\u003cb\u003eHeritability\u003c\u002fb\u003e\u003cbr\u003eOf course, different traits will have different allelic architectures (number of variants, effects, type...)...","\u003cb\u003eHeritability\u003c\u002fb\u003e\u003cbr\u003eThe enigma of missing heritability is especially relevant when we deal with complex diseases....","\u003cb\u003eHeritability\u003c\u002fb\u003e\u003cbr\u003eOf the main suspects of missing heritability, variants not covered by standard experiment designs are regarded as the most likely cause. Although candidates like structural variants have been proposed...","\u003cb\u003eHeritability\u003c\u002fb\u003e\u003cbr\u003e- Bahcall, O. Common variation and heritability estimates for breast, ovarian and prostate cancers. Nature iCOGS. Retrieved from https:\u002f\u002fdoi:10.1038\u002fngicogs.1...","\u003cb\u003eHeritability\u003c\u002fb\u003e\u003cbr\u003e- Manolio, T. A., Collins, F. S., Cox, N. J., Goldstein, D. B., Hindorff, L. A., Hunter, D. J., \u2026 Visscher, P. M. (2009). Finding the missing heritability of complex diseases. Nature, 461(7265), 747\u20137...","\u003cb\u003eHeritability\u003c\u002fb\u003e\u003cbr\u003e- Visscher, P. M. (2008). Sizing up human height variation. Nature Genetics, 40(5), 489\u201390. https:\u002f\u002fdoi.org\u002f10.1038\u002fng0508-489...","\u003cb\u003eHeritability\u003c\u002fb\u003e\u003cbr\u003e- Visscher, P. M., Hill, W. G., & Wray, N. R. (2008). Heritability in the genomics era--concepts and misconceptions. Nature Reviews. Genetics, 9(4), 255\u201366. https:\u002f\u002fdoi.org\u002f10.1038\u002fnrg2322...","\u003cb\u003eFinding similar documents\u003c\u002fb\u003e\u003cbr\u003eHere we a...","\u003cb\u003eFinding similar documents\u003c\u002fb\u003e\u003cbr\u003eHere we address the problem of looking for similar, but not necessarily equal items. This is frequently the case of looking for related documents. For example, all the versions of the same news articl...","\u003cb\u003eFinding similar documents\u003c\u002fb\u003e\u003cbr\u003eIf we wanted to find equal documents, finding an efficient solution would be relatively straightforward: hash them and see which ones fall in the same bucket. In this case, we have to define a similar...","\u003cb\u003eFinding similar documents\u003c\u002fb\u003e\u003cbr\u003eThe Jaccard similarity between two sets $$A$$ and $$B$$ is defined by looking at the relative size of their intersection:...","\u003cb\u003eFinding similar documents\u003c\u002fb\u003e\u003cbr\u003eCosine distance is used in spaces that have dimensions, and hence we can think of points as non-zero vectors. In this case, the distance between two vectors $$x$$ and $$y$$ would be the angle between ...","\u003cb\u003eFinding similar documents\u003c\u002fb\u003e\u003cbr\u003eEdit distance can be used when points are strings. The distance between two strings $$x$$ and $$y$$ will be the smallest number of single-character insertions and deletions that would convert $$x$$ in...","\u003cb\u003eFinding similar documents\u003c\u002fb\u003e\u003cbr\u003eIn this problem, we will start from $$N$$ items (documents, images, etc.) which we represent in a $$D$$-dimensional space. An exhaustive approach is calculating all pairwise distances, but that would ...","\u003cb\u003eFinding similar documents\u003c\u002fb\u003e\u003cbr\u003e1. Shingling: convert documents into vectors\u002fsets.\n2. Minhashing: convert sets to signatures.\n3. Locality-sensitive hashing: make an estimation of the similarity....","\u003cb\u003eFinding similar documents\u003c\u002fb\u003e\u003cbr\u003eA $$k$$-shingle of a document is the set of any substring of length $$k$$ that appears in the document...","\u003cb\u003eFinding similar documents\u003c\u002fb\u003e\u003cbr\u003eOr, instead of directly using substrings as shingles, we can apply a hash function that maps strings of length $$k$$ to a number of buckets, representing the data more compactly while conserving most ...","\u003cb\u003eFinding similar documents\u003c\u002fb\u003e\u003cbr\u003eSo it's possible to map every shingle the lower dimensionality space represented by the buckets....","\u003cb\u003eFinding similar documents\u003c\u002fb\u003e\u003cbr\u003eThe choice of $$k$$ is not trivial, and mostly depends on how long are usually the documents we are dealing with. Hence, $$k$$ must be chosen so that the probability of any given shingle appearing in ...","\u003cb\u003eFinding similar documents\u003c\u002fb\u003e\u003cbr\u003eThere are two kind of special elements in the text that must be dealt with: white spaces - tabs, spaces... - and _stop words_ - the most common words. Stop words are specially interesting because, whi...","\u003cb\u003eFinding similar documents\u003c\u002fb\u003e\u003cbr\u003e(Sets contain unique, unordered elements; multisets or bags are generalisations of sets which may contain several instances of the same element.)...","\u003cb\u003eFinding similar documents\u003c\u002fb\u003e\u003cbr\u003eIn the exhaustive approach, we could already take the shingle-sets and calculate the Jaccard index between all pairs, still a problem with $$O(N^2)$$ complexity. Additionally, shingle-sets are large. ...","\u003cb\u003eFinding similar documents\u003c\u002fb\u003e\u003cbr\u003eWe will do so through a process called minhashing. To understand it, we can imagine all our shingle-sets represented it as a characteristic matrix $$C$$, although our data most probably won't be repre...","\u003cb\u003eFinding similar documents\u003c\u002fb\u003e\u003cbr\u003e1. Generate a random permutation of the rows of $$C$$.\n1. For each column $$c$$\u002fdocument:\n   1. Get the first row $$k$$ that has a 1.\n   1. Fill the column $$c$$ of the signature matrix M with $$k$$....","\u003cb\u003eFinding similar documents\u003c\u002fb\u003e\u003cbr\u003eWe build a new representation of our set $$S$$ as a vector $$[h_1(S), h_2(S), ..., h_n(S)]$$. This is the only representation we will use to study similarity. Minhashing exploits an interesting proper...","\u003cb\u003eFinding similar documents\u003c\u002fb\u003e\u003cbr\u003eIn practice, permuting a huge matrix is very time consuming, and it is not feasible in real-world applications. Instead, $$n$$ hash functions $$h_i$$ are selected, which are applied on the row number....","\u003cb\u003eFinding similar documents\u003c\u002fb\u003e\u003cbr\u003eWe calculate $$M$$, which has a complexity of $$N$$. We will pick the documents that are similar enough on the M matrix as candidates to similar documents. Then, we will compute exactly the Jaccard si...","\u003cb\u003eFinding similar documents\u003c\u002fb\u003e\u003cbr\u003eWe do that by binning the rows into bands, and the compare the documents inside each band. We will get how many bands are exactly the same between two documents....","\u003cb\u003eFinding similar documents\u003c\u002fb\u003e\u003cbr\u003eDepending on the choice of $$r$$ and $$t$$, we modulate the curve. Hence we get to the sensitivity of our curve....","\u003cb\u003eFinding similar documents\u003c\u002fb\u003e\u003cbr\u003e- Leskovec, J., Rajaraman, A., & Ullman, J. D. (2014). Mining of Massive Datasets (3rd ed.). Cambridge University Press. Retrieved from http:\u002f\u002fi.stanford.edu\u002f~ullman\u002fmmdsn.html...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003eA key question in biology is whether and...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003eA key question in biology is whether and how an exposure, like the protein expression of a gene, causes a complex phenotype. However, using RCTs to study this issue would be quite challenging technica...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003eInterestingly, [genetic variants](https:\u002f\u002fen.wikipedia.org\u002fwiki\u002fSingle-nucleotide_polymorphism) are akin to a randomized treatment when comparing siblings...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003eWhich allele each sibling gets is randomized at conception and fixed throughout life, making them unaffected by confounders...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003eWhile there are exceptions ([linkage disequilibrium](https:\u002f\u002fen.wikipedia.org\u002fwiki\u002fLinkage_disequilibrium) and [population structure](\u003chttps:\u002f\u002fen.wikipedia.org\u002fwiki\u002fPopulation_structure_(genetics)\u003e) t...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003eHowever, many observational studies are carried out in the general population, where the distribution of variants is not completely randomized...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003eFor instance, due to assortative mating, people with similar heritable phenotypes will tend to.....","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003emate...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003eHowever, even in the general population and as long as [certain assumptions are met](#key-assumptions-and-how-to-verify-them) the distribution of SNPs mimics a random assignment...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003eMendelian Randomization (MR) exploits this fact to estimate the causal effect of an exposure on an outcome using genetic variants, often SNPs....","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003eRemember: our goal is to precisely estimate the effect of an exposure on a trait. However, the presence of confounders that are either unobserved or difficult to measure makes this impossible. An inst...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003e```mermaid\n---\nconfig:\n    layout: elk\n    look: handDrawn\n---\ngraph TD\n    Z[Instrument] --\u003e X[Exposure];\n    X --\u003e Y[Trait];\n    U[Confounder] --\u003e X;\n    U --\u003e Y;...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003e    style Z fill:#D4EDDA,stroke:#28A745,stroke-width:2px,rx:8px,ry:8px;\n    style X fill:#CCE5FF,stroke:#007BFF,stroke-width:2px,rx:8px,ry:8px;\n    style Y fill:#FFF3CD,stroke:#FFC107,stroke-width:2px...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003e    %% Dashed lines for the \"no direct effect\"\n    linkStyle 4 stroke-dasharray: 5 5;\n    linkStyle 5 stroke-dasharray: 5 5;\n```...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003eSince there are no backdoor paths between the SNP and the outcome, any effect between them must occur through the exposure....","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003eMore formally, we aim to estimate the effect of an exposure $$X$$ on an outcome $$Y$$ using an instrument $$Z$$. $$Z$$ is a SNP. When SNPs are di-allelic, and since our focus is (diploid) humans, they...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003e1. Estimate the relationship between the instrument and the exposure by solving $$X = \\beta_{ZX}Z + \\varepsilon$$ via a [GWAS](https:\u002f\u002fen.wikipedia.org\u002fwiki\u002fGenome-wide_association_study). For instanc...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003eMathematically, this is usually performed via [two-stage least squares](https:\u002f\u002fmr-dictionary.mrcieu.ac.uk\u002fterm\u002ftsls\u002f). Alternatively, we can obtain $$\\beta_{\\hat{X}Y}$$ without computing it explicitl...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003eMR relies on three assumptions (_relevance_, _independence_ and _exclusion restriction_), described below. Other common, but optional, assumptions are _linearity_ and _homogeneity_ (the effect does no...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003eThe instrument is _strongly_ associated with the exposure. When this assumption is violated, we say that the instrument is a weak instrument. The strength of this association can be measured using (Cr...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003ewhere $$n$$ is the number of samples, $$m$$ is the number of instruments, and $$R^2$$ is the instrument heritability. Conventionally, an instrument is considered _relevant_ when $$F \u003e 10$$....","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003eThe instrument is not associated with the outcome through a confounder, measured or not. Common violations include [assortative mating](https:\u002f\u002fmr-dictionary.mrcieu.ac.uk\u002fterm\u002fassortative-mating\u002f), [p...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003eThe instrument is _exclusively_ associated to the outcome through the exposure. [(Horizontal) pleiotropy](https:\u002f\u002fmr-dictionary.mrcieu.ac.uk\u002fterm\u002fhorizontal-pleiotropy\u002f) and linkage with other causal ...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003e[We just saw](#enter-mendelian-randomization) the simplest version of MR; but there are many algorithms to carry it out, depending on what data we have access to. I classify them here according to mul...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003eWe can conduct MR with varying **numbers of datasets**. In _one-sample MR_, the instrument, exposure and outcome are measured on the same subjects. This is the case of the [two-stage least squares](#e...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003eOne-sample MR is the ideal setting, since all the statistics are derived from the same dataset...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003eHowever, few datasets are rich enough to allow us to quantify effect sizes from the exposure and the outcome...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003eFor instance, we could measure the effect size of the instrument on gene expression on [GTEx](https:\u002f\u002fwww.gtexportal.org), and on the outcome on a [publicly available GWAS](https:\u002f\u002fwww.ebi.ac.uk\u002fgwas\u002f...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003eAs an attention note, all summary statistics need to come from ancestry-matched samples to preserve the [independence](#independence) assumptions....","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003eA single SNP offers simplicity but is likely to have weak association with the exposure...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003eWhile multiple SNPs can be a stronger instrument, they introduce challenges related to linkage disequilibrium and the need for conditional independence...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003eIn such a case, prior feature selection or dimensionality reduction techniques may be necessary to address these issues and ensure numerical stability...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003eIn the extreme case we could use a polygenic score (PGS), a weighted sum of multiple SNPs derived from a GWAS of the exposure, to capture a larger proportion of the variance in the exposure...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003eBeware, though, of the potential biases lurking in the GWAS weights and the lingering effects of linkage disequilibrium....","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003eThe **location of the SNPs** can be relevant when examining molecular traits like gene expression. In _cis-MR_ we use SNPs located in the coding region of the target gene, or very nearby. This proximi...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003eWhile I have focused on the case in which we deal with a single exposure (_univariable MR_), there are cases in which we use **multiple exposures** (_Multivariable MR_ or MVMR)....","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003eThere are multiple **types of exposures**. Some can be relatively simple phenotypes close to the genetics, like gene expression. Others can be complex, like the body mass index. There are also many in...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003e| Goal                         | Exposure                                    | Instrument                               | MR Flavor                                 |...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003e| ---------------------------- | ------------------------------------------- | ---------------------------------------- | ----------------------------------------- |...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003e| Learn causes of a trait      | Simple or complex                           | One or multiple exposure-associated SNPs | Univariate MR                             |...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003e| Characterize causal pathways | Multiple exposures, possibly with mediators | Independent SNPs for all exposures       | MVMR to model joint\u002fmediated effects      |...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003e| Find drug targets            | Protein activity or biomarker               | cis-pQTLs (preferred) or trans-pQTLs     | Cis-MR (preferred) or trans-MR            |...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003e| Identify relevant tissues    | Tissue-specific gene expression             | eQTLs stratified by tissue               | Tissue-Specific MVMR to partition effects |...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003e- Hartley, A. E., Power, G. M., Sanderson, E., & Smith, G. D. (2022). [A Guide for Understanding and Designing Mendelian Randomization Studies in the Musculoskeletal Field](https:\u002f\u002fdoi.org\u002f10.1002\u002fjbm...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003e- Burgess, S., Mason, A. M., Grant, A. J., Slob, E. A. W., Gkatzionis, A., Zuber, V., Patel, A., Tian, H., Liu, C., Haynes, W. G., Hovingh, G. K., Knudsen, L. B., Whittaker, J. C., & Gill, D. (2023). ...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003e- Teumer, A. (2018). [Common Methods for Performing Mendelian Randomization](https:\u002f\u002fdoi.org\u002f10.3389\u002ffcvm.2018.00051). Frontiers in Cardiovascular Medicine (Vol. 5)....","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003e- Holmes, M. V., Richardson, T. G., Ference, B. A., Davies, N. M., & Davey Smith, G. (2021). [Integrating genomics with biomarkers and therapeutic targets to invigorate cardiovascular drug development...","\u003cb\u003eMendelian randomization\u003c\u002fb\u003e\u003cbr\u003e- Sanderson, E. (2021). [Multivariable Mendelian Randomization and Mediation](https:\u002f\u002fdoi.org\u002f10.1101\u002fcshperspect.a038984). Cold Spring Harbor Perspectives in Medicine (Vol. 11, Issue 2)....","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eThe commonly taught approach to AI is **model-centric**: the data is assumed to be fixed and perfect, and the challenge is producing the best model. However this contrast with the real world applicati...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003e- Algorithms that understand the data and leverage that to improve the models. E.g., train on easy data first (curriculum learning).\n- Algorithms that modify the data. E.g., train on filtered datasets...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003e- Given a dataset, produce the best model\n- Change the model to improve performance (hyperparameters, loss function, etc.)...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003e- Given a model, improve the training set\n- Systematically\u002falgorithmically change the dataset...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eA motivating example is the kNN algorithm. It has no loss function, the prediction is simply a majority vote, and the quality of the prediction depends on the quality of the data. Another one is DALL-...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eNote that data-centric AI is not about hand-picking good datapoints or getting more data. Some examples are:...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003e- Outlier detection and removal: handling abnormal examples\n- Error detection and correction: handling incorrect examples\n- Data augmentation: adding examples that encode prior knowledge\n- Feature eng...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eEach training example in our dataset consists on a feature vector $$\\mathbf x$$, and an observed (noisy) label $$\\tilde y$$. Note that this label might not equal the latent true label $$y^\\ast$$. This...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003e1. Correctable mis-classification: $$\\tilde y$$ is wrong, but there is only one $$y^\\ast$$.\n2. Multi-label: multiple $$y^\\ast$$ are correct.\n3. Neither (potentially _out of distribution_): the $$y^\\as...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eWe will tackle the first case using the **confident learning** framework. This label noise can occur in three ways:...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003e- Uniform: $$p\\left(\\tilde{y}=i \\mid y^*=j\\right)=\\epsilon, \\forall i \\neq j$$\n- Systematic: $$p\\left(\\tilde{y}=i \\mid y^*=j\\right)$$ follows any valid distribution\n- Instance-dependent: $$p\\left(\\til...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eOn the other hand, second form is a compromise between what is realistic and what is tractable, and will be our focus...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eIn other words, we assume that $$p\\left(\\tilde{y}=i \\mid y^\\ast =j, \\boldsymbol{x}\\right) = p\\left(\\tilde{y}=i \\mid y^\\ast =j\\right)$$, and hence the label noise only depends on the original class...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eFor example, a _fox_ is much more likely to be mislabeled as a _dog_ than as a _banana_, but any _fox_ is equally likely to be mislabeled as a _dog_...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003e\u003caside\u003e\n\ud83d\udca1 **Confident learning** is a data-centric and model-agnostic framework. In other words, it allows to use **any** model\u2019s predicted probability to find label errors. It is implemented in the P...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003e\u003c\u002faside\u003e...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eConfident learning aims to estimate $$p\\left(\\tilde{y} \\mid y^\\ast \\right)$$ and $$p\\left(y^\\ast\\right)$$...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eIt does so in one step by estimating the joint distribution $$p\\left(\\tilde{y} , y^\\ast \\right)$$ as a contingency table that counts the frequency of samples with each label, correct or incorrect, ass...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eIn the absence of **both** model errors and label errors, this would produce a diagonal matrix...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eAnd in the absence of label errors only, off-diagonals would correspond to the mislabeled examples...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003e\u003caside\u003e\n\ud83d\udca1 A naive way to find label errors is to sort by the loss function (assuming no overfitting), i.e., the examples with the highest loss are likely to be label errors. However, how do we pick an...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003e\u003c\u002faside\u003e...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eIn order to estimate $$p\\left(\\tilde{y}, y^*\\right)$$, confident learning requires two inputs: the noisy labels $$\\tilde y$$ and the (out-of-sample) predicted probabilities $$\\hat p\\left(\\tilde{y} = i...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eThe difficulty lies in assigning latent labels to the examples. How to know which examples are mislabeled? The key insight is finding a threshold for the algorithm\u2019s confidence on the prediction. That...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003e$$\nt_j=\\frac{1}{\\left|\\boldsymbol{X}_{\\tilde{y}=j}\\right|} \\sum_{\\boldsymbol{x} \\in \\boldsymbol{X}_{\\tilde{y}=j}} \\hat{p}(\\tilde{y}=j ; \\boldsymbol{x}, \\boldsymbol{\\theta})\n$$...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eIn words, if a class is very confidently predicted on average, we will request every example to be predicted with a very high confidence....","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eFor each example in the **out-of-sample** data, we will take the highest probability class, and compare it to its respective threshold $$t_j$$. If the model is not that confident in the class, we will...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003e\u003caside\u003e\n\ud83d\udca1 This computation is robust to outliers: examples that are not confidently predicted for any of the classes will be dropped from the contingency table....","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003e\u003c\u002faside\u003e...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003e1. Estimate noise via counting\n2. Clean the data: rank & prune\n3. Re-train with the errors removed...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eOur ultimate goal is to estimate the joint distribution of true and noisy labels, $$\\boldsymbol{ Q_{\\tilde y, y^\\ast}}$$....","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eThe first step is computing a **confident joint** table $$\\boldsymbol{ C_{\\tilde y, y^\\ast} } \\in \\mathbf N^{m \\times m}$$. Diagonal elements count likely correct labels, and non-diagonals count likel...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003e$$\n\\boldsymbol{C}_{\\tilde{y}, y^*}[i][j]:=\\left|\\hat{\\boldsymbol{X}}_{\\tilde{y}=i, y^*=j}\\right|\n$$...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003ewhere...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003e$$\n\\hat{\\boldsymbol{X}}_{\\tilde{y}=i, y^*=j}:=\\left\\{\\boldsymbol{x} \\in \\boldsymbol{X}_{\\tilde{y}=i}: \\hat{p}(\\tilde{y}=j ; \\boldsymbol{x}, \\boldsymbol{\\theta}) \\geq t_j, \\underset{l \\in[m]: \\hat{p}(\\...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003e$$\nt_j=\\frac{1}{\\left|\\boldsymbol{X}_{\\tilde{y}=j}\\right|} \\sum_{\\boldsymbol{x} \\in \\boldsymbol{X}_{\\tilde{y}=j}} \\hat{p}(\\tilde{y}=j ; \\boldsymbol{x}, \\boldsymbol{\\theta})\n$$...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eNote that this goes against the common assumption that the true label is $$\\tilde y_k = \\operatorname*{argmax}_{i \\in [m]} \\hat{p}(\\tilde{y}=j ; \\boldsymbol{x}, \\boldsymbol{\\theta})$$. Instead we comp...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eThe next step is using $$\\boldsymbol{ C_{\\tilde y, y^\\ast} }$$ to estimate $$\\boldsymbol{ Q_{\\tilde y, y^\\ast} }$$:...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003e$$\n\\hat{\\boldsymbol{Q}}_{\\tilde{y}=i, y^*=j}=\\frac{\\frac{\\boldsymbol{C}_{\\tilde{y}=i, y^*=j}}{\\sum_{j \\in[m]} \\boldsymbol{C}_{\\tilde{y}=i, y^*=j}} \\cdot\\left|\\boldsymbol{X}_{\\tilde{y}=i}\\right|}{\\sum_...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eGiven $$\\boldsymbol{ C_{\\tilde y, y^\\ast} }$$ and $$\\boldsymbol{ Q_{\\tilde y, y^\\ast} }$$, any rank and prune approach can be used to clean the data....","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eA data-related issue in machine learning is **selection bias**. This happens when the training and the real-world distributions do not match. This can happen for several reasons....","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003e| **Reason**                                          | **Validation set**         |\n| --------------------------------------------------- | -------------------------- |\n| **Time:** the training data ...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eSelection bias can lead to **spurious correlations**, in which our models pick up \u201cshortcuts\u201d, rather than important features. To avoid this, we will pick the **validation data** which most closely re...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eSay we want to achieve 95% classification accuracy. What is the required sample size? To find out, we can train our model on increasingly lager subsamples to study how performance improves, then extra...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eOur labels will often come from a team of annotators. Hence, each training example might have multiple, sometimes contradicting annotations. Moreover, not every annotator sees every sample. We can dec...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003e- Consensus label: pick the single, best label\n- Confidence in the consensus label: how confident we are in that label. That will depend on:\n  - The number of annotations for that example\n  - The disa...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eConfidence score: fraction of annotators agreeing on the majority label. Out of those who emitted a label for that training example....","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eQuality of the annotator: fraction of the annotations that match the majority label....","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eProblems: it won\u2019t handle well examples with a single annotation examples (low confidence); it does not account for ties, or for the quality of the annotators....","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eSolution: train a classifier, which predicts each class, and treat it as a separate annotator. We learn on the majority vote annotator. This solves:...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003e- Ties: are broken by the classifier\n- Single-annotations: believe the label if the model is also very confident in it...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eSolution: learn the probability distribution of what the true label should be given the example and all the annotations. This will be a weighted average of all the annotations from both the annotators...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eCrucially, this requires identifying the relevant evaluation metric(s) to improve...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eSuch evaluation metrics (or, simply, metrics) measure prediction of the model on an _unseen_ example by comparing it to its given label...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003e(Although it is ideal to use multiple metrics, multi-objective problems are hard to tackle in practice.) The metric might use the **most likely** predicted class (e.g., accuracy, balanced accuracy, pr...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eIn practice, we will compute this metric for multiple unseen examples, then aggregate them by:...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003e- Averaging the metrics obtained over all the examples\n- Averaging the metrics obtained over all the examples _in each class_ (e.g., per-class accuracy)\n- Computing the confusion matrix. Although attr...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003e\u003e \ud83d\udca1 Invest as much time thinking about model evaluation as about the models themselves. It has **huge** impact in real world applications. I list some common pitfalls below....","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003e- Data leakage: we fail to use truly held-out data. This happens, for instance when we use data from multiple data sources which contain the same training examples.\n- Misspecified metric: reporting on...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eA **data slice** is a subset of the data that shares a common characteristic (a.k.a., cohort, subpopulation or subgroup), like race, gender, age or socioeconomics. Although it might be tempting to exc...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eIdeally, model predictions should not change across slices. When that does not happen, we can tackle that in different ways:...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003e1. Use a more flexible model, that can capture more complex relationships\n2. Over-sample\u002fup-weight the examples from the minority group receiving poor predictions\n3. Collect additional data from the g...","\u003cb\u003eData-centric machine learning\u003c\u002fb\u003e\u003cbr\u003eIn any way, the first step is to find out the subpopulations on which we are underperforming....","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003e\u003e Note: not _everything_ in...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003eEvery Python object has three builtin properties (a reference, a class, and a refcount) as well as additional, class-specific properties....","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003eA **reference** is a pointer, a way to access the memory address that stores the object. It can be associated to a name, or an element in a collection:...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003e```python\n# create a new integer object, and\n# copy its reference to the name \"a\"\na = 1...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003e# create a new integer object, and\n# append its reference to the list \"x\"\nx = list()\nx.append(1)\n```...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003e\u003e The assignment operator (`=`) just copies the reference to an object, not the object itself. Similarly, the deletion keyword (`del`) never deletes an object, just the reference to it....","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003e# same reference?\nassert x is x_copy\nassert id(x) == id(x_copy)\nassert x is not y...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003eA **class** is the type of the object (e.g., a float, or a string). Each object contains a pointer to its class, [as we will see below](#the-two-dictionaries-underlying-an-object). We can retrieve an ...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003e```python\nassert isinstance(1, int)\nassert isinstance(\"1\", str)\nassert isinstance(1., float)\n```...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003eThe **refcount** is a counter that keeps track of how many references point to an object. Its value gets increased by 1 when, for instance, an object gets assigned to a new name. It gets decreased by ...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003eNote that despite `x` being the only reference to that empty list, the output of `getrefcount` is 2. This is because the function itself contains a new reference to the list, hence temporarily increas...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003eI expected that a newly created `bool(True)` object would have a `refcount` of 1. However, the actual number is much higher. This is because `True`, `False`, `None` and [a few others](https:\u002f\u002fdocs.pyt...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003e```python\n# two integers are created\n# and each is assigned a name\nx = 256\ny = 256...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003eObjects have additional properties and methods that encode their state and behaviors. For instance, the `float` class has an additional property that stores the numerical value, as well as multiple me...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003eObjects are first-class citizens in Python, meaning they can be treated like any other value. In other words, they can:...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003eAs mentioned above, the assignment operator `=` does not copy objects, only references. If we need to copy an object, we need to use the `copy` module. There are two types of copies: shallow and deep....","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003eShallow copies, made with `copy.copy`, duplicate the object. However any reference it stores will just get copied as a reference, i.e., the referenced object will not be duplicated....","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003ex = [1, 2, [3, 4]]\n# copy the two first integers\n# but only a reference to the\n# 3rd element\nx_copy = copy(x)...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003eDeep copies, made with `copy.deepcopy`, recursively duplicates the object and all the referenced objects....","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003ex = [1, 2, [3, 4]]\n# copies the two first integers\n# as well as the list\nx_copy = deepcopy(x)...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003ePython allows us to define our own classes using the `class` keyword. New objects are instantiated using the class name. Let's see an example:...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003e    def __init__(self, species, weight):\n        self.species    = species\n        self.weight     = weight\n        self.__favorite = True...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003e    def set_favorite(self, flag):\n        if isinstance(flag, bool):\n            self.__favorite = flag\n        else:\n            msg = \"flag should be a bool;\" \\\n                f\"{type(flag)} found....","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003eThis is a defining an animal that can only eat while making cute noises. The animal also has some attributes: a phylum, a species, a weight and a boolean denoting if its my favorite or not. By default...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003eIn other languages, a class' attributes can be set as public (accessible to everyone), protected (only accessible within the class and subclasses) or as private (only accessible within the class). Thi...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003ePython emulates protected and private attributes by prepending one or two underscores respectively. In our `Animal` example, `Animal.__favorite` is a private attribute:...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003e```\nAttributeError: 'Animal' object has no attribute\n'__favorite'. Did you mean: 'is_favorite'?\n```...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003eWe can read the private attribute using the public function `Animal.is_favorite()`, and modify it using the private function `Animal.set_favorite()`:...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003eHowever, is just a convention: underscores signal intent, but access is not strictly restricted: you can _always_ modify attributes from the outside. However, we need to put some extra effort and by-p...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003eNote that we can define a new, public `__favorite` attribute on the instantiated object, which is different from the `_Animal__favorite` attribute defined at the class level. Hence, this becomes possi...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003eUnderlying every object there are two dictionaries. They are accessible, respectively, using `{instance}.__dict__` and `{Class}.__dict__`. The first one is an instance-specific dictionary containing i...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003eNote that private attributes like `__favorite` appear with an altered name of the form `_{class name}{attribute}`....","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003eSimilarly, each class has its own dictionary, containing the data and functions used by all instances (class' methods, attributes defined at the class level, etc.):...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003eFor instance, this is where the `Animal.eat()` method lives. This dictionary is shared by all the instances, which is why every non-static method requires the instance to be passed as the first argume...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003e```\nTraceback (most recent call last):\n  File \"\u003cstdin\u003e\", line 1, in \u003cmodule\u003e\nTypeError: Animal.eat() missing 1 required positional argument: 'self'\n```...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003eBoth dictionaries are linked by `instance.__class__`, which is assigned to the class object:...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003eAs we saw, an attribute might exist in either dictionary. To find an attribute at runtime, Python will first search `instance.__dict__`; if unsuccessful, it will search `Class.__dict__`....","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003eThe instance's dictionary keeps the class flexible, allowing to add new attributes at any time:...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003e```\n{'species': 'whale',\n 'weight': 100000,\n '_Animal__favorite': False,\n 'medium': 'water'}\n```...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003e`__slots__` allows us to fix the possible attributes a priori, allowing Python to reserve the exact amount of memory needed and to bypass the creation of the dictionary:...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003e    def __init__(self, species, weight):\n        self.species    = species\n        self.weight     = weight\n        self.__favorite = True...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003e```\nAttributeError: 'EfficientAnimal' object has no\nattribute '__dict__'. Did you mean: '__dir__'?\n```...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003eThis mainly optimizes memory, though it also blocks new attributes, helping to prevent bugs caused by typos in variable names:...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003e**Serialization** is the process of converting a Python object into a file format that can be used to reconstruct the object later. This allows us to store objects, make them persistent across executi...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003eclass Person:\n    def __init__(self, name, surname, age):\n        self.name = name\n        self.surname = surname\n        self.age = age...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003e    def __eq__(self, other):\n        return (\n            self.name == other.name and\n            self.surname == other.surname and\n            self.age == other.age\n        )...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003eIn such cases we need functions that assist Python in (de)serializing the object:...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003e```python\ndef person_encoder(obj):\n    if isinstance(obj, Person):\n        obj_dict = {\n            \"__person__\": True,\n            \"name\": obj.name,\n            \"surname\": obj.surname,\n            \"a...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003e        name    = dct[\"name\"]\n        surname = dct[\"surname\"]\n        age     = dct[\"age\"]...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003e# serialize\nwith open(\"obj.json\", mode=\"w\") as J:\n    json.dump(person1, J, default=person_encoder)...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003e# deserialize\nwith open(\"obj.json\", mode=\"r\") as J:\n    person2 = json.load(J, object_hook=person_decoder)...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003eJSON is attractive because it is human-readable and interoperable. However (de)serializing sophisticated objects using JSON can be pretty involved due to the need to define an encoder and a decoder. B...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003e# serialize or \"pickle\"\nwith open(\"obj.pkl\", mode=\"wb\") as P:\n    pickle.dump(person1, P)...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003e# deserialize or \"unpickle\"\nwith open(\"obj.pkl\", mode=\"rb\") as P:\n    person2 = pickle.load(P)...","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003eAn important downside is that pickle objects are able to execute arbitrary code during unpickling. Hence, unpickling untrusted files should be regarded as a security risk....","\u003cb\u003ePython objects\u003c\u002fb\u003e\u003cbr\u003e- D. Beazley, [Advanced Python Mastery](https:\u002f\u002fgithub.com\u002fdabeaz-course\u002fpython-mastery)\n- https:\u002f\u002fwww.interviewbit.com\u002fpython-interview-questions\u002f#freshers\n- More on classes: \u003chttps:\u002f\u002fdocs.python.org...","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003eA language is **statically typed** when variables have types, i.e., the type of the variables are checked before execution (usually at compilation). In contrast, in **dynamically typed** languages var...","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003eNo programming language is either interpreted or compiled. A language is just a set of instructions to tell a computer how to perform tasks. And this language can be implemented in different ways. For...","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003e1. \"Compile\" the source code into a Python-specific lower level code (`*.pyc`, stored in `__pycache__`), called _bytecode_.\n1. Execute the bytecode by the Python Virtual Machine. This is an infinite e...","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003e\u003e Step 1's \"compilation\" is qualitatively different from the compilation of so-called compiled languages. For starters compiling a C code results in a standalone executable. Another difference is that...","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003e- Numeric Types (`int`, `float`, `complex`)\n- Boolean Type (`bool`)\n- Iterator Types\n- Sequence Types (`list`, `tuple`, `range`)\n- Text Sequence Type (`str`)\n- Binary Sequence Types (`bytes`, `bytearr...","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003ePython has two kinds of data types, **mutable** and **immutable**, which respectively can and cannot be modified after being created. Examples of mutable data types are lists, dictionaries and sets; e...","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003e# we change the value of x. since\n# integers are immutable, a new\n# int(x + 1) is created to store\n# that value, and x is assigned\n# that new reference\nx += 1...","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003e# x and y do not point to the same\n# object anymore\nassert x != y\nassert x is not y\n```...","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003e# we change the value of x\n# since lists are mutable,\n# the original list gets altered\nx.append(2)...","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003eImmutability is leveraged to define singletons, which I discussed when [examining the refcount]({% post_url 2024-01-07-python-objects %}#refcount)....","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003eMutability also has implications on [memory allocation](#memory-management-in-python). Python knows at runtime how much memory an immutable data type requires. However, the memory requirements of muta...","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003eA **namespace** is a mapping from names to objects. In fact, underlying a namespace there is a dictionary: its keys are symbolic names (e.g., `x`) and its values are the object they reference (e.g., a...","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003e- **Builtin** namespaces: it is created when the interpreter starts up. It contain names such as `print`, `int` or `len`....","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003e- **Global** namespaces: _The_ global namespace contains every name created at the main level of the program. This dictionary can be examined using `globals()`. But, _other_ global namespaces are poss...","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003e- **Local** namespaces: one is created every time a function is called, and is \"forgotten\" when it terminates. This dictionary can be examined using `locals()`....","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003e- **Enclosed** namespaces: when a function calls another function, the child has access to its parent's namespace....","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003eNamespaces are related to scopes, which are the parts of the code in which a specific set of namespaces can be accessed. When a Python needs to lookup a name, if resolves it by examining the namespace...","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003e\t# use foo defined within f_enclosed\n\tprint(\"\\tInside f_enclosed():\")\n\tprint(f\"\\tfoo = {foo}\")...","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003e\t\t# use foo defined within f_local\n\t\tprint(\"\\t\\tInside f_local():\")\n\t\tprint(f\"\\t\\tfoo = {foo}\")...","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003e\t# f_local's foo is gone\n\t# we are back to f_enclosed's\n\tprint(\"\\tAfter f_local():\")\n\tprint(f\"\\tfoo = {foo}\")...","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003e\t\t# modifies the foo from the\n\t\t# enclosing namespace\n\t\t# i.e., f_enclosed's\n\t\tnonlocal foo\n\t\tprint(\"\\t\\tInside f_non_local():\")\n\t\tprint(f\"\\t\\toriginally, foo = {foo}\")...","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003e\t# f_enclosed's foo is changed even\n\t# outside of f_non_local\n\tprint(\"\\tAfter f_non_local():\")\n\tprint(f\"\\tfoo = {foo}\")...","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003e\t\t# modifies the foo from the\n\t\t# global namespace\n\t\tglobal foo\n\t\tprint(\"\\t\\tInside f_global():\")\n\t\tprint(f\"\\t\\toriginally, foo = {foo}\")...","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003e\t# f_enclosed's remains unchanged\n\tprint(\"\\tAfter f_global():\")\n\tprint(f\"\\tfoo = {foo}\")...","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003efoo = \"original\"\nprint(\"At the beginning:\")\nprint(f\"foo = {foo}\")\nf_enclosed()\nprint(\"Finally:\")\nprint(f\"foo = {foo}\")\n```...","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003e```\nAt the beginning:\nfoo = original\n\tInside f_enclosed():\n\tfoo = enclosed\n\t\tInside f_local():\n\t\tfoo = local\n\tAfter f_local():\n\tfoo = enclosed\n\t\tInside f_non_local():\n\t\toriginally, foo = enclosed\n\t\tbu...","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003eIn CPython, all the objects live in a _private_ [heap]({% post_url 2024-02-10-hardware %}#memory-allocation). Memory management is handled exclusively by the Python memory manager. In other words, and...","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003eWhen an object is created, the memory manager allocates some memory for it in the heap, and its reference is stored in the relevant namespace....","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003eConversely, the garbage collector is an algorithm that deallocates objects when they are no longer needed. The main mechanism uses the [reference count]({% post_url 2024-01-07-python-objects %}#refcou...","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003eThe global interpreter lock (GIL) is a mechanism to make CPython's thread safe, by only allows one thread to execute Python bytecode at a time. This vastly simplifies CPython's implementation and writ...","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003e1. Multiprocessing, i.e., launching multiple Python processes, each with their own interpreter, memory, and GIL.\n1. Developing a C extension, which gives us lower-level access to threading....","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003eA Python module is simply a file containing Python functions, classes, constants and runnable code. When we want to use them, we need to _import_ the module using the `import` statement. For instance:...","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003eIt imports [this file](https:\u002f\u002fgithub.com\u002fnumpy\u002fnumpy\u002fblob\u002fmain\u002fnumpy\u002f__init__.py) from your installed NumPy package as a module object and assigns its reference name `np`....","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003e1. Built-in modules: written in C, and part of the Python executable.\n1. Frozen modules: written in Python, and part of the Python executable.\n1. C extensions: written in C, but loaded dynamically int...","\u003cb\u003eThe Basics of Python\u003c\u002fb\u003e\u003cbr\u003e- [StackOverflow: If Python is interpreted, what are .pyc files?](https:\u002f\u002fstackoverflow.com\u002fquestions\u002f2998215\u002fif-python-is-interpreted-what-are-pyc-files)\n- [Python behind the scenes #11: how the Pyth...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e# The inner workings of diction...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003eelements can be retrieved using _keys_, i.e., so-called _hashable_ objects (further described below)...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003eThe underpinnings of dictionaries and sets are very similar, a data structure called the _hash map_...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e[Similarly to lists and tuples]({% post_url 2024-01-20-python-lists %}), we can visualize it as a finite number of memory buckets, each of which can store a reference to an object...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003eThe difference between dictionaries and sets lies in what goes in the bucket: dictionaries store key-value pairs, while sets only store keys....","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003eHash maps rely on the key objects implementing a hash method (`.__hash__()`). This function maps each object to a fixed-byte integer. When we apply the `hash()` function to the object, its hash method...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e```python\nhash(1)        # 1\nhash(1.)       # 1\nhash(\"1\")      # 6333942777250828306...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003ehash((1))      # 1\nhash((1, 1))   # 8389048192121911274\nhash((1., 1.)) # 8389048192121911274\n```...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003eImagine that our hash map has a capacity of 16, i.e., we have 16 buckets indexed from 0 to 15. However, as we just saw, hashing can produce very large integers, which makes it impossible to use the in...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003eIn other words, 62 would map to bucket number 2. However, there are faster _masks_. Specifically, Python uses the bitwise AND (`&`):...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e```python\n# bin(62) = 0b111110\n#               &\n# bin(15) = 0b001111\n# ------------------\n#           0b001110 = 14\n62 & 15\n```...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003eNote that since the number of buckets will often be much smaller than the hash, `&` is effectively operating only on the tails....","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003eIf the number of _empty_ buckets is large enough, a new key can be mapped to an empty bucket with high probability. (Empty buckets contain a `NULL` value.) In that case, we will simply store the key-v...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003eThe details of this process, called _probing_, are beyond the scope of this article...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003eAt lookup time, this probing process will be followed, until either the right key or an empty bucket is found....","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003eWhen a value is deleted, we cannot simply overwrite it with a `NULL`. That would make the bucket identical to a \"virgin\" bucket, and potentially disrupt the probing strategy, leading to inconsistent r...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e**Note:** User-defined classes are hashable by default, even if they don't implement `__hash__`, `__eq__` or `__cmp__`. The hash is computed using the object's `id()`, and all objects compare unequal....","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003eUnderstanding what underlies dictionaries and sets allows us to estimate the time complexity of the different operations....","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e**Lookup**: given a key, in the vast majority of cases a lookup is done in $$O(1)$$. The actual time depends on how fast the hash function is. Collisions are the main hurdle to lookup: in the worst ca...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e**Insertion**: for similar reasons, the amortized time complexity is $$O(1)$$ and the worst case is $$O(n)$$....","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e**Deletion**: for similar reasons, the amortized time complexity is $$O(1)$$ and the worst case is $$O(n)$$....","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e**Resizing**: Python doubles the capacity of a dictionary when it becomes 2\u002f3 full. Similarly, the capacity of a set gets quadrupled when it becomes 2\u002f3 full. When such a thing happens, all key-value ...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003eclass Dictionary:\n    \"\"\"\n    A dictionary implementation using linear probing for collision resolution.\n    \"\"\"...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e    def __init__(self, capacity: int = 1024) -\u003e None:\n        \"\"\"\n        Initialize the Dictionary with a given capacity....","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e        # virgin buckets are set to None, deleted buckets to False\n        self.__buckets = [None for _ in range(capacity)]\n        self.__size = capacity\n        self.__n_items = 0...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e    def __setitem__(self, key: Hashable, value: Any) -\u003e None:\n        \"\"\"\n        Set the value for a given key in the dictionary....","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e        Parameters:\n        - key (Hashable): The key to be inserted.\n        - value (Any): The value associated with the key....","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e        Returns:\n        - None\n        \"\"\"\n        idx = self.__find_key_bucket(key)\n        self.__buckets[idx] = (key, value)\n        self.__n_items += 1\n        self.__resize_check()...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e    def __getitem__(self, key: Hashable) -\u003e Any:\n        \"\"\"\n        Get the value associated with a given key from the dictionary....","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e        Raises:\n        - KeyError: If the key is not found in the dictionary.\n        \"\"\"\n        idx = self.__find_key_bucket(key)\n        return self.__buckets[idx][1]...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e    def __contains__(self, key: Hashable) -\u003e bool:\n        \"\"\"\n        Check if the dictionary contains a given key....","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e        Parameters:\n        - key (Hashable): The key to check for existence in the dictionary....","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e        Returns:\n        - bool: True if the key is present in the dictionary, False otherwise.\n        \"\"\"\n        try:\n            idx = self.__find_key_bucket(key)\n        except KeyError:\n        ...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e\n    def __delitem__(self, key: Hashable) -\u003e None:\n        \"\"\"\n        Delete the entry for a given key from the dictionary....","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e        Raises:\n        - KeyError: If the key is not found in the dictionary.\n        \"\"\"\n        idx = self.__find_key_bucket(key)\n        self.__buckets[idx] = False\n        self.__n_items -= 1...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e    def __resize_check(self) -\u003e None:\n        \"\"\"\n        Check if resizing of the dictionary is necessary based on the load factor (2\u002f3)....","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e        Returns:\n        - None\n        \"\"\"\n        if self.__n_items \u003c (self.__size * 2 \u002f 3):\n            return...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e        # reinsert all existing key-value pairs\n        for bucket in self.__buckets:\n            if not bucket:\n                continue\n            key, value = bucket\n            dummy_dict[key] = ...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e        self.__size = new_size\n        self.__buckets = dummy_dict.__buckets\n        self.__n_items = dummy_dict.__n_items...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e    def __find_key_bucket(self, key: Hashable) -\u003e int:\n        \"\"\"\n        Find the index of the bucket corresponding to a given key....","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e        Raises:\n        - KeyError: If the key is not found in the dictionary.\n        \"\"\"\n        idx = hash(key) & (self.__size - 1)\n        n_iters = 0...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e            # stop once we have checked all buckets\n            if n_iters \u003e= self.__size:\n                raise KeyError...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e            if isinstance(self.__buckets[idx], tuple):\n                if self.__buckets[idx][0] == key:\n                    break...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e            idx += 1\n            n_iters += 1\n            if idx \u003e= self.__size:\n                idx = 0...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e# 75% is full, which will trigger a resizing\nmy_dict[5] = \"c\"\n# [None, (1, 'a'), (2, 'b'), None, None, (5, 'c'), None,\n#  None, None, None, None, None, None, None, None, None]...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003emy_dict[140] = \"d\"\n# [None, (1, 'a'), (2, 'b'), None, None, (5, 'c'), None,\n#  None, None, None, None, None, (140, 'd'), None, None, None]...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003emy_dict[1] = 2\n# [None, (1, 2), (2, 'b'), None, None, (5, 'c'), None, None,\n#  None, None, None, None, (140, 'd'), None, None, None]...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003edel my_dict[1]\n# [None, False, (2, 'b'), None, None, (5, 'c'), None, None,\n#  None, None, None, None, (140, 'd'), None, None, None]...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003emy_dict[1] = \"x\"\n# [None, (1, 'x'), (2, 'b'), None, None, (5, 'c'), None,\n#  None, None, None, None, None, (140, 'd'), None, None, None]...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e```python\ningredients = {\n    \"carrots\": 3,\n    \"tomatoes\": 2,\n    \"lettuces\": 1,\n}...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003eSince Python 3.6, Python dictionaries preserve insertion order, i.e., the items are printed in the same order in which they were inserted in the dictionary:...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e```python\ningredients = {\n    \"carrots\": 3,\n    \"tomatoes\": 2,\n    \"lettuces\": 1,\n}\nprint(ingredients)\n```...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e```python\ningredients = {\n    \"lettuces\": 1,\n    \"tomatoes\": 2,\n    \"carrots\": 3,\n}\nprint(ingredients)\n```...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003eAfter Python 3.10, there are three ways of merging two dictionaries. Two of them are equivalent: unpacking using the `**` operator and the merge operator `|`:...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003eThese options create a new dictionary with all the key-value pairs. As one might expect, the key insertion order is preserved from left to right. Note than when there are shared keys, the last value i...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003eAn alternative is `dict.update()`, wich merges the dictionaries in place, updating the values when a key is shared:...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e```python\ndairy_1.update(dairy_2)\nprint(f\"dairy_1 = {dairy_1}\")\nprint(f\"dairy_2 = {dairy_2}\")\n```...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e```\ndairy_1 = {'cheese': 3, 'yogurt': 4, 'paneer': 2}\ndairy_2 = {'cheese': 3, 'paneer': 2}\n```...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003eThis is more memory efficient, since it does not create a new dictionary. However, it is not desirable if we want to keep the original dictionaries....","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003eThe [`dict.setdefault`](https:\u002f\u002fdocs.python.org\u002f3.8\u002flibrary\u002fstdtypes.html#dict.setdefault) method is useful to assign a value to a key if and only if the key is missing:...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e```python\ningredients = {\n    \"carrots\": 3,\n    \"tomatoes\": 2,\n    \"lettuces\": 1,\n}...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003eprint(f\"Number of carrots: {ingredients['carrots']}\")\nprint(f\"Number of pineapples: {ingredients['pineapples']}\")\n```...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003eHowever, and despite its name, `dict.setdefault` will also fetch the value (either the preexisting one, or the newly created):...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003eThe [`collections.defaultdict`](https:\u002f\u002fdocs.python.org\u002f3\u002flibrary\u002fcollections.html#collections.defaultdict) goes one step beyond. They are a good replacement for dictionaries when there is a unique de...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003efor ingredient, amount in ingredients.items():\n    ingredients_dd[ingredient] = amount...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003eprint(ingredients_dd)\nprint(f\"Number of cabbages: {ingredients_dd['cabbages']}\")\nprint(ingredients_dd)\n```...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e```\ndefaultdict(\u003cfunction \u003clambda\u003e at 0x1014eb6d0\u003e, {'carrots': 3, 'tomatoes': 2, 'lettuces': 1, 'pineapples': 0})\nNumber of cabbages: 0\ndefaultdict(\u003cfunction \u003clambda\u003e at 0x1014eb6d0\u003e, {'carrots': 3, ...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003eNote that the `ingredients_dd` contains an item for cabbages which was never explicitly inserted. `defaultdict` not only allows us to write simpler code, but is more efficient than `setdefault` to avo...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003eThe `collections.Counter` is a type of dictionary specialized in counting objects, i.e., the values are integers. It can be initialized from an existing dictionary:...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e```python\nprint(f\"Number of cabbages: {ingredients_counter['cabbage']}\")\nprint(ingredients_counter)\n```...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e```\nNumber of cabbages: 0\nCounter({'carrots': 3, 'tomatoes': 2, 'lettuces': 1, 'pineapples': 0})\n```...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003eCounters extend dictionaries in interesting ways. For instance, they make it easy to find the elements with the most counts:...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e```python\n# store ingredients and purchase date\ningredients = {\n    (\"carrots\",  \"2024-01-04\"): 3,\n    (\"tomatoes\", \"2024-01-13\"): 2,\n    (\"carrots\", \"2024-01-13\"): 1,\n}\n```...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003eWhen we have two lists of the same length, we can quickly combine them using `zip`:...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e```python\ningredient_list = [\"carrots\", \"tomatoes\", \"lettuces\"]\ncounts = [3, 2, 1]\ningredients = dict(zip(ingredient_list, counts))\nprint(ingredients)\n```...","\u003cb\u003ePython dictionaries\u003c\u002fb\u003e\u003cbr\u003e- D. Beazley, [Advanced Python Mastery](https:\u002f\u002fgithub.com\u002fdabeaz-course\u002fpython-mastery)\n- M. Gorelick & I. Ozsvald, High Performance Python: Practical Performant Programming for Humans. Chapter 4. Di...","\u003cb\u003ePython lists and tuples\u003c\u002fb\u003e\u003cbr\u003e[Similarly to the computer's memory]({% post_url 2024-02-10-hardware %}), lists and tuples can be visualized as a sequence of equally-sized buckets...","\u003cb\u003ePython lists and tuples\u003c\u002fb\u003e\u003cbr\u003eEach bucket can store a fixed-length integer (e.g., 64 bits in modern computers), representing the memory address to an object...","\u003cb\u003ePython lists and tuples\u003c\u002fb\u003e\u003cbr\u003eThe buckets are located consecutively in memory, in a data structure known as _array_...","\u003cb\u003ePython lists and tuples\u003c\u002fb\u003e\u003cbr\u003eWhen Python instantiates an array, it will request $$N$$ consecutive buckets to the kernel...","\u003cb\u003ePython lists and tuples\u003c\u002fb\u003e\u003cbr\u003eOut of those, the first bucket stores the length of the array, and the remainng $$N - 1$$ will store the elements...","\u003cb\u003ePython lists and tuples\u003c\u002fb\u003e\u003cbr\u003eHowever, lists are stored in so-called _dynamic_ arrays, while tuples are stored in _static_ arrays...","\u003cb\u003ePython lists and tuples\u003c\u002fb\u003e\u003cbr\u003e- Lists are mutable: we can keep adding and removing items to the buckets. Hence, sometimes we might need to store more than $$N - 1 $$ elements originally requested. That is why they are stored in so...","\u003cb\u003ePython lists and tuples\u003c\u002fb\u003e\u003cbr\u003e- Tuples, on the other hand, are immutable. Hence, they are supported by _static_ arrays, which cannot be resized. Hence, tuples only take up the strictly required memory, making them more lightweight...","\u003cb\u003ePython lists and tuples\u003c\u002fb\u003e\u003cbr\u003e**Lookup**: since the buckets in the array are equally-sized and consecutive, we can quickly retrieve any item by knowing where the array starts and the index of its bucket. For instance, if our array...","\u003cb\u003ePython lists and tuples\u003c\u002fb\u003e\u003cbr\u003e**Search**: if we need to find a particular object in an unsorted array, we need to perform a [linear search](https:\u002f\u002fen.wikipedia.org\u002fwiki\u002fLinear_search). This algorithm has a complexity $$O(n)$$. If...","\u003cb\u003ePython lists and tuples\u003c\u002fb\u003e\u003cbr\u003e**Sort**: Python uses [Timsort](https:\u002f\u002fen.wikipedia.org\u002fwiki\u002fTimsort), a combination of heuristics, and insertion and merge sort. Best case is $$O(n)$$, worst case is $$O(n \\log n)$$....","\u003cb\u003ePython lists and tuples\u003c\u002fb\u003e\u003cbr\u003e### Lists...","\u003cb\u003ePython lists and tuples\u003c\u002fb\u003e\u003cbr\u003e**Insertion**: we can replace an existing element in $$O(1)$$. That is also the case in most insertions of a new element at the end using `append()`. However, when the list's array gets full (the wors...","\u003cb\u003ePython lists and tuples\u003c\u002fb\u003e\u003cbr\u003e**Insertion**: though tuples are immutable, we can consider the combination of two tuples into a longer one as an insertion operation. If they have sizes $$m$$ and $$n$$, each item needs to be copied ...","\u003cb\u003ePython lists and tuples\u003c\u002fb\u003e\u003cbr\u003eThe [`list.sort`](https:\u002f\u002fdocs.python.org\u002f3\u002flibrary\u002fstdtypes.html#list.sort) method orders a list's elements in ascending order. It will work as long as the items have defined the `\u003c` comparison opera...","\u003cb\u003ePython lists and tuples\u003c\u002fb\u003e\u003cbr\u003e```python\nclass Animal:\n    def __init__(self, name, weight):\n        self.name = name\n        self.weight = weight...","\u003cb\u003ePython lists and tuples\u003c\u002fb\u003e\u003cbr\u003eanimals = [\n    Animal(\"whale\", 100000),\n    Animal(\"sea lion\", 200),\n    Animal(\"lion\", 200),\n    Animal(\"possum\", 2.5)\n]...","\u003cb\u003ePython lists and tuples\u003c\u002fb\u003e\u003cbr\u003e```\n[Animal(possum, 2.5), Animal(sea lion, 200), Animal(lion, 200), Animal(whale, 100000)]\n```...","\u003cb\u003ePython lists and tuples\u003c\u002fb\u003e\u003cbr\u003e```\n[Animal(lion, 200), Animal(possum, 2.5), Animal(sea lion, 200), Animal(whale, 100000)]\n```...","\u003cb\u003ePython lists and tuples\u003c\u002fb\u003e\u003cbr\u003eAs shown, `key` takes a function which will receive an item, and output a comparable value. If we want to order first by weight, then by name, we just need to combine both in a tuple:...","\u003cb\u003ePython lists and tuples\u003c\u002fb\u003e\u003cbr\u003e```python\n# sort by name\nanimals.sort(key = lambda x: (x.weight, x.name))\nprint(animals)\n```...","\u003cb\u003ePython lists and tuples\u003c\u002fb\u003e\u003cbr\u003e```\n[Animal(possum, 2.5), Animal(lion, 200), Animal(sea lion, 200), Animal(whale, 100000)]\n```...","\u003cb\u003ePython lists and tuples\u003c\u002fb\u003e\u003cbr\u003eWhen comparing tuples, Python first compares the initial elements. If they are equal, it then compares the second one, and so on....","\u003cb\u003ePython lists and tuples\u003c\u002fb\u003e\u003cbr\u003eWe can use unpacking to swap multiple of a list in place, without requiring additional temporary variables:...","\u003cb\u003ePython lists and tuples\u003c\u002fb\u003e\u003cbr\u003eIn some cases we might want to unpack a tuple whose length we don't know a priori. In such cases, we can use an starred expression, which will receive all the values that are not captured by another i...","\u003cb\u003ePython lists and tuples\u003c\u002fb\u003e\u003cbr\u003e```python\nx = [1, 2, 3, 4, 5]\nfirst, second, *middle, last = x\nprint(first, second, middle, last)\n```...","\u003cb\u003ePython lists and tuples\u003c\u002fb\u003e\u003cbr\u003e```python\nx = [1, 2, 3]\nfirst, second, *middle, last = x\nprint(first, second, middle, last)\n```...","\u003cb\u003ePython lists and tuples\u003c\u002fb\u003e\u003cbr\u003eStarred elements cannot be used without non-starred elements (e.g., `*all = x`); multiple starred expressions cannot be used together either (e.g., `first, *middle1, *middle2 = x`)....","\u003cb\u003ePython lists and tuples\u003c\u002fb\u003e\u003cbr\u003e- D. Beazley, [Advanced Python Mastery](https:\u002f\u002fgithub.com\u002fdabeaz-course\u002fpython-mastery)\n- M. Gorelick & I. Ozsvald, High Performance Python: Practical Performant Programming for Humans. Chapter 3. Li...","\u003cb\u003ePython functions\u003c\u002fb\u003e\u003cbr\u003e# Function's argu...","\u003cb\u003ePython functions\u003c\u002fb\u003e\u003cbr\u003eThe distiction between mutable and immutable datatypes has implications when they are passed to functions as arguments. When an immutable datatype is passed, Python creates a copy and assigns it to a ...","\u003cb\u003ePython functions\u003c\u002fb\u003e\u003cbr\u003eMutable types, on the other hand, are passed by reference. In other words, the function will operate on the same object that existed outside of the function:...","\u003cb\u003ePython functions\u003c\u002fb\u003e\u003cbr\u003eSometimes we might define some default value for a function's arguments. However it is often not a good idea to use mutable datatypes as a default. The default object is only created once, leading to ...","\u003cb\u003ePython functions\u003c\u002fb\u003e\u003cbr\u003e```python\ndef append_three(input: list = []):\n    input.append(3)\n    return input...","\u003cb\u003ePython functions\u003c\u002fb\u003e\u003cbr\u003e```python\ndef format(x, debug=False):\n    debug_info = \"\"\n    if debug:\n        debug_info = f\"Formatting string: {x}\\n\"\n    print(f\"{debug_info}{x.title()}\")...","\u003cb\u003ePython functions\u003c\u002fb\u003e\u003cbr\u003e```python\ndef format(x, *, debug=False):\n    debug_info = \"\"\n    if debug:\n        debug_info = f\"Formatting string: {x}\\n\"\n    print(f\"{debug_info}{x.title()}\")...","\u003cb\u003ePython functions\u003c\u002fb\u003e\u003cbr\u003e```\nTraceback (most recent call last):\n  File \"\u003cstdin\u003e\", line 1, in \u003cmodule\u003e\nTypeError: format() takes 1 positional argument but 2 were given\n```...","\u003cb\u003ePython functions\u003c\u002fb\u003e\u003cbr\u003eAs Python objects, functions are first-class citizens, which unlocks several useful features, like closures and decorators....","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003e# The inner workings of NumPy arrays...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003eThe main selling point of NumPy is the speed up in computations it offers. However, to understand that, we need to first understand the underpinnings of the NumPy array, or `ndarray`. The `ndarray` is...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003e# Python references - which, as we will see soon,\n# greatly undoes the benefits of NumPy\nnp.array([int, float, str])\n```...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003e[Remember]({% post_url 2024-01-20-python-lists %}) that each of the 64-bit buckets does not store the float, but a 64-bit reference to the float object...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003eThis keeps lists flexible, since each bucket can contain a reference to any object, float or not...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003eBut it comes with a memory overhead since, on top of the reference, we need to store the object itself, [which is more complex than a naked float value]({% post_url 2024-01-07-python-objects %}#proper...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003eIn contrast, an `ndarray` stores only the floats themselves, requiring less than 25% of the memory...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003eFurthermore, in the list of floats, the data is _fragmented_: the list itself, and all the objects it references are scattered across memory...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003eThe **metadata** includes important information about the data buffer. We can access the metadata like this:...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003e```python\n{'data': (105553130143936, False),\n 'descr': [('', '\u003ci8')],\n 'shape': (6,),\n 'strides': None,\n 'typestr': '\u003ci8',\n 'version': 3}\n```...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003eFor instance, each `ndarray` has a data type, or `dtype`, which specifies what type of elements it contains (e.g., `float64`, `float16` or `int32`). If we need further memory savings, we can consider ...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003e```python\nnp.array([1., 2., 3.], dtype = \"float16\").nbytes # 6\nnp.array([1., 2., 3.], dtype = \"float64\").nbytes # 24\n```...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003e# we sequentially iterate the\n# pairs and compute the sum\n[x + y for x, y in zip(xs, ys)]\n```...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003eImportantly, a vectorized operation is vastly faster than its explicit `for` loop counterpart. To understand why, we need to take a step back, and [understand how the RAM and the CPU interact]({% post...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003eThis severely hampers the prefetcher, and [cache misses]({% post_url 2024-02-10-hardware %}#cache-and-prefetching) are common...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003eIn other words, the list is an object with an attribute containing a reference to an array which in turn stores references to float objects...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003e(Potentially, this could be solved by [`arrays`](https:\u002f\u002fdocs.python.org\u002f3\u002flibrary\u002farray.html)...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003eBut they seem to have their own downsides.) Keeping data together, as `ndarrays` do, leads to less cache misses...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003e(Additional gains are possible by reducing the number of [cache lines]({% post_url 2024-02-10-hardware %}) an array spans, e.g., using aligning their beginning to the memory grid...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003eSo far I have find some indications, but not strong sources supporting that NumPy attempts this too.)...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003eSecond, Python is **dynamically** typed. This means that every operation between two numbers becomes a complex interaction between two heavy data structures. Internally, Python needs to find out the t...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003eVectorization has another meaning in hardware. Specifically, it refers to SIMD, the ability of the CPU to [handle multiple numbers in a single instruction]({% post_url 2024-02-10-hardware %}#registers...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003eBy default, `ndarrays` store matrices in a row-major order, that is, as a concatenation of the rows of the matrix. In other words, the elements from the same row live close (sharing cache lines), but ...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003e# the default order is \"C\",\n# which confusingly refers to row-major\nmatrix = np.ones((n_rows, n_cols), order = \"C\", dtype = \"int8\")...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003e# time row operation\nstart_time = time.time()\n_ = matrix.sum(axis = 1) # sum the columns row-wise\nend_time = time.time()\nrow_time = end_time - start_time\nprint(\"Time taken for the row operation:\", row...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003e# time column operation\nstart_time = time.time()\n_ = matrix.sum(axis = 0) # sum the rows col-wise\nend_time = time.time()\ncol_time = end_time - start_time\nprint(\"Time taken for the column operation:\", ...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003e```\nTime taken for the row operation: 8.663719177246094\nTime taken for the column operation: 9.641089916229248\n```...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003e(Note that `ndarray.sum()` calls an efficient, low-level function. The gap is much larger for most user-defined functions.)...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003eConsistenly, shifting to column-major order (`order = \"F\"`) produces the opposite result.Carefully considering the operations we will be carrying out can have a major impact....","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003eNumPy introduced an important but tricky concept: data _views_. A views is just a new way to access the data buffer of an existing `ndarray`, with different metadata. Some operations produce views, li...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003e# x and y point to the same data buffer\nassert x.__array_interface__['data'][0] == y.__array_interface__['data'][0]\n```...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003e```python\nx = np.array([1, 2, 3, 4, 5, 6])\n# advanced indexing\n# (e.g., integer or boolean arrays)\ny = x[[0, 1]]\nz = x[x \u003e 4]...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003eOther functions, like `numpy.reshape` or `numpy.ravel`, will produce a view whenever possible and a copy otherwise....","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003e```python\nx = np.array([1, 2, 3, 4, 5, 6])\npointer_1 = x.__array_interface__['data'][0]...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003eCasting is the conversion of data from one type into another. The simplest form of casting is simply changing the type of an `ndarray`:...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003eCasting commonly occurs when performing arithmetic operations between different types. In those cases, NumPy picks the largest type that can safely represent both operants without losing precision. Fo...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003eWe can explicitly trigger a reinterpretation of the data buffer under another type using `ndarray.view()`. Note that produces a view, not a re-casting of the original `ndarray`. Let's see an example:...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003e```python\nx = np.array([1, 2], dtype = \"uint8\")\nbinary_repr = ''.join(format(byte, '08b') for byte in x.data.tobytes())...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003eThis is what our array looks like in memory. We can see the binary encoding as an `np.unit8` of 1 (`00000001`) and of 2 (`00000010`). `ndarray.view()` can reinterpret this sting of 16 bits as an `np.i...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003e```python\nassert x.view(\"int16\").byteswap()[0] == int(binary_repr, base = 2) # 258\n```...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003eIgnore the `byteswap()` call. [It relates to how NumPy parses a sequence of bits.](https:\u002f\u002fnumpy.org\u002fdevdocs\u002fuser\u002fbyteswapping.html#changing-byte-ordering)...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003e- M. Gorelick & I. Ozsvald, High Performance Python: Practical Performant Programming for Humans. Chapter 5. Matrix and Vector Computation....","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003e  - [The limits of Python vectorization as a performance technique](https:\u002f\u002fpythonspeed.com\u002farticles\u002fvectorization-python-alternatives\u002f)...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003e  - [How vectorization speeds up your Python code](https:\u002f\u002fpythonspeed.com\u002farticles\u002fvectorization-python\u002f)...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003e  - [Massive memory overhead: Numbers in Python and how NumPy helps](https:\u002f\u002fpythonspeed.com\u002farticles\u002fpython-integers-memory\u002f)...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003e- [What scientists must know about hardware to write fast code](https:\u002f\u002fviralinstruction.com\u002fposts\u002fhardware\u002f)...","\u003cb\u003eVectors and matrices in Python\u003c\u002fb\u003e\u003cbr\u003e- [Scipy lecture notes: Advanced NumPy](https:\u002f\u002fscipy-lectures.org\u002fadvanced\u002fadvanced_numpy\u002f)...","\u003cb\u003eHow computers work\u003c\u002fb\u003e\u003cbr\u003eIn plain terms,...","\u003cb\u003eHow computers work\u003c\u002fb\u003e\u003cbr\u003eIn plain terms, the **CPU** is the part of the computer that carries out the computations themselves. It does so in a stream of discrete operations, called _CPU instructions_. Roughly, one instruction...","\u003cb\u003eHow computers work\u003c\u002fb\u003e\u003cbr\u003eThe **RAM** is the (temporary) memory of the computer, where the data lives. We can picture it as a grid of buckets, each of which can contain 1 byte of information. (From now on, I will use the terms...","\u003cb\u003eHow computers work\u003c\u002fb\u003e\u003cbr\u003e- The buckets are arranged into rows of 64, called _cache lines_.\n- The buckets have a natural ordering, hence can be univocally identified by enumerating them, their _memory address_....","\u003cb\u003eHow computers work\u003c\u002fb\u003e\u003cbr\u003eOne byte, 8 bits, can take 256 different values. In consequence, it can store an integer from 0 to 255. If we need to store a larger value, we need to use multiple bytes....","\u003cb\u003eHow computers work\u003c\u002fb\u003e\u003cbr\u003e- Load data\n- Store data\n- Add\n- Increment\u002fdecrement the index\n- Jump to a part of the program...","\u003cb\u003eHow computers work\u003c\u002fb\u003e\u003cbr\u003eWhen we execute a program, it interacts with the operating systems kernel to handle its resources. The memory assigned to a program is split into two components: the _stack_ and the _heap_. They work ...","\u003cb\u003eHow computers work\u003c\u002fb\u003e\u003cbr\u003eThe **stack** is the memory that serves as scratch space for the program. It handles function calls, local variables and context. It appropriately works as a stack, as a Last-In-First-Out (LIFO) struc...","\u003cb\u003eHow computers work\u003c\u002fb\u003e\u003cbr\u003eThe **heap** is the memory set aside for memory allocation: when a program needs more memory, it places a request to the kernel, which will _allocate_ the requested amount from the heap, i.e., reserve...","\u003cb\u003eHow computers work\u003c\u002fb\u003e\u003cbr\u003eOnce the program does not need that chunk anymore, it will _deallocate_ it, i.e., hand it back to the operating system...","\u003cb\u003eHow computers work\u003c\u002fb\u003e\u003cbr\u003eUnlike the stack, allocations and deallocations on the heap do not follow a specific order...","\u003cb\u003eHow computers work\u003c\u002fb\u003e\u003cbr\u003eInstead, it is the task of the program to keep track of what data is stored where, which parts are still used, and which ones are not and can be deallocated...","\u003cb\u003eHow computers work\u003c\u002fb\u003e\u003cbr\u003eSpecifically, many programming languages have a routine called the _garbage collector_...","\u003cb\u003eHow computers work\u003c\u002fb\u003e\u003cbr\u003eThe garbage collector monitors which pieces of data won't be needed anymore, and periodically deallocates them...","\u003cb\u003eHow computers work\u003c\u002fb\u003e\u003cbr\u003eHowever, this can be sped up by copying the data to the CPU's cache, which is closer to the CPU and faster...","\u003cb\u003eHow computers work\u003c\u002fb\u003e\u003cbr\u003eWhen the CPU needs a piece of data, it first checks if it is already available in the cache...","\u003cb\u003eHow computers work\u003c\u002fb\u003e\u003cbr\u003eIf it is not, we are in a situation known as _cache miss_, in which the program can't proceed until the required data is retrieved...","\u003cb\u003eHow computers work\u003c\u002fb\u003e\u003cbr\u003eCPUs have a component, called _prefetcher_, which tries to mitigate cache misses. To achieve that, it actively tries to predict which pieces of data the CPU will need in the near future, and preemptiv...","\u003cb\u003eHow computers work\u003c\u002fb\u003e\u003cbr\u003eThe registers are the small sized slots on which the CPU acts on. Traditionally they had 8 bytes in size, just enough for one 64-bit integer or float. For instance, adding two such numbers requires th...","\u003cb\u003eHow computers work\u003c\u002fb\u003e\u003cbr\u003e- [What scientists must know about hardware to write fast code](https:\u002f\u002fviralinstruction.com\u002fposts\u002fhardware\u002f)\n- [Book 5: What is Memory?](https:\u002f\u002fmasters-of-the-void.com\u002fbook5.htm)...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003e# Strings...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003e# Strings...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003eWhile Python offers multiple ways of formatting strings (i.e., combining predefined text and variables), [F-strings](https:\u002f\u002fdocs.python.org\u002f3\u002ftutorial\u002finputoutput.html#formatted-string-literals) are ...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003e```python\nconstants = {\n    \"pi\": 3.14159265358979323846,\n    \"sqrt(2)\": 1.41421356237309504880,\n    \"Euler's number\": 2.71828182845904523536\n}...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003e```\npi = 3.141592653589793\nsqrt(2) = 1.4142135623730951\nEuler's number = 2.718281828459045\n```...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003e```python\nfor name, value in constants.items():\n    print(f\"{name} = {value:.3f}\")\n```...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003e```python\nfor name, value in constants.items():\n    print(f\"{name:10} = {value:.3f}\")\n```...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003eNote that the string \"Euler's number\" exceeds the minimum length of 10, and is hence represented as is....","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003eThis is useful to cleanly produce long strings while respecting a certain maximum line length:...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003eThe [`enumerate`](https:\u002f\u002fdocs.python.org\u002f3\u002flibrary\u002ffunctions.html#enumerate) function creates a lazy generator over an iterable that will return a tuple (index, item). It can take a second parameter,...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003eThe [`zip`](https:\u002f\u002fdocs.python.org\u002f3\u002flibrary\u002ffunctions.html#zip) function combines two or more iterators, generating a lazy generator which yields the next item from each. It is particularly useful t...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003efor number, square in zip(numbers, squared):\n    print(f\"The square of {number} is {square}.\")\n```...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003eHowever, when the two iterables have different lenghts, `zip` will only emit as many elements as the shortest of them:...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003eWhen we do not wish this truncation to happen, [`itertools.zip_longest`](https:\u002f\u002fdocs.python.org\u002f3\u002flibrary\u002fitertools.html#itertools.zip_longest) might be what we need:...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003eThe [`list.sort`](https:\u002f\u002fdocs.python.org\u002f3\u002flibrary\u002fstdtypes.html#list.sort) method orders a list's elements in ascending order. It will work as long as the items have defined the `\u003c` comparison opera...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003e```python\nclass Animal:\n    def __init__(self, name, weight):\n        self.name = name\n        self.weight = weight...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003eanimals = [\n    Animal(\"whale\", 100000),\n    Animal(\"sea lion\", 200),\n    Animal(\"lion\", 200),\n    Animal(\"possum\", 2.5)\n]...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003e```\n[Animal(possum, 2.5), Animal(sea lion, 200), Animal(lion, 200), Animal(whale, 100000)]\n```...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003e```\n[Animal(lion, 200), Animal(possum, 2.5), Animal(sea lion, 200), Animal(whale, 100000)]\n```...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003eAs shown, `key` takes a function which will receive an item, and output a comparable value. If we want to order first by weight, then by name, we just need to combine both in a tuple:...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003e```python\n# sort by name\nanimals.sort(key = lambda x: (x.weight, x.name))\nprint(animals)\n```...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003e```\n[Animal(possum, 2.5), Animal(lion, 200), Animal(sea lion, 200), Animal(whale, 100000)]\n```...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003eThe [walrus operator](https:\u002f\u002fdocs.python.org\u002f3\u002freference\u002fexpressions.html#assignment-expressions) (`:=`) allows to assign variables in the middle of expressions:...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003e    Parameters:\n    - x (int): The dividend.\n    - y (int): The potential divisor....","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003e    Returns:\n    tuple: A tuple containing a boolean indicating whether y is a divisor of x,\n           and the remainder when x is divided by y. If y is a divisor, the\n           boolean is True, and...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003eThe walrus operator is present in the first line of the `is_divisor` function. It allows two things to happen at once. First, the `if` clause will evaluate the expression `x % y` (false if the remaind...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003eWe can use underscores `_` as visual separators between any pair of digits in integers, floats or complex numbers:...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003eMost decimal floating-point numbers cannot be represented as binary floating-point numbers. Instead, computers just store an approximation. This behavior is not evident by just asking Python to displa...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003eWhile this approximation is smaller than $$2^(-53)$$, that is enough to cause errors:...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003eIn this series, we have seen multiple examples in which the type of a variable is specified. For instance:...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003e```python\ndef pretty_print(x: str, prefix: str | None = None) -\u003e None:\n    prefix = f\"{prefix}: \" if prefix else \"\"\n    print(f\"{prefix}{x.title()}.\")\n```...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003eNote that typing hints are a relatively recent addition to Python. Typing hints of recent verions of Python might produce parsing errors on older version....","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003eThe stdlib's [`typing`](https:\u002f\u002fdocs.python.org\u002f3\u002flibrary\u002ftyping.html) module gives many options to control type hints. (Widely used packages bring their own typing hints, like [numpy](https:\u002f\u002fnumpy.o...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003eThe decorator `@typing.overload` allows to overload functions, that is, have a function behave differently depending on the argument type....","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003edef square(x: list[int] | int) -\u003e list[int] | int:\n    if isinstance(x, list):\n        return [square(_x) for _x in x]\n    return x * 2\n```...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003ePython is a dynamically typed language. Hence, typing hints are just, that, hints. However, we can use [mypy](https:\u002f\u002fgithub.com\u002fpython\u002fmypy) on our entire codebase to check that types are used correc...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003eTheir concern is such that they are willing to sacrifice code readability for minor gains in performance...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003eSuch people might get satisfaction from replacing arithmetic operations involving integers by bitwise operations...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003eSince those act directly on the bit representation of the integer, they can be more efficient...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003eDespite compilers performing some optimization of their own, [there is some somewhat old evidence supporting that bitwise operations are faster.](https:\u002f\u002fstackoverflow.com\u002fquestions\u002f37053379\u002ftimes-two...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003eThe `\u003e\u003e` and the `\u003c\u003c` operators shift the bit representation to the left and to the right, respectively. This can be used to quickly divide or multiply integers by powers of two:...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003eThe `&` operator is the bitwise AND operator. When we use `&` between any integer and a 1, we are effectively cheching if the last bit is a 1 (odd) or a 0 (even):...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003eThe `~` operator is the complement operator, which switches 1s by 0s and vice versa. Let's see it in action:...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003eSince the first bit represents the sign, it has the effect of turning $$x$$ into $$-x - 1$$. This is useful when we need to simultaneously iterate the front and the back of a list:...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003e```python\ndef is_palindrome(word: str) -\u003e bool:\n    return all([word[i] == word[~i] for i in range(len(word) \u002f\u002f 2)])...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003eHandling exceptions with `try: ... except: ...` is a common in Python code. But there are some additional nuances:...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003e```python\ny = list()\nx = 1\ntry:\n    x + 1\n    y.append(1)\n    {}[1]\n# we can handle multiple, specific exceptions\nexcept TypeError:\n    print(f\"Can't sum an integer and a {type(x)}.\")\nexcept Attribute...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003eA context manager is a programming construct that makes it easy to allocate and release resources. It is useful to handle file operations, network connections or database transactions, when it is impo...","\u003cb\u003eQuirks of Python\u003c\u002fb\u003e\u003cbr\u003e- D. Beazley, [Advanced Python Mastery](https:\u002f\u002fgithub.com\u002fdabeaz-course\u002fpython-mastery)\n- B. Slatkin, Effective Python: 90 Specific Ways to Write Better Python.\n- [PEP 515 \u2013 Underscores in Numeric Li...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eDivide and conquer algorithms work by breaking down a problem into _two or more_ smaller subproblems of the same type. These subproblems are tackled recursively, until the subproblem is simple enough ...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eThe input of interval problems is a list of lists, each of which contains a pair `[start_i, end_i]` representing an interval. Typical questions revolve around how much they overlap with each other, or...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e**Note:** There are many corner cases, like no intervals, intervals which end and start at the same time or intervals that englobe other intervals. Make sure to think it through....","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e**Note:** If the intervals are not sorted, the first step is _almost always_ **sorting them**, either by start or by end. This usually brings the time complexity to $$O(n \\log n)$$. In some cases we n...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eSorting consists on arranging the elements of an input array according to some criteria. There are multiple ways to sort an input, each offerintg different trade-offs:...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e- Memory usage: _in-place_ approaches sort the items in place, without using extra space.\n- Stability: stable algorithms preserve the original relative order when faced with two equal keys.\n- Internal...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e| Algorithm                        | Time complexity           | Space complexity |\n| -------------------------------- | ------------------------- | ---------------- |\n| [Selection](#selection-sort)  ...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e        for j in range(len(x) - i):\n            if x[j] \u003e curr_max:\n                curr_max = x[j]\n                curr_max_idx = j...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e```python\ndef bubble_sort(x):\n    for i in range(len(x) - 1):\n        for j in range(i + 1, len(x)):\n            if x[i] \u003e x[j]:\n                x[i], x[j] = x[j], x[i]\n    return x...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e    # recursively sort the two halves\n    mid = len(x) \u002f\u002f 2\n    sorted_left = merge_sort(x[:mid])\n    sorted_right = merge_sort(x[mid:])...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e    while i \u003c len(sorted_left) and j \u003c len(sorted_right):\n        if sorted_left[i] \u003c sorted_right[j]:\n            merged.append(sorted_left[i])\n            i += 1\n        else:\n            merged.app...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e    # since slicing forgives out of bounds starts\n    # hence, this will work when i \u003e= len(sorted_left)\n    merged.extend(sorted_left[i:])\n    merged.extend(sorted_right[j:])...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e    pivot = x[-1] # preferrable to modifying the input with x.pop()\n    lower = []\n    higher = []...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e    # populate lower and higher in one loop,\n    # instead of two list comprehensions\n    for num in x[:-1]:\n        if num \u003c= pivot:\n            lower.append(num)\n        else:\n            higher.app...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e- [Sorting Out The Basics Behind Sorting Algorithms](https:\u002f\u002fmedium.com\u002fbasecs\u002fsorting-out-the-basics-behind-sorting-algorithms-b0a032873add)...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eTraversing a linked list simply consists on passing through every element. We can do that starting from the head, following the pointer to the next node and so on....","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e\ndef create_list():\n    a = Node(\"A\")\n    b = Node(\"B\")\n    c = Node(\"C\")\n    d = Node(\"D\")\n    a.next = b\n    b.next = c\n    c.next = d\n    return a...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e```python\ndef fetch_values(node):\n    if not node: return values\n    return [node.val] + fetch_values(node.next)...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eOften multiple pointers are needed in order to perform certain operations on the list, like reversing it or deleting an element in the middle....","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e    while curr:\n        right = curr.next\n        curr.next = left\n        left, curr = curr, right...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e        if counter & 1:\n            tail.next = curr_1\n            curr_1 = curr_1.next\n        else:\n            tail.next = curr_2\n            curr_2 = curr_2.next...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eUsing two pointers that iterate the list at different speeds can help with multiple problems: finding the middle of a list, detecting cycles, or finding the element at a certain distance from the end....","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e```python\ndef find_middle(head):\n    fast = slow = head\n    while fast and fast.next:\n        fast = fast.next.next\n        slow = slow.next\n    return slow.val...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eA very useful algorithm to know is how to iterate a BST in order, from the smallest to the largest value in the tree. It has a very compact recursive implementation:...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e```python\nclass TreeNode:\n    def __init__(self, val=0, left=None, right=None):\n        self.val = val\n        self.left = left\n        self.right = right...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e\ndef inorder_traversal(root):\n    if root:\n        return inorder_traversal(root.left) + [root.val] + inorder_traversal(root.right)\n    else:\n        return []\n```...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eHowever, a non-recursive implementation might be more easily adaptable to other problems:...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e        root = stack.pop()\n        output.append(root.val)\n        root = root.right...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e\n# Construct the BST\n#       3\n#      \u002f \\\n#     1   4\n#      \\\n#       2\nroot = TreeNode(3)\nroot.left = TreeNode(1)\nroot.right = TreeNode(4)\nroot.left.right = TreeNode(2)...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eThe bread and butter of graph problems are traversal algorithms. Let's study them....","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eIn a depth-first traversal (DFT), given a starting node, we recursively visit each of its neighbors before moving to the next one. In a 2D grid, it would involve picking a direction, and following it ...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e1. When we visit a node, we push all of its neighbors. Hence, each frame in the stack is a node to visit.\n2. We pop from the stack to visit the next node. Then we add its neighbors to the stack and co...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e```python\ngraph = {\n    \"a\": {\"b\", \"c\"},\n    \"b\": {\"d\"},\n    \"c\": {\"e\"},\n    \"d\": {\"f\"},\n    \"e\": set(),\n    \"f\": set(),\n}...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003edef depth_first_print(graph: dict[str, set[str]], seed: str) -\u003e None:\n    stack = [seed]...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e    while stack:\n        curr_node = stack.pop()\n        print(curr_node)\n        stack.extend(graph[curr_node])...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e```python\ndef depth_first_print(graph: dict[str, set[str]], seed: str) -\u003e None:\n    print(seed)\n    for neighbor in graph[seed]:\n        depth_first_print(graph, neighbor)...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eFor a graph with nodes $$V$$ and edges $$E$$, the time complexity is $$O(\\|V\\|+\\|E\\|)$$ and the space complexity is $$O(\\|V\\|)$$....","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e**Note:** Watch out for _cycles_. Without explicing handling, we might get stuck in infinite traversals. We can keep track of which nodes we have visited using a set, and exit early as soon as we re-v...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e**Note:** Some corner cases are the empty graph, graphs with one or two nodes, graphs with multiple components and graphs with cycles....","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eIn a breadth-first traversal (BFT), given a starting node, we first visit its neighbors, then their neighbors, and so on....","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eIn a 2D grid, it doesn't favour any direction. Instead, it looks like a water ripple....","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e1. When we visit a node, we push all of its neighbors to the queue. As in DFT, each item is a node to visit.\n2. We popleft to get the next node. We push allof its neighbors.\n3. As before, once the que...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e```python\ngraph = {\n    \"a\": {\"b\", \"c\"},\n    \"b\": {\"d\"},\n    \"c\": {\"e\"},\n    \"d\": {\"f\"},\n    \"e\": set(),\n    \"f\": set(),\n}...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003edef breadth_first_print(graph: dict[str, set[str]], seed: str) -\u003e None:\n    queue = deque([seed])...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e    while queue:\n        curr_node = queue.popleft()\n        print(curr_node)\n        queue.extend(graph[curr_node])...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eFor a graph with nodes $$V$$ and edges $$E$$, the time complexity is $$O(\\|V\\|+\\|E\\|)$$ and the space complexity is $$O(\\|V\\|)$$....","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eA topological sort (or _top sort_) is an algorithm whose input is a DAG, and whose output is an array such that every node appears after all the nodes that point at it. (Note that, in the presence of ...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e1. Compute the indegree of every node, store it in a hash map.\n1. Identify a node with no inbound edges in our hash map.\n1. Add the node to the ordering.\n1. Decrement the indegree of its neighbors.\n1....","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003ePut together, the time complexity of top sort is $$O(\\|V\\| + \\|E\\|)$$, and the space complexity, $$O(\\|V\\|)$$....","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eAs for graph related problems, problems involving trees often require traversals, either [depth](#depth-first-traversal) or [breadth](#breadth-first-traversal) first. The same principles and data stru...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e- [Graph Algorithms for Technical Interviews - Full Course](https:\u002f\u002fwww.youtube.com\u002fwatch?v=tWVWeAqZ0WU)...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eThe two pointer approach can be used in problems involving searching, comparing and modifying elements in a sequence. A naive approach would involve two loops, and hence take $$O(n^2)$$ time. Instead,...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e**Note:** Some two pointer problems require the sequence to be sorted to move the pointers efficiently. For instance, to find the two elements that produce a sum, having a sorted array is key to know ...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e**Note:** Sometimes we need to iterate an $$m \\times n$$ table. While we can use two pointers for that, we can to with a single pointer $$i \\in [0, m \\times n)$$: `row = i \u002f\u002f n`, `col = i % n`....","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eThey are optimization problems involving **contiguous** sequences (substrings, subarrays, etc.), particularly involving cumulative properties...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eThe general approach consists on starting with two pointers, `st` and `ed` at the beginning of the sequence...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eWe can keep track of the cumulative property and update it as the window expands or contracts...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e- The candidate solutions are built incrementally.\n- The solutions have **constraints**, so not all candidates are valid....","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eSince solutions are built incrementally, backtracting they can be visualized as a **depth-first search** on a tree. At each node, the algorithm checks if it will lead to a valid solution. If the answe...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e**Note:** Because of the need to backtrack, a recursive implementation of the DFS is often more convenient, since undoing a step simply involves invoking `return`. A stack might require a more elabora...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eAs we will see in a few examples, the solution to a backtracking problem looks like this:...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e    for child in get_children(candidate):\n        if is_valid(child):\n            place(child)\n            solve(child)\n            remove(child)\n```...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eA famous application of backtracking is solving the [eight queens puzzle](https:\u002f\u002fen.wikipedia.org\u002fwiki\u002fEight_queens_puzzle):...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e\u003e The eight queens puzzle is the problem of placing eight chess queens on an 8\u00d78 chessboard so that no two queens threaten each other; thus, a solution requires that no two queens share the same row, ...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003edef under_attack(row, col):\n    for row_i, col_i in board:\n        if row_i == row or col_i == col:\n            return True...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e        # check the diagonals\n        if abs(row_i - row) == abs(col_i - col):\n            return True...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e    for col in range(8):\n        # check the constraints: the explored square\n        # is not under attack\n        if not under_attack(row, col):\n            board.append((row, col))\n            # ex...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003etotal_solutions = eight_queens()\nprint(f\"Total solutions: {total_solutions}\")\n```...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eboard = [[0, 0, 0, 1, 0, 0, 0, 0, 5],\n         [0, 0, 0, 0, 0, 4, 0, 1, 0],\n         [1, 0, 3, 0, 0, 8, 4, 2, 7],\n         [0, 0, 1, 7, 4, 6, 0, 9, 0],\n         [0, 0, 6, 0, 3, 2, 1, 0, 8],\n         [...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e    for i in range(9):\n        if board[row][i] == num:\n            return False\n        elif board[i][col] == num:\n            return False\n        if board[block_row + i \u002f\u002f 3][block_col + i % 3] == ...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e            for num in range(1, 10):\n                if is_valid(board, row, col, num):\n                    board[row][col] = num\n                    if solve(board):\n                        return Tr...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e```\n[[2, 7, 4, 1, 9, 3, 8, 6, 5],\n [6, 5, 8, 2, 7, 4, 3, 1, 9],\n [1, 9, 3, 6, 5, 8, 4, 2, 7],\n [5, 8, 1, 7, 4, 6, 2, 9, 3],\n [7, 4, 6, 9, 3, 2, 1, 5, 8],\n [9, 3, 2, 5, 8, 1, 6, 7, 4],\n [3, 2, 7, 8, 1,...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e    for i in range(size):\n        # exclude element i\n        rest = nums[:i] + nums[i+1:]\n        perms = [[nums[i]] + x for x in permute(rest)]\n        res.extend(perms)...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eThe key to the problem is identifying the _trivially_ smallest input, the case for which the answer is trivially simple....","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eRecursion is a technique in to solve problems which in turn depend on solving smaller subproblems. It permeates many other methods, like [backtracking](#backtracking-problems), [merge sort](#merge-sor...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e1. Definition of the **base case(s)**, the case(s) in which solving a problem is trivial, and a solution is provided, stopping the recursion.\n1. Divide the problem into smaller subproblems, which are ...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eThe space complexity of recursion will be, at least, the length of the stack which accumulates all the function calls....","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e**Note:** CPython's recursion limit is 1,000. This can limit to the depth of the problems we can tackle....","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eIn DP, combining recursion and memoization is a powerful way to trade space complexity for time complexity. Specifically, since problems are overlapping, it is likely we are solving the same subproble...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eHere is a recipe for solving these problems (from [here](https:\u002f\u002fwww.youtube.com\u002fwatch?v=oBt53YbR9Kk)):...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e1. Visualize the problem as a tree\n1. Implement the tree using recursion, in which the leaves are the base cases. This will produce the brute force solution.\n1. Test it for a few simple cases.\n1. Memo...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e- `m`: the average length of the elements of the input. For instance, if the input is a list, `m = len(input)`; it it is an integer, it is `m = input`. This will impact the height of the tree.\n- `n`: ...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e**Brute force:** for every node, we have a `n` options. Usually, the time complexity of DP problems will be exponential, of $$O(n^m*k)$$, where $k$ is the complexity of a single recursive call. The me...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e**Memoized:** memoization reduces the branching factor by storing previous results. In other words, it trades time complexity for space complexity; usually both become polynomial....","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e   1. Design the size of the table based on the inputs. Often the size of the table is one unit longer in each dimension than the respective inputs. That allows us to include the trivial case (usually...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e   1. Infuse the trivial answer into the table, the case for which we immediately know the answer...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e1. Iterate through the table, filling the positions ahead based on the current position....","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e1. Note that sometimes the trivial case might not have the solution we need to solve the algorithm. Watch out for such situations....","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eThese are some materials that helped me understand dynamic programming (the order matters!):...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e1. [A graphical introduction to dynamic programming](https:\u002f\u002favikdas.com\u002f2019\u002f04\u002f15\u002fa-graphical-introduction-to-dynamic-programming.html)\n1. [Dynamic Programming - Learn to Solve Algorithmic Problems ...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e```python\ndef how_sum(target: int, nums: list[int], memo: dict = {}) -\u003e None | list[int]:\n    if target == 0: return []\n    if target \u003c 0: return None\n    if target in memo.keys(): return memo[target]...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e```python\ndef best_sum(target: int, nums: list[int], memo: dict = {}) -\u003e None | list[int]:...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e    if target in memo: return memo[target]\n    if target == 0: return []\n    if target \u003c 0: return None...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e        if solution is not None and len(solution) \u003c length_best_solution:\n            memo[target] = solution + [num]\n            length_best_solution = len(memo[target])...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eprint(best_sum(7, [5, 3, 4, 7]))\nprint(best_sum(8, [1, 4, 5]))\nprint(best_sum(100, [1, 2, 5, 25]))\n```...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e```python\ndef can_construct(target: str, dictionary: list, memo: dict = {}) -\u003e bool:...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e    for word in dictionary:\n        if target.startswith(word):\n            new_target = target.removeprefix(word)\n            if can_construct(new_target, dictionary, memo):\n                memo[targ...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eprint(can_construct(\"abcdef\", [\"ab\", \"abc\", \"cd\", \"def\", \"abcd\"]))\nprint(can_construct(\"skateboard\", [\"bo\", \"rd\", \"ate\", \"t\", \"ska\", \"sk\", \"boar\"]))\nprint(can_construct(\"eeeeeeeeeeeeeeeeeeeeeeeeeeeeee...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e```python\ndef count_construct(target: str, dictionary: list, memo: dict = {}) -\u003e int:...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e    for word in dictionary:\n        if target.startswith(word):\n            new_target = target.removeprefix(word)\n            memo[target] += count_construct(new_target, dictionary, memo)...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eprint(count_construct(\"abcdef\", [\"ab\", \"abc\", \"cd\", \"def\", \"abcd\"]))\nprint(count_construct(\"purple\", [\"purp\", \"p\", \"ur\", \"le\", \"purpl\"]))\nprint(count_construct(\"skateboard\", [\"bo\", \"rd\", \"ate\", \"t\", \"...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e```python\ndef all_construct(target: str, dictionary: list, memo: dict = {}) -\u003e list[list[str]]:...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e    for word in dictionary:\n        if target.startswith(word):\n            new_target = target.removeprefix(word)\n            constructs = all_construct(new_target, dictionary, memo)\n            cons...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eprint(all_construct(\"abcdef\", [\"ab\", \"abc\", \"cd\", \"def\", \"abcd\", \"ef\", \"c\"]))\nprint(all_construct(\"purple\", [\"purp\", \"p\", \"ur\", \"le\", \"purpl\"]))\nprint(all_construct(\"skateboard\", [\"bo\", \"rd\", \"ate\", \"...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e    for i in range(n):\n        table[i + 1] += table[i]\n        table[i + 2] += table[i]...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e    for i in range(m + 1):\n        for j in range(n + 1):\n            if (i + 1) \u003c= m:\n                grid[i + 1][j] += grid[i][j]\n            if (j + 1) \u003c= n:\n                grid[i][j + 1] += grid[...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eprint(grid_traveler(1, 1))\nprint(grid_traveler(2, 3))\nprint(grid_traveler(3, 2))\nprint(grid_traveler(3, 3))\nprint(grid_traveler(18, 18))\n````...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e```python\ndef can_sum_t(target: int, nums: list) -\u003e bool:\n    \"\"\"\n    Complexity:\n        - Time: O(m*n)\n        - Space: O(m)\n    \"\"\"...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e        for num in nums:\n            if (i + num) \u003c= len(grid):\n                grid[i + num] = True...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eprint(can_sum_t(7, [2 ,3])) # True\nprint(can_sum_t(7, [5, 3, 4])) # True\nprint(can_sum_t(7, [2 ,4])) # False\nprint(can_sum_t(8, [2, 3, 5])) # True\nprint(can_sum_t(300, [7, 14])) # False\n```...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e```python\ndef how_sum_t(target: int, nums: list[int]) -\u003e None | list[int]:\n    \"\"\"\n    Complexity:\n        - Time: O(m*n^2)\n        - Space: O(m*n)\n    \"\"\"\n    grid = [None] * (target + 1)\n    grid[0]...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e        for num in nums:\n            if (i + num) \u003c len(grid):\n                grid[i + num] = grid[i].copy()\n                grid[i + num].append(num)...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e\nprint(how_sum_t(7, [2 ,3])) # [2, 2, 3]\nprint(how_sum_t(7, [5, 3, 4, 7])) # [3, 4]\nprint(how_sum_t(7, [2 ,4])) # None\nprint(how_sum_t(8, [2, 3, 5])) # [2, 2, 2, 2]\nprint(how_sum_t(300, [7, 14])) # No...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e```python\ndef best_sum_t(target: int, nums: list[int], memo: dict = {}) -\u003e None | list[int]:\n    \"\"\"\n    Complexity:\n        - Time: O(m*n^2)\n        - Space: O(m^2)\n    \"\"\"...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e        for num in nums:\n            if (i + num) \u003c len(grid):\n                if grid[i + num] is None or len(grid[i + num]) \u003e len(grid[i]):\n                    grid[i + num] = grid[i].copy()\n       ...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eprint(best_sum_t(7, [2 ,3])) # [2, 2, 3]\nprint(best_sum_t(7, [5, 3, 4, 7])) # [7]\nprint(best_sum_t(7, [2 ,4])) # None\nprint(best_sum_t(8, [2, 3, 5])) # [5, 3]\nprint(best_sum_t(300, [7, 14])) # None\n``...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e```python\ndef can_construct_t(target: str, words: list[str]) -\u003e bool:\n    \"\"\"\n    Complexity:\n        - Time: O(m^2*n)\n        - Space: O(m)\n    \"\"\"...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e        prefix = target[:i]\n        for word in words:\n            if (i + len(word)) \u003e= len(grid):\n                continue...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e            if target.startswith(prefix + word):\n                grid[i + len(word)] = True...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eprint(can_construct_t(\"abcdef\", [\"ab\", \"abc\", \"cd\", \"def\", \"abcd\"])) # True\nprint(can_construct_t(\"skateboard\", [\"bo\", \"rd\", \"ate\", \"t\", \"ska\", \"sk\", \"boar\"])) # False\nprint(can_construct_t(\"enterapot...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e```python\ndef count_construct_t(target: str, words: list[str]) -\u003e int:\n    \"\"\"\n    Complexity:\n        - Time: O(m^2*n)\n        - Space: O(m)\n    \"\"\"\n    grid = [0] * (len(target) + 1)\n    grid[0] = 1...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e        for word in words:\n            if (i + len(word)) \u003e= len(grid):\n                continue...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e            if target.startswith(prefix + word):\n                grid[i + len(word)] += grid[i]...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eprint(count_construct_t(\"abcdef\", [\"ab\", \"abc\", \"cd\", \"def\", \"abcd\"])) # 1\nprint(count_construct_t(\"purple\", [\"purp\", \"p\", \"ur\", \"le\", \"purpl\"])) # 2\nprint(count_construct_t(\"skateboard\", [\"bo\", \"rd\",...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003edef all_construct_t(target: str, words: list[str]) -\u003e list[list[str]]:\n    \"\"\"\n    Complexity:\n        - Time: O(n^m)\n        - Memory: O(n^m)\n    \"\"\"...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e        for word in words:\n            if (i + len(word)) \u003e len(grid):\n                continue...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e            if target.startswith(prefix + word):\n                new_constructs = deepcopy(grid[i])\n                for x in new_constructs:\n                    x.append(word)...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e                if grid[i + len(word)]:\n                    grid[i + len(word)].extend(new_constructs)\n                else:\n                    grid[i + len(word)] = new_constructs...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eprint(all_construct_t(\"abcdef\", [\"ab\", \"abc\", \"cd\", \"def\", \"abcd\", \"ef\", \"c\"])) # [['ab', 'cd', 'ef'], ['ab', 'c', 'def'], ['abc', 'def'], ['abcd', 'ef']]...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eprint(all_construct_t(\"purple\", [\"purp\", \"p\", \"ur\", \"le\", \"purpl\"])) # [['purp', 'le'], ['p', 'ur', 'p', 'le']]...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eprint(all_construct_t(\"skateboard\", [\"bo\", \"rd\", \"ate\", \"t\", \"ska\", \"sk\", \"boar\"])) # []...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003eprint(all_construct_t(\"enterapotentpot\", [\"a\", \"p\", \"ent\", \"enter\", \"ot\", \"o\", \"t\"])) # # [['enter', 'a', 'p', 'ot', 'ent', 'p', 'ot'], ['enter', 'a', 'p', 'ot', 'ent', 'p', 'o', 't'], ['enter', 'a', ...","\u003cb\u003eCatalog of Algorithms\u003c\u002fb\u003e\u003cbr\u003e```...","\u003cb\u003eHow to take coding interviews\u003c\u002fb\u003e\u003cbr\u003e# 1. Problem statement...","\u003cb\u003eHow to take coding interviews\u003c\u002fb\u003e\u003cbr\u003eThe interviewer might come with a written down problem statement. They might share it with us ahead of our meeting or right at the start....","\u003cb\u003eHow to take coding interviews\u003c\u002fb\u003e\u003cbr\u003e      - What are its data types? Is it sorted? Do we know the range of the integers? (Can they be negative?) A batch of a stream? Et cetera....","\u003cb\u003eHow to take coding interviews\u003c\u002fb\u003e\u003cbr\u003e      - Expected input size: if they know it, might give an idea of the complexity we should aim for. For inputs of size 1 to 100, $$O(n^2)$$ is acceptable; for larger inputs, we should do better....","\u003cb\u003eHow to take coding interviews\u003c\u002fb\u003e\u003cbr\u003e   1. Ask about the specific runtime our solution will need. That will be very useful to screen out solutions and algorithms....","\u003cb\u003eHow to take coding interviews\u003c\u002fb\u003e\u003cbr\u003eWhile it can be tempting to start implementing a solution right away, it is worth spending some time drafting the problem. After all, our interviewer will have given it some thought already, and could...","\u003cb\u003eHow to take coding interviews\u003c\u002fb\u003e\u003cbr\u003e1. Try to match this problem to the problems you have seen. Here's a cheat sheet for [data structures]({% post_url 2024-02-15-data-structures %}):...","\u003cb\u003eHow to take coding interviews\u003c\u002fb\u003e\u003cbr\u003e   - Linked lists: if we require fast insertions and deletions, especially when order matters...","\u003cb\u003eHow to take coding interviews\u003c\u002fb\u003e\u003cbr\u003e   - Union-finds: if we're investigating the if sets are connected or cycles exist in a graph...","\u003cb\u003eHow to take coding interviews\u003c\u002fb\u003e\u003cbr\u003e     And here's a list of common [algorithms]({% post_url 2024-02-15-algorithms %}):...","\u003cb\u003eHow to take coding interviews\u003c\u002fb\u003e\u003cbr\u003e1. Don't be shy! Let the interviewer hear out your thought process. They will surely appreciate knowing whats on your mind, and be able to chip in. Specially, if they do say something, _listen_. After...","\u003cb\u003eHow to take coding interviews\u003c\u002fb\u003e\u003cbr\u003e1. Once you seem to have converged to a specific approach, state the main parts of the algorithm and make sure they understand and agree....","\u003cb\u003eHow to take coding interviews\u003c\u002fb\u003e\u003cbr\u003e   - We might want to start with a suboptimal solution, as long as we let them know that we know that! Once we have that working, we can identify the main bottlenecks and go back to the drawing board....","\u003cb\u003eHow to take coding interviews\u003c\u002fb\u003e\u003cbr\u003eDuring the implementation phase, it might help to go from the big picture to the small picture. Start by defining the global flow of the program, calling unimplemented functions with clear names. This...","\u003cb\u003eHow to take coding interviews\u003c\u002fb\u003e\u003cbr\u003eIn order to allow our interviewer follow our logic, it is important that they can follow along:...","\u003cb\u003eHow to take coding interviews\u003c\u002fb\u003e\u003cbr\u003e- Make sure they are ok with us using additional dependencies. They might prefer to keep the algorithm lean!...","\u003cb\u003eHow to take coding interviews\u003c\u002fb\u003e\u003cbr\u003e- If you realize your solution might not work, let them know. You might need to go back to brainstorming....","\u003cb\u003eHow to take coding interviews\u003c\u002fb\u003e\u003cbr\u003e- Keep your code clean: avoid duplicated code, use helper functions, keep function and variable names understandable....","\u003cb\u003eHow to take coding interviews\u003c\u002fb\u003e\u003cbr\u003e- Scan the code for mistakes. For instance, when working with arrays, index errors are common.\n- Compute the complexity of your code. This might hint at what could be improved. It might also highlight...","\u003cb\u003eHow to take coding interviews\u003c\u002fb\u003e\u003cbr\u003eOnce our solution is ready, it might be a good idea to give it a go. Simply call your function on a few examples. Consider:...","\u003cb\u003eHow to take coding interviews\u003c\u002fb\u003e\u003cbr\u003eIf some examples fail, we need to debug our code. Throw in a few print statements, predict what you expect to see, and go for it....","\u003cb\u003eHow to take coding interviews\u003c\u002fb\u003e\u003cbr\u003eAfter successfully presenting a solution, our interviewer might have some follow-up questions:...","\u003cb\u003eHow to take coding interviews\u003c\u002fb\u003e\u003cbr\u003e  - Time and space complexity? Usually, we should consider the worst case complexity, but if the amortized case is significantly better you should point it out....","\u003cb\u003eHow to take coding interviews\u003c\u002fb\u003e\u003cbr\u003e    - While abstracting specific aspects into functions is helpful, it might also be less efficient (e.g., if we have to iterate the input multiple times instead of one)....","\u003cb\u003eHow to take coding interviews\u003c\u002fb\u003e\u003cbr\u003e  - Consider non-technical constraints, such as development time, maintainability, or extensibility....","\u003cb\u003eHow to take coding interviews\u003c\u002fb\u003e\u003cbr\u003e- Identify the best theoretical time complexity. This involves considering what is the minimum number of operations involved. For instance if we need to visit every element, probably $$O(n)$$ is optim...","\u003cb\u003eHow to take coding interviews\u003c\u002fb\u003e\u003cbr\u003e\u003e Some algorithms have some implicit and potentially unexpected behaviors. Visit the [algorithms]({% post_url 2024-02-15-algorithms %}) post, and `Ctrl + F` \"Note:\" in order to find some of them....","\u003cb\u003eHow to take coding interviews\u003c\u002fb\u003e\u003cbr\u003e- [\"Blind 75\" problem set](https:\u002f\u002fwww.teamblind.com\u002fpost\u002fNew-Year-Gift---Curated-List-of-Top-75-LeetCode-Questions-to-Save-Your-Time-OaM1orEU)\n- [Code templates](https:\u002f\u002fleetcode.com\u002fexplore\u002fintervie...","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003eGraphs are data structures composed of a set of objects (_nodes_) and pairwise relationships between them (_edges_). Notably, edges can have properties, like a direction or a weight....","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003e- Adjacency matrices: matrices in which every row $$i$$ contains the edges of node $$i$$. Specifically, $$\\text{row}_{ij}$$ is 1 if nodes $$i$$ and $$j$$ are connected, and 0 otherwise. They are symme...","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003eA common type of graph in computer science are grids, in which nodes are laid in a grid, and they are connected to the nodes selected top, bottom, left and right....","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003eA tree is a graph in which there is only one path between every pair of nodes. Some concepts related to trees are: root, the (only) node on level 1; parent, the connected node in the level above; chil...","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003eOften trees are represented using classes. Specifically, we would have an object `Node` like:...","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003e```python\nclass Node:\n    def __init__(self, val=None):\n        self.val = val\n        self.left = None\n        self.right = None\n```...","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003eWe would keep a reference to the root, and build a try by successively creating new nodes and assigning them to `.left` or `.right`....","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003e(Min-)Heaps are binary trees in which the value of every parent is lower or equal than any of its children. This gives them their most interesting property: the minimum element is always on top. (Simi...","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003eIn Python, [`heapq`](https:\u002f\u002fdocs.python.org\u002f3\u002flibrary\u002fheapq.html) provides an implementation of the heap. Any populated list can be transformed in-place into a heap:...","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003eThe elements have been reordered to represent a heap: each parent note is indexed by $$k$$, and its children by $$2k+1$$ and $$2k+2$$....","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003e  - Push, then pop:\n    ```python\n    heapq.heappushpop(x, -7) # [-5, 2, 5, 3, 123, 6, 8]\n    ```\n    ```\n    -7\n    ```\n  - Pop, then push:\n    ```python\n    heapq.heapreplace(x, -7) # [-7, 2, 5, 3, ...","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003e**Note:** Heaps are great to recover the smallest element, but not the k\u003csup\u003eth\u003c\u002fsup\u003e smallest one. [BSTs](#binary-search-trees) might me more appropriate for that....","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003eBinary serach trees (BSTs) are binary trees in which every node meets two properties:...","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003e- All descendants on the left are smaller than the parent node.\n- All descendants on the right are larger than the parent node....","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003e- Search: done recursively on the tree. When balanced, search is as good as binary search on a sorted array.\n- Insertion: also done recursively, by traversing the tree from the root in order until we ...","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003eThe time complexity of both is $$O(\\log n)$$ when the tree is **balanced**; otherwise it is $$O(n)$$. (Balanced trees are those whose height is small compared to the number of nodes. Visually, they lo...","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003e# Tries...","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003e- Nodes represent characters, except for the root, represents the string start.\n- Children represent each of the possible characters that can follow the parent.\n- Leaf nodes represent the end of the s...","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003e1. Saving space when storing words sharing the same prefix, since they only store the prefix once.\n1. Searching words, which can be done in $$O(\\text{word length})$$. Similarly, they make it very fast...","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003eThese two properties make them excellent at handling spell checking and autocomplete functions....","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003eUnion-finds, also known as Disjoint-sets, store a collection of non-overlapping sets. Internally, sets are represented as directed trees, in which every member points towards the root of the tree. The...","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003e- **Find:** returns the set an element belongs to. Specifically, it returns its representative.\n- **Union:** combines two sets. Specifically, first, it performs two finds. If the representatives diffe...","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003eUnion-finds can be represented as an array, in which every member of the universal set is one element. Members linked to a set take as value the index of another member of the set, often the root. Con...","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003eEvery set has a property, the _rank_, which approximates its depth. Union is performed _by rank_: the root with the highest rank is picked as the new root. Find performs an additional step, called _pa...","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003e```python\nclass UnionFind:\n    def __init__(self, size):\n        self.parent = [i for i in range(size)]\n        self.rank = [0] * size...","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003e    def find(self, x):\n        if self.parent[x] != x:\n            self.parent[x] = self.find(self.parent[x])  # Path compression\n        return self.parent[x]...","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003e    def union(self, x, y):\n        rootX = self.find(x)\n        rootY = self.find(y)\n        if rootX != rootY:\n            if self.rank[rootX] \u003e self.rank[rootY]:\n                self.parent[rootY] =...","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003eBloom filters are data structures to probabilistically check if an element is a member of a set. It can be used when false positives are acceptable, but false negatives are not. For instance, if we ha...","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003eThe core structure underlying bloom filters is a bit array, which makes it highly compact in memory. When initialized, all the positions are set to 0. When inserting a given element, we apply multiple...","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003e_See [how they relate to RAGs]({% post_url 2025-08-16-rags %}#nearest-neighbor-search)._...","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003e**Skip lists** are a data structure consisting of a set of [linked lists](#linked-lists), each one containing a subset of the items in the collection:...","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003e{% include figure.liquid path=\"assets\u002fimg\u002fposts\u002f2025-08-16-rags\u002fskip_list.png\" class=\"img-fluid\" %}...","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003e\u003cdiv class=\"caption\"\u003e\n    Example of search path for a value (9) in a 5-layered skip list.\n\u003c\u002fdiv\u003e...","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003eThe topmost list contains only a few items, while the bottommost list contains all the items. Each item in a list points to the next item in the same list, and also to the next item in the lists below...","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003e    if node.right and node.right.value \u003c query_number:\n        # keep moving right whenever possible\n\t\treturn find_entry(node.right, query_number)\n    elif node.down:\n        # move down when we can't...","\u003cb\u003eCompendium of Data Structures\u003c\u002fb\u003e\u003cbr\u003eA linked list is a DAG in which almost every node has exactly one inbound edge and one outbound edge. The exceptions are the _head_, a node with no inbound egde, and the _tail_, a node with no outboun...","\u003cb\u003ePandas\u003c\u002fb\u003e\u003cbr\u003e# Data structures prov...","\u003cb\u003ePandas\u003c\u002fb\u003e\u003cbr\u003ePandas provides several data structures, out of which two are particularly popular: Series and DataFrames....","\u003cb\u003ePandas\u003c\u002fb\u003e\u003cbr\u003e## Series...","\u003cb\u003ePandas\u003c\u002fb\u003e\u003cbr\u003eA Series is a vector-like structure, that extends [NumPy vectors]({% post_url 2024-02-04-python-vectors %}#the-inner-workings-of-numpy-arrays)....","\u003cb\u003ePandas\u003c\u002fb\u003e\u003cbr\u003eThe Series stores the data as a NumPy vectors, inheriting its advantages and disadvantages. But computations on Series come with an extra overhead, since Pandas puts extra effort in handling missing v...","\u003cb\u003ePandas\u003c\u002fb\u003e\u003cbr\u003eDataFrames are matrix-like structures, which build on top of Series. They can be created in multiple ways, some of which are:...","\u003cb\u003ePandas\u003c\u002fb\u003e\u003cbr\u003e- A dictionary of lists\u002farrays\u002fseries\n  ```python\n  data = {'Column1': [1, 2, 3], 'Column2': ['A', 'B', 'C']}\n  df = pd.DataFrame(data)\n  ```\n- A list of dictionaries:\n  ```python\n  data = [{'Column1'...","\u003cb\u003ePandas\u003c\u002fb\u003e\u003cbr\u003eThe DataFrame stores data as multiple Series with a shared index. While the data of a Series lives altogether, the different Series of a DataFrame are scattered in memory. In consequence, adding a new...","\u003cb\u003ePandas\u003c\u002fb\u003e\u003cbr\u003eAs in NumPy vectors, we can access a Series' elements using their _positional_ indexes. But, furthermore, it has an _index_, a hash map structure which allows us to access each element in the array us...","\u003cb\u003ePandas\u003c\u002fb\u003e\u003cbr\u003e- `.iloc[]` uses the positional indices, and slicing works as usual:\n  ```python\n  x.iloc[2:3]\n  ```\n  ```\n  c    2\n  dtype: int64\n  ```\n- `.loc[]` uses labels, and slicing includes both beginning and...","\u003cb\u003ePandas\u003c\u002fb\u003e\u003cbr\u003eDataFrames also have a `.loc[]` and an `.iloc[]` function, which accepts columns as a second argument....","\u003cb\u003ePandas\u003c\u002fb\u003e\u003cbr\u003eThanks to their dictionary-like properties, indexes allow to access an element in constant time. However, including non-unique indexes might lead to a worst case $$O(n)$$ lookup time....","\u003cb\u003ePandas\u003c\u002fb\u003e\u003cbr\u003eUnless otherwise specified, the index gets initialized to a (lazy) enumeration of the rows\u002fitems. We can access the index using `.index()`, and revert it to this default behaviour using `.reset_index(...","\u003cb\u003ePandas\u003c\u002fb\u003e\u003cbr\u003e[MultiIndex](https:\u002f\u002fpandas.pydata.org\u002fdocs\u002fuser_guide\u002fadvanced.html) is an index in which is key is a (unique) tuple. We can create them from lists of lists or of tuples, from DataFrames, or from the...","\u003cb\u003ePandas\u003c\u002fb\u003e\u003cbr\u003eindex = pd.MultiIndex.from_product((class_1, class_2),\n                                   # the name of the levels themselves\n                                   names = [\"first\", \"second\"])\nx.index = ...","\u003cb\u003ePandas\u003c\u002fb\u003e\u003cbr\u003ex\n```...","\u003cb\u003ePandas\u003c\u002fb\u003e\u003cbr\u003e```\nfirst  second\nfoo    1         1\n       2         2\nbar    1         3\n       2         4\ndtype: int64\n```...","\u003cb\u003ePandas\u003c\u002fb\u003e\u003cbr\u003eAs shown above, the items within a particular position in the tuple to not need to be unique within that position. This allows to select subgroups using **partial** indexes:...","\u003cb\u003ePandas\u003c\u002fb\u003e\u003cbr\u003e[As NumPy]({% post_url 2024-02-04-python-vectors %}#views-copies-and-in-place-operations), Pandas distinguishes between _viewing_ an object and _copying_ it....","\u003cb\u003ePandas\u003c\u002fb\u003e\u003cbr\u003e- [Pandas Illustrated: The Definitive Visual Guide to Pandas](https:\u002f\u002fbetterprogramming.pub\u002fpandas-illustrated-the-definitive-visual-guide-to-pandas-c31fa921a43)...","\u003cb\u003eIntroduction to Graphs\u003c\u002fb\u003e\u003cbr\u003e```mermaid\n---\nconfig:\n  layout: elk\n  lo...","\u003cb\u003eIntroduction to Graphs\u003c\u002fb\u003e\u003cbr\u003e```mermaid\n---\nconfig:\n  layout: elk\n  look: handDrawn\n---\ngraph LR\n    N[North Bank]\n    K[Kneiphof Island]\n    L[Lomse Island]\n    S[South Bank]...","\u003cb\u003eIntroduction to Graphs\u003c\u002fb\u003e\u003cbr\u003e    N === K\n    N === K\n    N === L\n    S === L\n    S === K\n    S === K\n    K === L\n```...","\u003cb\u003eIntroduction to Graphs\u003c\u002fb\u003e\u003cbr\u003eThe problem was to find a path such that a walker would cross each bridge exactly once...","\u003cb\u003eIntroduction to Graphs\u003c\u002fb\u003e\u003cbr\u003eTo solve this problem (by proving it had no solution), Euler found two useful abstractions: vertices representing land masses, and edges representing bridges...","\u003cb\u003eIntroduction to Graphs\u003c\u002fb\u003e\u003cbr\u003eA key insight of framing the problem like this was that a graph can be represented in many ways (e.g., where to position the vertices), and all of them are equivalent...","\u003cb\u003eIntroduction to Graphs\u003c\u002fb\u003e\u003cbr\u003eEuler's theorem (yes, one of them) is the first theorem in graph theory, and proves that a graph with a number of vertices with an odd [degree]({% post_url 2025-02-09-graph-properties %}#degree) other...","\u003cb\u003eIntroduction to Graphs\u003c\u002fb\u003e\u003cbr\u003eIn the 21st century, we define graphs as sets of objects (vertices) and pairwise relations between them (edges)...","\u003cb\u003eIntroduction to Graphs\u003c\u002fb\u003e\u003cbr\u003eIn maths, **[equivalence](https:\u002f\u002fen.wikipedia.org\u002fwiki\u002fEquivalence_relation)** formalize the notion than objects can have a relationship of \"sameness\"...","\u003cb\u003eIntroduction to Graphs\u003c\u002fb\u003e\u003cbr\u003eAn equivalence relation is a binary relation that is reflexive, transitive and symmetric...","\u003cb\u003eIntroduction to Graphs\u003c\u002fb\u003e\u003cbr\u003e\"Is greater than\" is an example of non-equivalence, since it does not meet the symmetric property (e.g., $$2 \u003e 1$$ does not imply that $$1 \u003e 2$$)...","\u003cb\u003eIntroduction to Graphs\u003c\u002fb\u003e\u003cbr\u003eSince edges in a graph also capture this notion of \"sameness\" in some sense, they are tighly connected to equivalences: $$u \\sim v$$ implies that there is a [path]({% post_url 2025-01-23-graphs-glossa...","\u003cb\u003eIntroduction to Graphs\u003c\u002fb\u003e\u003cbr\u003eEquivalently, $$u$$ and $$v$$ are in the same [component]({% post_url 2025-01-23-graphs-glossary %}#component)....","\u003cb\u003eIntroduction to Graphs\u003c\u002fb\u003e\u003cbr\u003eWhere $$V$$ denotes the set of vertices and $$E$$ the set of edges (pairs of vertices)....","\u003cb\u003eIntroduction to Graphs\u003c\u002fb\u003e\u003cbr\u003e\u003e **_Notation note:_** $$V$$ and $$E$$ above refer sets, specifically to the vertex and edge set of a specific graph ($$G$$). Note that they are in italics. In contrast, the $$\\text{V}$$ in $$\\text{V}...","\u003cb\u003eIntroduction to Graphs\u003c\u002fb\u003e\u003cbr\u003e- Undirected graph: $$E \\subseteq \\{ \\{u, v\\} \\mid u, v \\in V \\}$$, i.e., the edges do not have directions....","\u003cb\u003eIntroduction to Graphs\u003c\u002fb\u003e\u003cbr\u003e      vertex_a === vertex_b\n      vertex_a === vertex_c\n      vertex_b === vertex_c\n  ```...","\u003cb\u003eIntroduction to Graphs\u003c\u002fb\u003e\u003cbr\u003e- Directed graphs: $$E \\subseteq \\{ (u, v) \\mid u, v \\in V \\}$$, i.e., the edges have directions....","\u003cb\u003eIntroduction to Graphs\u003c\u002fb\u003e\u003cbr\u003e      vertex_a --\u003e vertex_b\n      vertex_a --\u003e vertex_c\n      vertex_b --\u003e vertex_c\n  ```...","\u003cb\u003eIntroduction to Graphs\u003c\u002fb\u003e\u003cbr\u003eSometimes, graphs are defined as triples. An example are [**multigraphs**]({% post_url 2025-01-23-graphs-glossary %}#multigraph), graphs in which multiple edges between the same pair of vertices are a...","\u003cb\u003eIntroduction to Graphs\u003c\u002fb\u003e\u003cbr\u003e$$\n\\phi(x) = \\begin{cases}\n  \\{ N, K \\} & \\text{if $x = e_1$} \\\\\n  \\{ N, K \\} & \\text{if $x = e_2$} \\\\\n  \\{ N, L \\} & \\text{if $x = e_3$} \\\\\n  \\{ S, L \\} & \\text{if $x = e_4$} \\\\\n  \\{ S, K \\} & \\text{...","\u003cb\u003eIntroduction to Graphs\u003c\u002fb\u003e\u003cbr\u003eAnother type of graph that requires a triple are **weighted** graphs, in which each edge has a weight. They are triples $$G = (V, E, w)$$ in which $$w$$ is a function that maps edges to their weights....","\u003cb\u003eIntroduction to Graphs\u003c\u002fb\u003e\u003cbr\u003e\u003e **Note:** unless specified otherwise, in this series I will focus on [_simple_]({% post_url 2025-01-23-graphs-glossary %}#simple-graph) graphs, which have at most one edge between any pair of vertic...","\u003cb\u003eGraph Glossary\u003c\u002fb\u003e\u003cbr\u003eIn an [undirected](#undirected-graph) graph, a [connected](#connected-graph) [subgraph](#subgraph) that is not part of a larger connected subgraph....","\u003cb\u003eGraph Glossary\u003c\u002fb\u003e\u003cbr\u003eA [trail](#trail) in which the first and last vertices are equal. In contrast to the [cycle](#cycle), any vertex can be repeated....","\u003cb\u003eGraph Glossary\u003c\u002fb\u003e\u003cbr\u003e## Cycle...","\u003cb\u003eGraph Glossary\u003c\u002fb\u003e\u003cbr\u003eA [trail](#trail) in which _only_ the first and last vertices are equal. Except for the tails and in contrast to the [circuit](#circuit), vertices cannot be repeated....","\u003cb\u003eGraph Glossary\u003c\u002fb\u003e\u003cbr\u003eAn example of a flow is a heat diffusion process across a graph. In such processes, each vertex starts with a certain amount of heat and, at each time point, exchanges heat with its [neighbors](#neigh...","\u003cb\u003eGraph Glossary\u003c\u002fb\u003e\u003cbr\u003e## Module...","\u003cb\u003eGraph Glossary\u003c\u002fb\u003e\u003cbr\u003eA [subgraph](#subgraph) whose vertices are densely connected to each other, and loosely to the rest of the graph....","\u003cb\u003eGraph Glossary\u003c\u002fb\u003e\u003cbr\u003eAn orientation of an [undirected](#undirected-graph) graph is the [directed](#directed-graph) graph resulting of assigning a direction to each of its vertices. A [directed](#directed-graph) graph is o...","\u003cb\u003eGraph Glossary\u003c\u002fb\u003e\u003cbr\u003e## Path...","\u003cb\u003eGraph Glossary\u003c\u002fb\u003e\u003cbr\u003eA graph resulting from subsetting vertices from a larger graph, as well as a subset of the edges connecting them....","\u003cb\u003eGraph Glossary\u003c\u002fb\u003e\u003cbr\u003eA [subgraph](#subgraph) containing _all_ the edges connecting the vertices in the original graph....","\u003cb\u003eGraph Glossary\u003c\u002fb\u003e\u003cbr\u003e## Trail...","\u003cb\u003eGraph Glossary\u003c\u002fb\u003e\u003cbr\u003eA set of 3 vertices and at least 2 edges between them, none of which are redundant or loops. _Open_ triplets have exactly 2 edges; _closed_ triplets have exactly 3....","\u003cb\u003eGraph Glossary\u003c\u002fb\u003e\u003cbr\u003e## Walk...","\u003cb\u003eGraph Glossary\u003c\u002fb\u003e\u003cbr\u003eA walk _on a graph_ is an alternating sequence of vertices and edges, such that every vertex is [incident](#incidence) with the previous and the following edge (if any)....","\u003cb\u003eGraph Glossary\u003c\u002fb\u003e\u003cbr\u003eA vertex is adjacent with _another vertex_ if they are connected by an edge. $$u \\sim v$$ denote that $$u$$ and $$v$$ are adjacent....","\u003cb\u003eGraph Glossary\u003c\u002fb\u003e\u003cbr\u003e## Degree...","\u003cb\u003eGraph Glossary\u003c\u002fb\u003e\u003cbr\u003eThe degree of a vertex in a (simple) [undirected](#undirected-graph) graph is the number of edges [incident](#incidence) with that vertex. In a (simple) [directed](#directed-graph) graph we distinguis...","\u003cb\u003eGraph Glossary\u003c\u002fb\u003e\u003cbr\u003eIn a [directed](#directed-graph) graph, the destination _of an edge_ is the vertex at the head of the edge....","\u003cb\u003eGraph Glossary\u003c\u002fb\u003e\u003cbr\u003eThe neighborhood of vertex $$v$$ is the [induced subgraph](#induced-subgraph) containing all the vertices [adjacent](#adjacency) to $$v$$....","\u003cb\u003eGraph Glossary\u003c\u002fb\u003e\u003cbr\u003eA vertex is incident _with an edge_ if the vertex is one of the two vertices the edge connects....","\u003cb\u003eGraph Glossary\u003c\u002fb\u003e\u003cbr\u003e## Source...","\u003cb\u003eGraph Glossary\u003c\u002fb\u003e\u003cbr\u003eIn a [directed](#directed-graph) graph, the source _of an edge_ is the vertex at the tail of the edge....","\u003cb\u003eGraph Glossary\u003c\u002fb\u003e\u003cbr\u003eA [acyclical](#acyclical-graph) graph whose vertices can be divided into two sets such that no pair of vertices in the same set are [adjacent](#adjacency). Often, each of these sets are referred to as...","\u003cb\u003eGraph Glossary\u003c\u002fb\u003e\u003cbr\u003eA simple, [undirected](#undirected-graph) graph in which every pair of vertices are connected by an edge. Complete graph are usually denoted by letter $$K$$ with a subindex that indicates the total nu...","\u003cb\u003eGraph Glossary\u003c\u002fb\u003e\u003cbr\u003eAn [undirected](#undirected-graph) graph in which any two vertices are connected by at most one path. That is, a disjoint union of [trees](#tree)....","\u003cb\u003eGraph Glossary\u003c\u002fb\u003e\u003cbr\u003eA graph in which we can find a path between any two nodes via a greedy strategy that choses the neighbor closest according to a distance function....","\u003cb\u003eGraph Glossary\u003c\u002fb\u003e\u003cbr\u003eAn [undirected](#undirected-graph) graph in which there is only one [path](#path) between every pair of nodes....","\u003cb\u003eGraph Glossary\u003c\u002fb\u003e\u003cbr\u003eA [triplet](#triplet) with 3 edges. It consists of _three_ closed triplets, each centered around each of the vertices....","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003e# Matrices associated to grap...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003eA graph $$G = (V, E)$$ s.t. $$V = \\{v_1, \\dots, v_n\\}$$ and $$E = \\{e_1, \\dots, e_m \\}$$ has several important associated matrices. For convenience, I often refer to vertex $$v_i$$ simply by its index...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003e```mermaid\n---\nconfig:\n  layout: elk\n  look: handDrawn\n---\ngraph LR\n    vertex_1((1))\n    vertex_2((2))\n    vertex_3((3))\n    vertex_4((4))...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003e    vertex_1 === vertex_2\n    vertex_1 === vertex_3\n    vertex_1 === vertex_4\n    vertex_2 === vertex_3\n```...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003e[Vertex degree]({% post_url 2025-01-23-graphs-basics %}#degree) is ised to define the **degree** matrix $$D$$ is a diagonal $$n \\times n$$ matrix such that $$D_{ii} = \\deg i$$, and 0 elsewhere. For in...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003e$$\n\\text{D}(G_1) = \\begin{bmatrix}\n3 & 0 & 0 & 0 \\\\\n0 & 2 & 0 & 0 \\\\\n0 & 0 & 2 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n\\end{bmatrix}\n$$...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003e[Incidence]({% post_url 2025-01-23-graphs-glossary %}#incidence) is used to define the **incidence** matrix $$Q$$, a $$n \\times m$$ matrix such that $$Q_{ij}$$ equals:...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003e- If $$G$$ is _directed_:\n  - $$0$$ if vertex $$i$$ and edge $$e_j$$ are not incident\n  - $$1$$ if edge $$e_j$$ originates at vertex $$i$$\n  - $$-1$$ if edge $$e_j$$ terminates at vertex $$i$$\n- If $$...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003e[Adjacency]({% post_url 2025-01-23-graphs-glossary %}#adjacency) is used to define the **adjacency** matrix $$A$$, a matrix $$n \\times n$$ such that the $$A_{ij}$$ equals:...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003e- $$0$$ if vertices $$i$$ and $$j$$ are not adjacent (note that in simple graphs vertices are not self-adjacent)\n- $$1$$ otherwise...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003e$$\nA = \\begin{bmatrix}\n0 & 1 & 1 & 1 \\\\\n1 & 0 & 1 & 0 \\\\\n1 & 1 & 0 & 0 \\\\\n1 & 0 & 0 & 0 \\\\\n\\end{bmatrix}\n$$...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003eThe adjacency matrix relates to the concept of [paths]({% post_url 2025-01-23-graphs-glossary %}#path) in an unweighted graph: $$(A^k)_{ij}$$ represents the number of paths of length $$k$$ from vertex...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003eThe **Laplacian** matrix $$L$$ is a $$n \\times n$$ matrix such that the $$L_{ij}$$ equals::...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003e- For $$i \\neq j$$:\n  - $$0$$ if vertices $$i$$ and $$j$$ are not adjacent\n  - $$-1$$ otherwise\n- For $$i = j$$, the degree of $$i$$....","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003eMore concisely, $$L = D - A$$. Or, given any oriented incidence matrix $$Q(G)$$, $$L = QQ^T$$....","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003e$$\nL = D - A = \\begin{bmatrix}\n3 & -1 & -1 & -1 \\\\\n-1 & 2 & -1 & 0 \\\\\n-1 & -1 & 2 & 0 \\\\\n-1 & 0 & 0 & 1 \\\\\n\\end{bmatrix}\n$$...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003eThe Laplacian relates to the connectedness of a graph, giving rise to [spectral graph theory](#spectral-graph-theory). It also is connected to [flows]({% post_url 2025-01-23-graphs-glossary %}#flow). ...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003eThe presence of [hubs]({% post_url 2025-01-23-graphs-glossary %}#hub) results in large diagonal entries in the Laplacian. There are normalized versions of the Laplacian that downweigh such vertices by...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003eThe **symmetrically** normalized Laplacian $$L_\\text{sym}$$ is a symmetric matrix derived as follows:...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003eThe **random walk** normalized Laplacian $$L_\\text{rw}$$ is a matrix closely related to [random walks]({% post_url 2025-01-27-graphs-random-walks %}) that is derived as follows:...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003e**Spectral graph theory** studies how the eigenvalues and eigenvectors of a graph's associated matrices relate to its properties. Looking more closely at two of the matrices described above, we can se...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003e- If $$G$$ is undirected, $$A$$ is both real and symmetric. Hence, it is diagonalizable and has only _real_ values.\n- Since for an undirected graph both $$D$$ and $$A$$ are symmetric, $$L$$ is also re...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003eSpectral graph theory often focuses on studying the eigenvalues of the Laplacian....","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003eThe eigenvectors of $$L$$ are closely related to the connectivity of its associated graph....","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003eA simple, but ultimately insightful property of $$L$$ is that, for an undirected graph, the sum over the rows or the columns equals 0...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003eIn other words, multiplying $$L$$ by an all-ones vector $$\\mathbf{1}$$ results in the zero vector...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003eThis tells us that $$L$$ has an eigenvalue of 0, corresponding to the eigenvector $$\\mathbf{1}$$...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003eSeparately, linear algebra tells us that since $$L$$ is real and symmetric, it has _real_ eigenvalues and _orthogonal_ eigenvectors...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003eAs we have just seen, the [first eigenvalue]({% post_url 2025-01-23-graphs-glossary %}#first-k-eigenvectors), $$\\lambda_1$$, of $$L$$ is 0, corresponding to the $$\\mathbf{1}$$ eigenvector...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003eIf a vector has multiple [components]({% post_url 2025-01-23-graphs-glossary %}#component), $$L$$ is block diagonal...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003eThis makes it easy to see that the indicator vectors, representing the membership of each vertex to one of the components, are eigenvectors with an eigenvalue of 0...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003eThis highlights another important property of the Laplacian: given an undirected graph, the multiplicity of the eigenvalue 0 of $$L$$ equals the number of [components]({% post_url 2025-01-23-graphs-gl...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003eConversely, for a [connected]({% post_url 2025-01-23-graphs-glossary %}#connected-graph) graph, $$\\lambda_2 \u003e 0$$...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003eMore generally, less _smooth_ eigenvectors (i.e., those in which consecutive elements change sharply) indicate a less connected. Equivalently, smaller eigenvalues correspond to smoother eigenvectors, ...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003eThe goal of **spectral clustering** is finding a partition of the graph into $$k$$ groups such that the are densely\u002fstrongly connected with each other, and sparsely\u002fweakly connected to the others. (If...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003e```pseudocode\n\\begin{algorithm}\n\\caption{Spectral Clustering}\n\\begin{algorithmic}[1]\n\\PROCEDURE{GraphSpectralClustering}{$$A, k$$}\n    \\STATE $$n \\gets \\text{number of nodes (rows in A)}$$...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003e    \\STATE Compute degree matrix $$D$$ where $$D[i,i] = \\sum_{j=1}^n A[i,j]$$\n    \\STATE $$D_{\\text{sqrt-inv}} \\gets \\text{diag}(1\u002f\\sqrt{D[i,i]})$$\n    \\STATE $$L_{\\text{sym}} \\gets I - D_{\\text{sqrt-...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003e    \\STATE Compute first $$k$$ eigenvectors $$u_1, \\ldots, u_k$$ of $$L_{\\text{sym}}$$\n    \\STATE Form matrix $$U \\in \\mathbb{R}^{n \\times k}$$ with columns $$u_1, \\ldots, u_k$$...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003e    \\FOR{$$i = 1$$ \\TO $$n$$}\n        \\STATE $$U[i] \\gets U[i] \u002f \\|U[i]\\|$$ \\COMMENT{Row normalization}\n    \\ENDFOR...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003e    \\STATE $$\\text{labels} \\gets \\text{KMeans}(U, k)$$ \\COMMENT{Cluster embedded nodes}\n    \\RETURN $$\\text{labels}$$\n\\ENDPROCEDURE\n\\end{algorithmic}\n\\end{algorithm}\n```...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003e\u003e **Note:** spectral clustering is often applied as a clustering technique on datasets. The aim is to divide the observation into $$k$$ groups based on their pairwise similarities. In that case, the f...","\u003cb\u003eGraphs and Linear Algebra\u003c\u002fb\u003e\u003cbr\u003e- [A Tutorial on Spectral Clustering](https:\u002f\u002fpeople.csail.mit.edu\u002fdsontag\u002fcourses\u002fml14\u002fnotes\u002fLuxburg07_tutorial_spectral_clustering.pdf)\n- [Four graph partitioning algorithms](https:\u002f\u002fmathweb.ucsd.ed...","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003eA **random walk (RW)** is a [stochastic](https:\u002f\u002fen.wikipedia.org\u002fwiki\u002fStochastic_process), discrete process. At every time step a walker, located in one of the graph's vertices, picks one of its neig...","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003eNote that $$P$$ corresponds to a [row stochastic matrix](#markov-chains). The outcome of a single random walk is a [walk]({% post_url 2025-01-23-graphs-glossary %}#walk) of length $$t$$, where $$t$$ i...","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003e- At step 0, $$\\mathbf{\\pi}_0 = (0, 0, \\cdots, 1, \\cdots, 0)$$. That is, $$\\pi_0$$ is an $$n$$-dimensional row vector that is $$0$$ almost everywhere, with a $$1$$ at component $$i$$.\n- At step 1, $$\\...","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003e$$\\pi_t$$ is an $$n$$-dimensional row vector $$\\mathbf{\\pi}_t$$ in which $$\\pi_{tj}$$ represents the probability of the walker starting at vertex $$i$$ and being on vertex $j$ at time $t$....","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003eWe might be interested in what happens if we let the random walk run indefinitely:...","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003eWhen taking powers of a matrix, it is useful to use its [eigendecomposition](https:\u002f\u002fen.wikipedia.org\u002fwiki\u002fEigendecomposition_of_a_matrix). After computing the eigenvectors ($$\\mathbf{u}_1, \\cdots, \\m...","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003e$$\\mathbf{\\pi}_0 = c_1 \\mathbf{u}_1 + c_2 \\mathbf{u}_2 + \\cdots + c_n \\mathbf{u}_n$$...","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003e$$\n\\begin{multline*}\n  \\mathbf{\\pi}_{t} = \\mathbf{\\pi}_0 P^t \\\\\n  = (c_1 \\mathbf{u}_1 + \\cdots + c_n \\mathbf{u}_n) P^t \\\\\n  = c_1 \\mathbf{u}_1 P^t + \\cdots + c_n \\mathbf{u}_n P^t \\\\\n  = c_1 \\lambda^t_...","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003eThe eigenvalues of a stochastic matrix are always less than or equal to 1 in absolute value. When the random walk is _ergodic_ (see below), $$P$$ has an eigenvalue of 1 with an eigenvector $$\\pi$$ suc...","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003eThe degree row-vector $$\\mathbf{d} = ({d_1}, \\cdots, d_n )$$ is a left eigenvector of $$P$$:...","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003ewhere $$\\mathbf{1}$$ represents the row vector of all ones. That is, $$\\mathbf{d}$$ is an eigenvector with eigenvalue 1 and non-negative entries. In order to transform it into a valid probability dist...","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003eThis is the stationary distribution of the random walk. It formalizes the intuitive result that high [degree]({% post_url 2025-02-09-graph-properties %}#degree) vertices are more likely to be visited....","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003e\u003e **Ergodicity and _lazy_ random walks:** A unique stationary distribution does not always exists...","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003eA random walk is _ergodic_ if a stationary distribution exists and is the same for any $$\\pi_0$$...","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003eFor the random walk to be ergodic, the graph needs to be connected and non [bipartite]({% post_url 2025-01-23-graphs-glossary %}#bipartite-graph)...","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003eIf the graph has multiple components, starting in different components will produce different stationary distributions...","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003eIf the graph is bipartite, at step $$t$$ the walker will be on one side or another, depending on the initial vertex and the parity of $$t$$...","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003eBipartite graphs have a ergodic [_lazy_ random walk](https:\u002f\u002fpeople.orie.cornell.edu\u002fdpw\u002forie6334\u002fFall2016\u002flecture11.pdf), in which the walker has a probability $$\\frac 1 2$$ of remaining at the curre...","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003eThe [Laplacian]({% post_url 2025-01-23-graphs-linear-algebra %}#normalized-laplacian-matrices) and the transition matrices are deeply related:...","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003eIn fact, their eigenvectors and eigenvalues are connected. If $$\\mathbf{u}$$ is an eigenvector of $$P$$, with eigenvalue $$\\lambda$$:...","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003e$$\n\\mathbf{u} L_{rw} = \\mathbf{u} (I - P) = \\mathbf{u} - \\mathbf{u} P = (1 - \\lambda) \\mathbf{u}\n$$...","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003eThat is, $$P$$ and $$L_{rw}$$ have the same eigenvectors, and the eigenvalues are related as $$\\lambda_i(L_{rw}) = 1 - \\lambda_i(P)$$. Since the [smallest eigenvalue of $$L_{rw}$$]({% post_url 2025-01...","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003e- This result holds regardless of what the starting vertex is. In fact, $$\\pi_0$$ could be a probability distribution over the vertices.\n- The _speed_ at which the distribution converges depends on th...","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003eIn the **random walk with restart (RWR)**, the walker can return to its root vertex with a restart probability $$r \\in [0, 1]$$:...","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003eIf $$r = 0$$, the walker will never be teleported back to the root, and a RW is equivalent to a RWR...","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003eIf $$r = 1$$, the walker will not be allowed to move out of the root, and $$\\mathbf{\\pi}_t = \\mathbf{\\pi}_0$$...","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003eHowever, for certain values of $r$, the walker is allowed to explore the root's neighborhood before teleporting back...","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003eIf the root is part of a [module]({% post_url 2025-01-23-graphs-glossary %}#module), the walk will mostly happen within that module...","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003eA **Markov chain** is a sequence of events in which the probability of each event only depends on the state attained in the previous event. A random walk is a Markov chain: the probability of visiting...","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003e- _Time reversibility_\n- _Symmetry_: a Markov chain is symmetric when the underlying graph is [regular]({% post_url 2025-01-23-graphs-glossary %}#regular)....","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003eIn the context of Markov chains, the transition matrix $$P$$ is known as the **right stochastic matrix**....","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003e- **_Row\u002fright_ stochastic matrix**: square matrix with non-negative entries where each row sums to $$1$$.\n- **_Column\u002fleft_ stochastic matrix**: square matrix with non-negative entries where each col...","\u003cb\u003eRandom walks and Markov chains\u003c\u002fb\u003e\u003cbr\u003e- [Full title: The Unreasonable Effectiveness of Spectral Graph Theory: A Confluence of Algorithms, Geometry, and Physics](https:\u002f\u002fwww.youtube.com\u002fwatch?v=8XJes6XFjxM)...","\u003cb\u003eProperties of Graphs\u003c\u002fb\u003e\u003cbr\u003eFor instance, how information flows through a social network is partly a function who posts the message and how they are connected to the rest of the network, with their immediate connections being li...","\u003cb\u003eProperties of Graphs\u003c\u002fb\u003e\u003cbr\u003eIn this section, I review three levels at which networks operate: [local](#local-properties), [mesoscale](#mesoscale-properties) and [global](#global-properties)...","\u003cb\u003eProperties of Graphs\u003c\u002fb\u003e\u003cbr\u003eThey refer, respectively, to properties of the nodes, properties of parts of the network and properties of the whole network....","\u003cb\u003eProperties of Graphs\u003c\u002fb\u003e\u003cbr\u003eIn an undirected network, the **degree** of a vertex $$u$$ ($$\\deg u$$) refers to the number of edges that are incident on $$u$$. In a directed network, this concept is split between _indegree_ ([$$\\d...","\u003cb\u003eProperties of Graphs\u003c\u002fb\u003e\u003cbr\u003eThe **(local) clustering coefficient** _of a vertex_ measures the probability that its [neighbors]({% post_url 2025-01-23-graphs-glossary %}#neighborhood) are connected. It is computed as the ratio be...","\u003cb\u003eProperties of Graphs\u003c\u002fb\u003e\u003cbr\u003e[Often](https:\u002f\u002fr.igraph.org\u002freference\u002ftransitivity.html), the clustering coefficient of a directed graph is computed without considering the direction of the edges....","\u003cb\u003eProperties of Graphs\u003c\u002fb\u003e\u003cbr\u003eThe **modularity** measures how well a graph can be divided into [modules]({% post_url 2025-01-23-graphs-glossary %}#modules). Given a partition of a graph into $$k$$ modules, the modularity $$Q$$ is ...","\u003cb\u003eProperties of Graphs\u003c\u002fb\u003e\u003cbr\u003ewhere $$e_{ii} = \\frac {\\| \\{\\{u, v\\} \\mid u \\in V_i, v \\in V_i, \\{u, v\\} \\in E \\} \\|} {\\|E\\|} $$,$$a*i = \\frac {\\| \\{\\{u, v\\} \\mid u \\in V_i, \\{u, v\\} \\in E \\} \\|} {\\|E\\|} $$ and $$V_i$$ is the set o...","\u003cb\u003eProperties of Graphs\u003c\u002fb\u003e\u003cbr\u003eThe **within-module degree** of a vertex is the module version of the [degree](#degree). It is often normalized as a z-score; the z-score for node $$i$$, mapped to module $$k$$:...","\u003cb\u003eProperties of Graphs\u003c\u002fb\u003e\u003cbr\u003ewhere $$\\kappa_i$$ is within-module degree (the number of edges between $$i$$ and other vertices in module $$k$$); $$\\bar \\kappa_k$$ is the average within-module degree; and $$\\sigma_{\\kappa_k}$$ is t...","\u003cb\u003eProperties of Graphs\u003c\u002fb\u003e\u003cbr\u003eThe **participation coefficient** of a vertex... TODO\nIt is a mesoscale measure of [centrality](#centrality).\n{% endcomment %}...","\u003cb\u003eProperties of Graphs\u003c\u002fb\u003e\u003cbr\u003eThe radius and the diameter measure how easy it is to traverse a graph. They both are quantities based on the maximum [distance]({% post_url 2025-01-23-graphs-glossary %}#distance) between any two ver...","\u003cb\u003eProperties of Graphs\u003c\u002fb\u003e\u003cbr\u003eThe **global clustering coefficient** _of a graph_ is the ratio between closed and open [triplets]({% post_url 2025-01-23-graphs-glossary %}#triplet) in that graph. Or, equivalently:...","\u003cb\u003eProperties of Graphs\u003c\u002fb\u003e\u003cbr\u003e[Often](https:\u002f\u002fr.igraph.org\u002freference\u002ftransitivity.html), the clustering coefficient of a directed graph is computed without considering the direction of the edges....","\u003cb\u003eProperties of Graphs\u003c\u002fb\u003e\u003cbr\u003e**Centrality** assigns a score or a ranking to every vertex in the graph, which represents its importance in the network according to some metric. [Degree](#degree) and [participation](#participation)...","\u003cb\u003eProperties of Graphs\u003c\u002fb\u003e\u003cbr\u003eWe can clasify graphs into different types by using the [global properties](#global-properties) of their nodes....","\u003cb\u003eProperties of Graphs\u003c\u002fb\u003e\u003cbr\u003e**Regular** graphs are those in which every node has the same degree. They have a high average [clustering coefficient](#local-clustering-coefficient) and a large [diameter](#radius-and-diameter)....","\u003cb\u003eProperties of Graphs\u003c\u002fb\u003e\u003cbr\u003eSmall world graphs have a high average [clustering coefficient](#local-clustering-coefficient) and a small [diameter](#radius-and-diameter). They are called small world because, despite their size, th...","\u003cb\u003eProperties of Graphs\u003c\u002fb\u003e\u003cbr\u003e[Milgram](https:\u002f\u002fen.wikipedia.org\u002fwiki\u002fStanley_Milgram) _et al._ conducted experiments that were key to understand the topology of social graphs....","\u003cb\u003eProperties of Graphs\u003c\u002fb\u003e\u003cbr\u003eThey gave letters to randomly chosen people from Nebraska and Kansas, each of which was to reach a target random person from Massachusetts. To that end, they could only give it to their friend or rela...","\u003cb\u003eProperties of Graphs\u003c\u002fb\u003e\u003cbr\u003eThis ultimately lead to the postulation of the six degrees of separation, and the realization that social graphs are small world graphs....","\u003cb\u003eKnockoffs\u003c\u002fb\u003e\u003cbr\u003eWhen dealing with statistical tests, Benjamin...","\u003cb\u003eKnockoffs\u003c\u002fb\u003e\u003cbr\u003eWhen dealing with statistical tests, Benjamini\u2013Hochberg and Benjamini\u2013Yekutieli are common procedures to keep the FDR below a level $$\\alpha$$...","\u003cb\u003eKnockoffs\u003c\u002fb\u003e\u003cbr\u003eHowever, such strategies rely on certain assumptions; for instance, that P-values are well-calibrated or that tests have certain correlation structures...","\u003cb\u003eKnockoffs\u003c\u002fb\u003e\u003cbr\u003eIf these are not met, the statistical guarantees on FDR control are also out the window...","\u003cb\u003eKnockoffs\u003c\u002fb\u003e\u003cbr\u003eFurthermore, they require having P-values to work with; in many cases we just want to control the FDR of selected features, but do not have a well-characterized null hypothesis...","\u003cb\u003eKnockoffs\u003c\u002fb\u003e\u003cbr\u003eFor instance, given a set of active features in Lasso, how can we make sure the fraction of non-explanatory features is controlled...","\u003cb\u003eKnockoffs\u003c\u002fb\u003e\u003cbr\u003eThe **knockoff** filter is a procedure to perform feature selection while keeping the FDR controlled. Given an outcome $$\\mathbf{Y}$$ and a feature matrix $$X$$, the goal is to select a subset of feat...","\u003cb\u003eKnockoffs\u003c\u002fb\u003e\u003cbr\u003eThe procedure computes and leverages a new matrix $$\\tilde{X}$$, with the same dimensions as $$X$$, containing \"knockoff\" copies of the original features...","\u003cb\u003eKnockoffs\u003c\u002fb\u003e\u003cbr\u003eEach original variable $$\\mathbf{X_i}$$ has its own knockoff $$\\mathbf{\\tilde{X}_i}$$...","\u003cb\u003eKnockoffs\u003c\u002fb\u003e\u003cbr\u003eThese knockoffs are engineered to mimic the correlation structure of the original features: for any $$i \\neq j$$, $$\\rho(\\mathbf{X_i}, \\mathbf{X_j}) = \\rho(\\mathbf{X_i}, \\mathbf{\\tilde{X}_j}) = \\rho(\\...","\u003cb\u003eKnockoffs\u003c\u002fb\u003e\u003cbr\u003eThe **Model-X** paradigm assumes that the explanatory variables are random variables with a known joint distribution. Although theoretically appealing, this assumption can be impractical for real-worl...","\u003cb\u003eKnockoffs\u003c\u002fb\u003e\u003cbr\u003eThe **Fixed-X** paradigm makes no assumptions on the distribution of the explanatory variables. Instead, they can be treated as fixed quantities. This makes it more applicable in practice. However, it...","\u003cb\u003eKnockoffs\u003c\u002fb\u003e\u003cbr\u003e- $$F_{Y \\mid X}$$ must be linear and homoscedastic\n- The problem must be low dimensional (number of samples $$\u003e$$ number of features)\n- The statistics $$D(X_i, Y)$$ and $$D(\\tilde{X}_i, Y)$$ must sat...","\u003cb\u003eKnockoffs\u003c\u002fb\u003e\u003cbr\u003eIntuitively, by comparing the association measure computed for each original feature against its knockoff, one can determine which features provide true signals. Specifically, the knockoff-based featu...","\u003cb\u003eKnockoffs\u003c\u002fb\u003e\u003cbr\u003eCreate synthetic copies of the features that retain the original correlation structure without any outcome information. An obvious question is how to synthesize such knockoff copies....","\u003cb\u003eKnockoffs\u003c\u002fb\u003e\u003cbr\u003eFor each feature, calculate the association measure $$D(\\mathbf{Y}, \\mathbf{X_k})$$ and its counterpart $$D(\\mathbf{Y}, \\tilde{\\mathbf{X}}_k)$$ on the knockoff....","\u003cb\u003eKnockoffs\u003c\u002fb\u003e\u003cbr\u003eKernel-based measures are powerful tools for detecting complex, non-linear dependencies:...","\u003cb\u003eKnockoffs\u003c\u002fb\u003e\u003cbr\u003e- **HSIC (Hilbert-Schmidt Independence Criterion):** Computes the covariance between kernel-transformed versions of the feature and the outcome, capturing a broad range of dependency structures.\n- **C...","\u003cb\u003eKnockoffs\u003c\u002fb\u003e\u003cbr\u003eThese measures satisfy the sure independence screening property under bounded kernel conditions\u2014meaning that, with high probability, the truly active features are recovered when a proper threshold is ...","\u003cb\u003eKnockoffs\u003c\u002fb\u003e\u003cbr\u003e\u003e A potential limitation of kernel knockoffs is its sometimes overly conservative nature. To keep the FDR low, the procedure may end up selecting very few\u2014or even no\u2014features. This suggests that the c...","\u003cb\u003eKnockoffs\u003c\u002fb\u003e\u003cbr\u003eDefine the statistic as $$w_k = D(Y, X_k) - D(Y, \\tilde{X}_k)$$. A larger $$w_k$$ indicates stronger evidence that the original feature is associated with the outcome....","\u003cb\u003eKnockoffs\u003c\u002fb\u003e\u003cbr\u003eIdentify the smallest threshold $$t$$ such that $$\\frac{\\#\\{w_k \\le -t\\}}{\\#\\{w_k \\ge t\\}} \\le \\alpha$$ where $$\\alpha$$ is the desired FDR level. Retain all features with $$w_k \\ge t$$....","\u003cb\u003eKnockoffs\u003c\u002fb\u003e\u003cbr\u003eA notable challenge arises when the number of features $$p$$ is large compared to the sample size $$n$$ (i.e., $$2p\u003en$$). In such high-dimensional settings, constructing knockoffs directly is infeasib...","\u003cb\u003eKnockoffs\u003c\u002fb\u003e\u003cbr\u003e- Pre-screen Features: Use a subset of the data to rank and reduce the feature set.\n- Apply the knockoff filter: With the reduced set of features, generate knockoffs using the remaining samples (ensur...","\u003cb\u003eKnockoffs\u003c\u002fb\u003e\u003cbr\u003eThis two-step approach helps maintain statistical power while ensuring robust FDR control....","\u003cb\u003eKnockoffs\u003c\u002fb\u003e\u003cbr\u003e- [Variable Selection with Knockoffs](https:\u002f\u002fweb.stanford.edu\u002fgroup\u002fcandes\u002fknockoffs\u002f)\n- [B. Poignard, P. J. Naylor, H. Climente-Gonz\u00e1lez, M. Yamada, in International Conference on Artificial Intelli...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eWe will use `uv` for a p...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eWe will use `uv` for a prototypical machine learning project: train a neural network to classify images of handwritten digits from the [MNIST dataset](https:\u002f\u002fen.wikipedia.org\u002fwiki\u002fMNIST_database) usi...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eThe input of the model will be a 28x28 pixel image, and the output will be a vector of 10 probabilities, one for each digit...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eIf you are interested in the details of the model, you can check out the [code](https:\u002f\u002fgithub.com\u002fhclimente\u002fhclimente.github.io\u002ftree\u002fmain\u002fassets\u002fpython\u002f2025-03-15-python-uv\u002fmnist-classifier)....","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003e\u003cdiv style=\"width:50%; margin:0 auto;\"\u003e\n    {% include figure.liquid\n        loading=\"eager\"\n        path=\"assets\u002fpython\u002f2025-03-15-python-uv\u002fmnist-classifier\u002fimg\u002fmnist_examples.webp\"\n        class=\"i...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eI'd start by creating a new conda environment and installing several packages via slow `conda install` commands before getting to work...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eIf during model development I needed additional packages, I'd run another `conda install` hoping to avoid the dreaded `Solving environment: failed with initial frozen solve...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eOnce I'd finish, I'd dump my environment into an `environment.yaml`, strip-out dev-only dependencies, and hope that the final environment sufficiently resembles the one I worked on...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eFinally, I'd package the model into a Docker image to get it ready for production....","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eClearly, I wasn\u2019t thrilled with my old workflow. Let's see how `uv` made it a more pleasant experience....","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eBefore diving into the details, it's worth justifying why we need _yet another tool_ for managing Python-centric data science projects....","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eFirst, `uv` is **fast**. As is common in new high-performance tools, it's written in Rust, a compiled language known for its performance. It also uses different strategies to speed up package installa...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eSecond, `uv` boosts **reproducibility**. As we will see below, it makes it easy to create and share virtual environments. This is key to ensure that multiple developers can work on a consistent enviro...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eThird, `uv` leverages **common standards** within the Python ecosystem. This reduces the risk of being locked into its ecosystem, and makes it easy to collaborate with other developers that use differ...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eLast, `uv` is **one** tool, which means that I don't need to remember the syntax of multiple tools, or how to use them together....","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eI'm always quite enthusiastic about the new, shinier tool. But before jumping straight into `uv`, it's worth considering the downsides of adopting it....","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eFirst, `uv` is **young**. In contrast, tools like `pip`, `conda` or `venv` have been around for more than a decade. I have no doubt they will be around for at least another decade and are unlikely to ...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eSecond, and on a related note, `uv` is **not widely adopted**. This means that I have had a hard time troubleshooting some errors. It has also meant that it's not a standard, and you might need to be ...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eLast, `uv` is mainly developed by [Astral](https:\u002f\u002fastral.sh), a VC-backed startup that hasn't started monetizing their products yet. It remains to be seen how their future business model will impact ...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eAfter [installing `uv`](https:\u002f\u002fdocs.astral.sh\u002fuv\u002fgetting-started\u002finstallation\u002f), we simply need to run `uv init` to start a new project:...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eThis creates a directory `mnist-classifier` in the current directory containing a few files we'll soon dig into...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eOne of them is a [`pyproject.toml`](https:\u002f\u002fpackaging.python.org\u002fen\u002flatest\u002fguides\u002fwriting-pyproject-toml\u002f) file that stores the project's metadata and configuration...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eThis is a standard file used by [many](https:\u002f\u002fpython-poetry.org\u002fdocs\u002fpyproject\u002f) [tools](https:\u002f\u002fpip.pypa.io\u002fen\u002fstable\u002freference\u002fbuild-system\u002fpyproject-toml\u002f) [in](https:\u002f\u002fdocs.astral.sh\u002fruff\u002fconfigu...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eFor instance, `pip install .` would install all the packages listed under the `dependencies` field...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003e```toml\n[project]\nname = \"mnist-classifier\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nreadme = \"README.md\"\nrequires-python = \"\u003e=3.10\"\ndependencies = []\n```...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eBy default, `uv init` creates an _application_ project. This is appropriate for scripts, like simple tools. This is why the command above created a `main.py` file, meant to be the entry point of our a...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eMultiple Python projects can co-exist on the same machine, each requiring different packages and versions of the same packages. This is facilitated by _virtual environments_, self-contained directorie...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003e`uv` leverages Python's built-in package to handle virtual environments: `venv`. The virtual environment contains its own installation of Python, whose version is specified in `.python-version`. `uv i...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eWhen Python runs from within an environment, it uses the packages installed in that environment, and only those packages...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eTypically we would activate this virtual environment from the terminal with `source .venv\u002fbin\u002factivate`...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eThis will append `.venv\u002fbin\u002f` to our `PATH`, loading the `python` located there into our environment...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eHowever, this comes with an overhead: we need to remember to activate the environment before running any Python script, and we need to deactivate (`deactivate`) it when we are done...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eThis is a source of errors, as we may forget to activate the environment or, worse, forget to deactivate it....","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eThat's why `uv` does not require explicitly activating the environment. Instead, we can use `uv run \u003cscript\u003e.py` to run any Python script or command using the environment's Python. For instance, `uv i...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eAs stated, we can run it using our default Python, as we are used to (`python main.py`), maybe after `source .venv\u002fbin\u002factivate`. But we can also run it using `uv run main.py`, which will run the scri...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eUpon its first invocation, `uv run main.py` creates a virtual environment. To do this, it examines the (empty) `dependencies` list in `pyproject.toml` and resolves an (empty) set of packages....","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eTo start our little data science project, we'll need to install the [PyTorch](https:\u002f\u002fpytorch.org\u002f) library. Typically I would have run `conda install conda-forge::pytorch`; in `uv` we use `uv add tor...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eThis is great, as it allows us to keep track of the packages that we needed for our project, reducing our overhead down the road as the project matures....","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eNote that this is not reflected in `pyproject.toml`, which lists only our direct dependencies, not every transitive package...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eFurthermore, as we install additional packages like `torchvision` or `matplotlib`, `uv` will need to resolve all the dependencies and potential conflicts between the packages...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003e`uv` keeps an additional file, the lockfile (`uv.lock`) that records the exact state of the environment with all the specific package resolutions...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eFor instance, after `uv add torch` it expanded to 353 lines describing all the specific packages, their versions and the metadata that were installed in the environment...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003esdist = { url = \"https:\u002f\u002ffiles.pythonhosted.org\u002fpackages\u002f0a\u002f10\u002fc23352565a6544bdc5353e0b15fc1c563352101f30e24bf500207a54df9a\u002ffilelock-3.18.0.tar.gz\", hash = \"sha256:adbc88eabb99d2fec8c9c1b229b171f18afa...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003e    { url = \"https:\u002f\u002ffiles.pythonhosted.org\u002fpackages\u002f4d\u002f36\u002f2a115987e2d8c300a974597416d9de88f2444426de9571f4b59b2cca3acc\u002ffilelock-3.18.0-py3-none-any.whl\", hash = \"sha256:c401f4f8377c4464e6db25fff06205...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003e]...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003e`uv.lock` should be under git control, providing the exact recipe to replicate an environment. This is key, for instance, to ensure that all developers work on a consistent environment. It can also fa...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003e\u003e If needed, `uv.lock` can be exported into a [`requirements.txt`](https:\u002f\u002fpip.pypa.io\u002fen\u002fstable\u002freference\u002frequirements-file-format\u002f) file for legacy tools, via `uv export --format=requirements-txt \u003er...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eBesides `uv add`, there are other commands that can be used to manage packages. For starters, its counterpart `uv remove \u003cpackage_name\u003e` will uninstall `\u003cpackage_name\u003e`. Another command that can trigg...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eSyncing refers to (un)installing packages in the project environment to match the lockfile. `uv run` will do this automatically, as we just saw. But it can also be forced manually with `uv sync`....","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eLast, when adding new packages, `uv` will tend to be conservative. It will install the most recent version of the package that is compatible with the current environment. To force a specific version, ...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003e\u003e To keep compatibility with `pip` workflows, `uv` also supports `uv pip install \u003cpackage_name\u003e` and `uv pip uninstall \u003cpackage_name\u003e`. These will (un)install the package in the current environment, b...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eIn order to run Jupyter notebooks on our `uv` environment, we need to install the [IPython kernel `ipykernel`](https:\u002f\u002fpypi.org\u002fproject\u002fipykernel\u002f)...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eHowever, `ipykernel`'s role is different from other packages: it is not a dependency of our code, but a tool needed for development...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eOnce my code is ready, I will distribute it as a standalone Python script that has no dependencies on `ipykernel`...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eThe same principle applies to tools like `pytest`, used to test your code, but which the end-user shouldn't require unless they intend to contribute to the project....","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003e`uv` allows you to add development dependencies with `uv add --dev ipykernel`, which will add the following to `pyproject.toml`:...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eThis should allow my tool of choice, Visual Studio Code, to find this virtual environment and run Jupyter notebooks on it. However, in my experience, it has been somewhat unreliable: Visual Studio Cod...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eHere comes the actual data science, which I will just skim over. I wrote a simple script to train a convolutional neural network on the MNIST dataset. The script is located in [`train.py`](https:\u002f\u002fgit...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eNow that we have a working model, let's see how `uv` helps us package the model into a Docker image....","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eFirst, we need to pick our base image. Astral provides [multiple pre-built images](https:\u002f\u002fdocs.astral.sh\u002fuv\u002fguides\u002fintegration\u002fdocker\u002f#available-images) that include `uv` and different versions of Py...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003e# copy the minimum required files:\n## the uv files needed to recreate the environment\nCOPY pyproject.toml uv.lock .\u002f\n## the prediction script\nCOPY predict.py .\n## the model weights\nCOPY data\u002fmnist_cnn...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eThe key command here was `uv sync`, which will recreate the environment using the exact versions of the packages specified in `uv.lock`. This ensures that the environment used to train the model is id...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003e\u003e Note: If you use GPU-specific packages, wheels may differ. See [Astral\u2019s docs](https:\u002f\u002fdocs.astral.sh\u002fuv\u002fguides\u002fintegration\u002fpytorch\u002f#installing-pytorch)....","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003e```\nSimpleCNN(\n  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n  (fc1): Linear(in_features=1600, out_features=128, bias=True)\n ...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eNice!...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003eWe have seen how `uv` can be used to manage Python projects, packages and environments. It satisfies my craving for reproducibility, is snappy and has simplified repetitive workflows. I look forward t...","\u003cb\u003eAn intro to uv\u003c\u002fb\u003e\u003cbr\u003e- [A year of uv: pros, cons, and should you migrate](https:\u002f\u002fwww.bitecode.dev\u002fp\u002fa-year-of-uv-pros-cons-and-should)...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003e# Shapley values...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eSHAP values have their roots in game theory, specifically in **Shapley** values. Imagine a group of players collaborating to achieve a payout. The Shapley value is a method to find out how to fairly d...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eA core concept of Shapley values is **coalitions**: given $$n$$ players, a coalition is a subset of the players. Another concept is the **characteristic function**, $$v: 2^N \\rightarrow \\mathbb{R} $$,...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003e$$\n\\phi_i(v) = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!\\; (n-|S|-1)!}{n!} (v(S\\cup\\{i\\})-v(S))\n$$...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eThese values satisfy four key properties, which collectively ensure fair attribution: efficiency, symmetry, dummy player, and additivity....","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eThe grand coalition is the coalition of all players, $$N$$. Efficiency means that the sum of all Shapley values equals the value of the grand coalition, i.e., the entire payout is distributed among th...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eTwo players $$i$$ and $$j$$ are symmetric if their marginal contribution to any coalition not containing either player is the same. That is, if $$v(S \\cup \\{i\\}) = v(S \\cup \\{j\\})$$ for any coalition ...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eIf a player $$i$$ does not change the value of any coalition they join (i.e., $$v(S \\cup \\{i\\}) = v(S)$$ for all $$S \\subseteq N \\setminus \\{i\\}$$), they are a dummy player. The dummy player property ...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eIf two games with characteristic functions $$v_1$$ and $$v_2$$ are combined into a new game $$v_1 + v_2$$ (where $$(v_1+v_2)(S) = v_1(S) + v_2(S)$$ for any coalition $$S$$), the Shapley values are add...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eMachine learning models like linear regression are _interpretable_, as the model parameters indicate how each input feature contributes to the prediction...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eHowever, many complex models like neural networks or random forests are less directly interpretable: their output is a complex, non-linear combination of the input features...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eSHAP values ([Lundberg and Lee, 2017](https:\u002f\u002farxiv.org\u002fabs\u002f1705.07874)) provide a framework to quantify the contribution of each feature to a specific prediction for _any_ model...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eSHAP stands for SHapley Additive exPlanations, highlighting their connection to _Shapley_ values....","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eIntuitively, SHAP values quantify how much each feature's presence changes the prediction. Some features will have a negative contribution (pushing the prediction lower) and others a positive contribu...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eTo establish the connection to Shapley values, we map the game theory concepts to the machine learning context:...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003e- The $$n$$ players become $$n$$ _predictive features_.\n- The game is the _trained model_.\n- The payout for a coalition of features is the _model's prediction_ when only those features are known....","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eThe Shapley value $$\\phi_i$$ for feature $$i$$ in this context is then calculated as:...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003e$$\n\\phi_i = \\sum_{S \\subseteq F \\setminus \\{i\\}} \\frac{|S|!\\; (n-|S|-1)!}{n!} (f(\\mathbf{x}_{S\\cup\\{i\\}})-f(\\mathbf{x}_S))\n$$...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003ewhere $$\\mathbf{x}_S$$ is the input datapoint including only the features in $$S$$; $$F$$ is the set of all features; $$n$$ is the total number of features; and $$f_S$$ is the model trained only on th...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eHowever, this na\u00efve approach is very computationally intensive, since it'd require retraining $$2^n$$ models, one per possible coalition...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003e(See a worked out example [here](https:\u002f\u002fwww.aidancooper.co.uk\u002fhow-shapley-values-work\u002f).) SHAP values get around re-training models by approximating the effect of feature subsets using conditional ex...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eIn other words, we fix the features that are in $$S$$ to the sample values, and average over the predictions when sampling the remaining features from the dataset....","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003e\u003e **_Simplified_ features:** We use $$\\mathbf{x}$$ for the features in the original space $$\\chi$$, a vector of length $$n$$...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eThe SHAP theoretical framework uses a simplified feature vector $$\\mathbf{x}' \\in \\{0,1\\}^m$$, where $$m$$ is the number of simplified features (which can be different from $$n$$)...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003e$$x'_j = 1$$ indicates that simplified feature $$j$$ is \"present\" in a coalition, and $$x'_j = 0$$ indicates it is \"absent\"...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eFor instance, if $$\\mathbf{x}$$ represented the individual pixels of an image, $$\\mathbf{x}'$$ could represent the presence of the \"super pixels\" that form a cat, grass or the sky...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eA mapping function $$h_\\mathbf{x}: \\{0,1\\}^m \\rightarrow \\chi$$ links the simplified representation back to the original feature space...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eFor $$\\mathbf{x}' = \\mathbf{1}$$ (all simplified features present), $$h_\\mathbf{x}(\\mathbf{1}) = \\mathbf{x}$$...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eFor other $$\\mathbf{x}'$$, $$h_\\mathbf{x}(\\mathbf{x}')$$ represents the original instance with features corresponding to $$x'_j=0$$ appropriately handled (e.g., replaced by baseline values)...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eNote that $$h_\\mathbf{x}$$ is specific to the instance $$\\mathbf{x}$$ being explained....","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eThat covers the _Shapley_ part of SHAP; let's now focus on the _Additive exPlanation_ bit. The goal of SHAP is to obtain a local, additive explanation model $$g$$ for each prediction $$f(\\mathbf{x})$$...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003ewhere $$\\phi_0$$ is the expectation over all training examples $$\\mathbb{E}[f(X)]$$. $$g(\\mathbf{x}')$$ is a very easy to interpret function that we'll use to explain $$f(\\mathbf{x})$$....","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eSince SHAP values are Shapley values, they meet all the properties specified above. But they also satisfy three additional properties that are desirable for model explainers....","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eWhen all simplified features are present ($$\\mathbf{x}' = \\mathbf{1}$$), the explanation model $$g$$ must equal the prediction $$f(\\mathbf{x})$$:...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eThe consistency ensures that if a model $$f$$ changes into another model $$f'$$, such that a feature's contribution doesn't decrease, the SHAP values do not decrease either. Formally, if...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eLet's understand SHAP values better by looking at an example. I trained a model that uses 10 clinical features (body mass index, cholesterol, age, and a few others) to predict a continuous measure of ...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003e| Age         | Sex        | BMI        | Blood pressure | \u2026   | Target |\n| ----------- | ---------- | ---------- | -------------- | --- | ------ |\n| 0.0380759   | 0.0506801  | 0.0616962  | 0.0218724 ...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003e\u003e SHAP values can be computed on the dataset used to train the model (train set) or on a holdout set. Using a larger dataset like the train set might provide a more stable picture of overall feature c...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eSHAP values are implemented in Python via the [`shap`](https:\u002f\u002fshap.readthedocs.io\u002fen\u002flatest\u002findex.html) package. While I won't be showing any code here, you can see the code that generated the figure...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eSHAP values provide **local** explanations, showing the contribution of each feature to a particular prediction. I computed the SHAP values describing the importance of each of the 10 variables for ea...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003e{% include figure.liquid loading=\"eager\" path=\"assets\u002fpython\u002f2025-04-01-shapley\u002fimg\u002fwaterfall_diabetes.webp\" class=\"img-fluid rounded z-depth-1\" %}...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eThe waterfall plot shows how the prediction for this patient (186.53) departs from the average prediction over the training set (152.132)...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eAs per the local accuracy property, the SHAP values for this instance sum up to this difference...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eFeatures colored in pink push the prediction toward higher values, and features in blue toward lower values...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eWe can see that, for this patient, the body mass index was the most important feature, contributing positively by 22.25....","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003e{% include figure.liquid loading=\"eager\" path=\"assets\u002fpython\u002f2025-04-01-shapley\u002fimg\u002fbeeswarm_diabetes.webp\" class=\"img-fluid rounded z-depth-1\" %}...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eIn the swarmplot, each point represents the SHAP value for a patient for a specific feature. Features are shown on the y-axis, and their corresponding SHAP values on the x-axis. As in the waterfall pl...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eGlobal explanations can be derived by aggregating the local SHAP values over a dataset. A common global measure is the average absolute SHAP value for each feature:...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003e{% include figure.liquid loading=\"eager\" path=\"assets\u002fpython\u002f2025-04-01-shapley\u002fimg\u002fglobal_diabetes.webp\" class=\"img-fluid rounded z-depth-1\" %}...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003ePlotting these averages shows which features have the largest impact on the model's predictions _on average_ across the dataset, providing a global measure of feature importance....","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eLastly, SHAP values can be used for clustering. While traditional clustering groups data points based on their original feature values, clustering in SHAP space groups points based on how features _co...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003e\u003cstyle\u003e\n    .colored-slider {\n        --divider-color: rgba(0, 0, 0, 0.5);\n        --default-handle-color: rgba(0, 0, 0, 0.5);\n        --default-handle-width: clamp(40px, 10vw, 200px);\n  }\n\u003c\u002fstyle\u003e\n\u003ci...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eApplying PCA to the SHAP values (\"supervised PCA\") and the original features (\"unsupervised PCA\") for this dataset, we can visualize how instances are grouped....","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eOne key limitation of interpreting SHAP values is their behavior with **highly correlated features**. When features are strongly correlated, the model might arbitrarily use one over the others, or dis...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eWhile the fundamental Shapley value calculation inherently accounts for interactions (by averaging marginal contributions over different coalitions), the basic additive SHAP explanation model $$g(\\mat...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eThe $$\\phi_j$$ values represent the _average_ contribution of feature $$j$$, including its interactive effects, making their interpretation as pure \"main effects\" challenging when interactions are sig...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eHowever, SHAP [_can_ be extended](https:\u002f\u002fshap.readthedocs.io\u002fen\u002flatest\u002fexample_notebooks\u002ftabular_examples\u002ftree_based_models\u002fBasic%20SHAP%20Interaction%20Value%20Example%20in%20XGBoost.html) to comput...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eFinally, it's important to remember that SHAP values explain _how the model makes a prediction_, not whether the prediction itself is correct. If the model is biased, overfit, or simply wrong for a gi...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eUnfortunately, it is very computationally intensive: exploring all possible coalitions is equivalent to exploring all $$2^m$$ subsets of features...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eFor that reason, different flavors of SHAP values have been proposed to make computations more efficient...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eI describe below the **permutation approximation**, a model-agnostic method to compute SHAP values $$\\phi_i$$...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eHowever, there are others specialized in specific model types, like [Tree SHAP](https:\u002f\u002fshap.readthedocs.io\u002fen\u002flatest\u002fgenerated\u002fshap.TreeExplainer.html) for tree-based models (e.g., random forests, gr...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003eThe permutation approximation approximates SHAP values by estimating the expected marginal contribution of each feature over many random permutations of the features. Let's study the permutation appro...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003e1. Initializing a list to store the marginal contribution of each feature. In this example, I will focus on the contribution of the first feature, $$x_\\text{age}$$, so I will call this list just $$\\te...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003e   1. A random ordering of the features is produced, e.g., $$(\\text{BP}, \\text{age}, \\text{BMI}, \\text{sex})$$, and a random sample $$\\mathbf{z}$$ is sampled from the background dataset $$X_\\text{bg}$...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003e   1. Create two synthetic examples: - $$\\mathbf{x}_1 = (x_\\text{BP}, x_\\text{age}, z_\\text{BMI}, z_\\text{sex})$$ - $$\\mathbf{x}_2 = (x_\\text{BP}, z_\\text{age}, z_\\text{BMI}, z_\\text{sex})$$...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003e      Note that the only difference between the two examples is the value of the age feature....","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003e   1. Compute the marginal contribution of the age feature as $$\\delta = f(\\mathbf{x}_1) - f(\\mathbf{x}_2)$$....","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003e1. Approximate the SHAP value as the average marginal contribution: $$\\phi_\\text{age} \\cong \\frac{1}{K} \\sum_i \\delta_{i}.$$...","\u003cb\u003eSHAP values\u003c\u002fb\u003e\u003cbr\u003e- [Interpretable Machine Learning: Shapley values](https:\u002f\u002fchristophm.github.io\u002finterpretable-ml-book\u002fshapley.html)\n- [Interpretable Machine Learning: SHAP](https:\u002f\u002fchristophm.github.io\u002finterpretable-...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eThis is the workflow that Hugging Face \ud83e\udd17 and its [`tr...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eThis is the workflow that Hugging Face \ud83e\udd17 and its [`transformers`](https:\u002f\u002fhuggingface.co\u002fdocs\u002ftransformers\u002findex) Python library aim to eradicate. `transformers` provides a unified API to fetch, use a...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eLet's dive into the `transformers` library. Although big tech is going crazy over LLMs, DNA language models are where the money is.\u003cd-footnote\u003eCitation required\u003c\u002fd-footnote\u003e In that spirit, in this po...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eI will be providing snippets of code along with the text. If you are still curious about the nitty-gritty, all the code is available [on Github](https:\u002f\u002fgithub.com\u002fhclimente\u002fhclimente.github.io\u002fblob\u002fm...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eThe [Nucleotide Transformer](https:\u002f\u002fwww.nature.com\u002farticles\u002fs41592-024-02523-z) (NT) is an encoder-only transformer, essentially a [BERT model](\u003chttps:\u002f\u002fen.wikipedia.org\u002fwiki\u002fBERT_(language_model)\u003e) ...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e{% include figure.liquid path=\"assets\u002fimg\u002fposts\u002f2025-05-02-hf-transformers\u002fnucleotide_transformer.jpg\" class=\"img-fluid\" %}...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e\u003cdiv class=\"caption\"\u003e\n    Training of the NT using masked language modelling (MLM). Adapted from Figure 1 in the \u003ca href=\"https:\u002f\u002fwww.nature.com\u002farticles\u002fs41592-024-02523-z\"\u003eNT article\u003c\u002fa\u003e.\n\u003c\u002fdiv\u003e...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eIn MLM a random bit of the input DNA sequence will be hidden from the model. The task of the model is to retrieve the masked subsequences using the rest of the sequence. Let's say that the input seque...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e1. **Tokenizer:** First, we convert the input DNA sequence into a sequence of integers (_tokens_), each representing a subsequence of length 6 nucleotides (\"6-mers\"). The total number of tokens is 4,1...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e   In the case of our 18-nucleotide sequence `ATGGTAGCTACATCATCT`, the tokenizer transforms it into a tokenized sequence of length 4: `[3, 506, 3662, 1567]`. This includes the CLS token (`3`) and thre...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e2. **Embedding layer:** An embedding layer transforms the tokenized sequence of integers into an fixed-length vector of real values (_embedding_). On this embedding, a positional encoding is added to ...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e3. **Transformer encoder:** Here comes the main event: the stacked Transformer encoder blocks (since the NT is an encoder-only model, remember?). These blocks are where the magic actually happens, pro...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e4. **Token probabilities:** Finally the last layer's embedding is transformed into a probability of each token in each of the input positions. Since there were 3 input positions and 4,107 possible tok...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e   In our example, the masked token was `1567` and was in the last position. If our model has done a good job, the matrix entry (3, 1567) will be close to 1, and the rest of the entries in that row wi...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eBy repeating this process over and over, on DNA sequences obtained from very different species, the model learns to guess the hidden sequence from it's genomic context...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eBut, **what is it _really_ learning?** My intuition is that it's picking up general patterns across genomes...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eFor instance, after looking at many protein-coding sequences it might learn the pattern that we would adscribe to an alpha helix...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eBy putting together some of such patterns, it might learn that protein-coding sequences are related...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eThen, it could leverage this knowledge in the MLM task to predict a sequence that preserves the alpha helix with the observed codon usage...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eSimilarly, it might learn that another mask is around the right genomic distance from an ORF, and deduce it should predict what we recognize as a promoter...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eIn all this proess the NT has no access to phenotypic information or explicit knowledge about promoters, genes or alpha helices...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eAlthough it is getting a glimpse of evolutionary constraints by being exposed to different genomes, it won't be able to learn sophisticated genomic regulation patterns....","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eNow that the theory is out of the way, let's start exploring the Hugging Face ecosystem. There are two elements of it that vastly facilitate sharing and leveraging pre-trained models....","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eOne is the [Model Hub](https:\u002f\u002fhuggingface.co\u002fdocs\u002fhub\u002fen\u002findex), a repository for the community to share and discover pre-trained models. In this post I use the smallest NT, [a 50 million parameter m...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eThe other one is the many [`transformers` AutoClasses](https:\u002f\u002fhuggingface.co\u002fdocs\u002ftransformers\u002fmodel_doc\u002fauto). They abstract away the specific model architecture, and the changes that would be neede...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e```python\nmodel = AutoModelForMaskedLM.from_pretrained(\n  \"InstaDeepAI\u002fnucleotide-transformer-v2-50m-multi-species\",\n  trust_remote_code=True\n)...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e```\n...\n    (layer_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n    (decoder): Linear(in_features=512, out_features=4107, bias=False)\n  )\n)\n```...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eBy using the `from_pretrained` method, we are loading both the architecture and the weights of the model. By default, the model is in evaluation mode; if we were to further fine-tune it, we would need...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eIf instead we wanted to leverage the pre-trained model for binary classification, we would run:...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e```python\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"InstaDeepAI\u002fnucleotide-transformer-v2-50m-multi-species\",\n    num_labels=2,\n    trust_remote_code=True\n)...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e```\n...\n    (dropout): Dropout(p=0.0, inplace=False)\n    (out_proj): Linear(in_features=512, out_features=2, bias=True)\n  )\n)\n```...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eAs we can see, this added a (disabled) dropout layer, and a linear layer with two outputs, as requested....","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eThe model cannot be applied directly to a DNA sequence, which needs to be [tokenized first](#a-worked-out-training-example). Another AutoClasses, the [AutoTokenizer](https:\u002f\u002fhuggingface.co\u002fdocs\u002ftransf...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e```python\ntokenizer = AutoTokenizer.from_pretrained(\n  \"InstaDeepAI\u002fnucleotide-transformer-v2-50m-multi-species\",\n  trust_remote_code=True\n)\n```...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eThe NT's [Model Card](https:\u002f\u002fhuggingface.co\u002fInstaDeepAI\u002fnucleotide-transformer-v2-50m-multi-species) shows how to embed DNA sequences. I copied that code below for your convenience:...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e```python\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nimport torch...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e# Import the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(\n    \"InstaDeepAI\u002fnucleotide-transformer-v2-50m-multi-species\",\n    trust_remote_code=True\n    )\nmodel = AutoModelForMask...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e# Choose the length to which the input sequences are padded. By default, the\n# model max length is chosen, but feel free to decrease it as the time taken to\n# obtain the embeddings increases significa...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e# Create a dummy dna sequence and tokenize it\nsequences = [\"ATGGTAGCTACATCATCT\"]\ntokens_ids = tokenizer.batch_encode_plus(\n    sequences,\n    return_tensors=\"pt\",\n    padding=\"max_length\",\n    max_len...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e# Compute the embeddings\nattention_mask = tokens_ids != tokenizer.pad_token_id\ntorch_outs = model(\n    tokens_ids,\n    attention_mask=attention_mask,\n    encoder_attention_mask=attention_mask,\n    out...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e# Compute sequences embeddings\nembeddings = torch_outs['hidden_states'][-1].detach().numpy()\nprint(f\"Embeddings shape: {embeddings.shape}\")\nprint(f\"Embeddings per token: {embeddings}\")...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e# Add embed dimension axis\nattention_mask = torch.unsqueeze(attention_mask, dim=-1)...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e# Compute mean embeddings per sequence\nmean_sequence_embeddings = torch.sum(attention_mask*embeddings, axis=-2)\u002ftorch.sum(attention_mask, axis=1)\nprint(f\"Mean sequence embeddings: {mean_sequence_embed...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e```...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e```mermaid\n---\nconfig:\n  layout: elk\n  look: handDrawn\n---\nflowchart LR\n    %% Style definitions\n    classDef process fill:#a8dadc,stroke:#2f4f4f,stroke-width:2px,rx:8,ry:8,color:#000\n    classDef dat...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e    %% Process nodes\n    P2[Tokenizer]:::process\n    P3[Model Inference]:::process\n    P5[Postprocessing]:::process...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e    %% Data nodes\n    D1[DNA Sequence]:::data\n    D21[Tokens]:::data\n    D22[Attention Mask]:::data\n    D3[Embeddings]:::data\n    D5[Masked Embeddings]:::data...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e    %% Connections\n    D1 --\u003e P2\n    P2 --\u003e D21\n    P2 --\u003e D22\n    D21 --\u003e P3\n    D22 --\u003e P3\n    P3 --\u003e D3\n    D22 --\u003e P5\n    D3  --\u003e P5\n    P5 --\u003e D5\n```...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eThe attention mask is a binary mask that, for a given input sequence, identifies the padding tokens that are there just to make the sequence fit the desired shape. They help the model avoid wasting (C...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e[Hugging Face's `pipelines`](https:\u002f\u002fhuggingface.co\u002fdocs\u002ftransformers\u002fpipeline_tutorial) exist to encapsulate these inference steps while cutting the boilerplate code. In particular, every pipeline re...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e- A function to sanitize the pipeline user-provided arguments\n- A preprocessing function that converts inputs (DNA sequences) into tokenized sequences\n- A forward function that passes the tokenized se...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eI implemented a small pipeline to embed DNA sequences. Its inputs are Python strings and the output are numpy arrays....","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e    def _sanitize_parameters(\n        self,\n        **kwargs,\n    ) -\u003e Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\n        \"\"\"\n        Sanitize the parameters for the pipeline....","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e        Returns:\n            Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]: A tuple containing\n                the sanitized parameters for preprocessing, model forward pass, and\n             ...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e        if \"max_length\" in kwargs:\n            preprocess_params[\"max_length\"] = kwargs[\"max_length\"]...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e        unrecognized_params = set(kwargs.keys()) - recognized_params\n        if unrecognized_params:\n            raise ValueError(f\"Unrecognized pipeline parameters: {unrecognized_params}\")...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e    def preprocess(\n        self,\n        model_inputs: Union[str, List[str]],\n        max_length: Optional[int] = None,\n    ) -\u003e List[pt.Tensor]:\n        \"\"\"\n        Preprocess the input sequences be...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e        Args:\n            model_inputs (Union[str, List[str]]): The input sequence(s) to be tokenized.\n            max_length (Optional[int]): The maximum length of the tokenized sequences.\n          ...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e        Returns:\n            List[pt.Tensor]: The tokenized input sequences.\n        \"\"\"\n        if max_length is None:\n            max_length = self.tokenizer.model_max_length...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e        tokens_ids = self.tokenizer.batch_encode_plus(\n            model_inputs,\n            return_tensors=\"pt\",\n            padding=\"longest\",\n            max_length=max_length,\n            truncati...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e    def _forward(\n        self,\n        model_inputs: pt.Tensor,\n    ) -\u003e Dict[str, Any]:\n        \"\"\"\n        Forward pass through the model....","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e        Returns:\n            Dict[str, Any]: The model outputs.\n        \"\"\"\n        # find out which of the tokens are padding tokens\n        # these tokens will be ignored by the model\n        attent...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e        out = self.model(\n            model_inputs,\n            attention_mask=attention_mask,\n            encoder_attention_mask=attention_mask,\n            output_hidden_states=True,\n        )...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e    def postprocess(\n        self,\n        model_outputs: Dict[str, Any],\n    ) -\u003e dict[str, np.ndarray]:\n        \"\"\"\n        Compute the mean sequence embedding from the last hidden layer (size 512)....","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e        Returns:\n            dict[str, np.ndarray]: The mean sequence embeddings for each input sequence.\n        \"\"\"\n        embeddings = model_outputs[\"hidden_states\"][-1].detach()\n        attention...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e```python\ntokenizer = AutoTokenizer.from_pretrained(\n    \"InstaDeepAI\u002fnucleotide-transformer-v2-50m-multi-species\",\n    trust_remote_code=True,\n)\nmodel = AutoModelForMaskedLM.from_pretrained(\n    \"Ins...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003epipeline = DNAEmbeddingPipeline(model=model, tokenizer=tokenizer)\nembedding = pipeline(\"ATGGTAGCTACATCATCTG\")\n```...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eEncapsulating the model into its own inference pipeline has a few advantages. Beyond the obvious benefit of cleaner code by separating inference steps into logical abstractions, it makes swapping mode...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eI will be using the NT to embed protein-coding DNA sequences from six species: three animals (human, mouse and fruit fly); one plant (arabidopsis); one bacteria (_E. coli_); and one yeast (_S. cerevis...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eTo this end, I [downloaded the DNA sequences](https:\u002f\u002fgithub.com\u002fhclimente\u002fhclimente.github.io\u002fblob\u002fmain\u002fassets\u002fpython\u002f2025-05-02-hf-transformers\u002fprepare_data.sh) of all protein coding genes for the s...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eI chose the length of the sequence because of convenience: they are a common sequence length for FASTA files, and short enough for my modest home computer to handle...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eHalf of them were the train set, used for model building; the other half constituted the test set, used exclusively for performance evaluation...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eI [embedded the sequences](https:\u002f\u002fgithub.com\u002fhclimente\u002fhclimente.github.io\u002fblob\u002fmain\u002fassets\u002fpython\u002f2025-05-02-hf-transformers\u002fmain.ipynb) and used a UMAP to visualize the embeddings:...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e\u003cdiv class=\"l-page\"\u003e\n    \u003ciframe src=\"{{ '\u002fassets\u002fpython\u002f2025-05-02-hf-transformers\u002fplotly\u002fumap_embeddings.html' | relative_url }}\" frameborder='0' scrolling='no' height=\"500px\" width=\"100%\"\u003e\u003c\u002fiframe\u003e...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e\u003cdiv class=\"caption\"\u003e\n    Scatter plot of the two UMAP dimensions from the embeddings computed by applying the \u003cem\u003epre-trained\u003c\u002fem\u003e NT to the 6,000 DNA sequences test dataset, containing 1,000 sequenc...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eSome disclaimers need to be made. First, I took a minuscule sample of all protein coding sequences, which my sampling process slightly biases towards the beginning of the protein. Second, I am using t...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eEven with these limitations, sequences from the same species tend to inhabit similar regions of the underlying manifold...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eSince this is probably not too reassuring, maybe I can do better: I trained a multiclass logistic regression tasked with predicting the species from the sequence embeddings...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eThis classifier achieved an accuracy of $$0.47$$, convincingly above the accuracy of a random classifier ($$\\frac 1 6 = 0.16$$)...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eFurthermore, some of the errors are clearly between the two closest species from an evolutionary standpoint: human and mouse....","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e{% include figure.liquid loading=\"eager\" path=\"assets\u002fpython\u002f2025-05-02-hf-transformers\u002fimg\u002fconfusion_matrix_test.webp\" class=\"img-fluid\" %}...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eThe NT was trained via MLM, and it never got any explicit information about which species it was looking at. Hence, it's not too surprising that it can't separate different species right off the bat. ...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e```python\nclassif_nt = AutoModelForSequenceClassification.from_pretrained(\n    \"InstaDeepAI\u002fnucleotide-transformer-v2-50m-multi-species\",\n    num_labels=6,\n    trust_remote_code=True\n)\n```...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eThe `Trainer` makes fine-tuning the model quite easy. The task is to predict the species from the sequence. I froze the first few layers from the NT, which should capture low level features of the seq...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e```python\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=64,\n    per_device_eval_batch_size=64,\n    num_train_epochs=10,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    l...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003etrainer = Trainer(\n    model=classif_nt,\n    args=training_args,\n    train_dataset=tr_train_ds,    # train on 90% of the train set\n    eval_dataset=tr_val_ds,       # evaluate on 10% of the train set\n...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eAfter the model is trained, as specified in the trainer arguments, the model with the best performance on the validation dataset will be the loaded....","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eWe can create a new inference pipeline focus around classification. The pipeline will output both the probability of each class, as well as the embeddings, obtained from the last layer. Since this mod...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e\u003cdiv class=\"l-page\"\u003e\n    \u003ciframe src=\"{{ '\u002fassets\u002fpython\u002f2025-05-02-hf-transformers\u002fplotly\u002fumap_embeddings_ft-model.html' | relative_url }}\" frameborder='0' scrolling='no' height=\"500px\" width=\"100%\"\u003e...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003e\u003cdiv class=\"caption\"\u003e\n    Scatter plot of the two UMAP dimensions from the embeddings computed by applying the \u003cem\u003efine-tuned\u003c\u002fem\u003e NT to the 6,000 DNA sequences test dataset, containing 1,000 sequence...","\u003cb\u003eDNA language model fine-tuning and inference\u003c\u002fb\u003e\u003cbr\u003eIn this post, I have given a primer on how to use Hugging Face's libraries for a particular flavor of BioML work. Yet, in my opinion, Hugging Face's greatest strength lies just in the boundaries of th...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e$$\nU \\sim N(0,...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003eWe want to discover this structure from observational data. Since both $$X$$ and $$Y$$ are caused by $$U$$, a correlation is not very enlightening and will just return the fully connected graph:...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e{% include figure.liquid loading=\"eager\" path=\"assets\u002fpython\u002f2025-05-29-precision-matrix\u002fimg\u002fcorrelations.webp\" class=\"img-fluid\" %}...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e\u003cdiv class=\"caption\"\u003e\n    Scatter plots of each pair of variables on 200 observations, each with the correlation between the variables and the associated P-value indicated above.\n\u003c\u002fdiv\u003e...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003eA sensible way of going about it is to study the correlation between each pair of variables after adjusting for the remaining variable. If we assume all relationships are linear, these are called **pa...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e```python\ndef pcorr_residuals(X: np.ndarray) -\u003e np.ndarray:\n    \"\"\"\n    Compute the matrix of partial correlations from the residuals of linear regression models....","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e    Returns\n    -------\n    np.ndarray\n        The matrix of partial correlations.\n    \"\"\"...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e    for i in range(p):\n        for j in range(i + 1, p):\n            covariates_indices = [k for k in range(p) if k != i and k != j]\n            X_covars = X[:, covariates_indices]...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e                # fit a linear model\n                beta, *_ = np.linalg.lstsq(X_covars, y, rcond=None)\n                y_pred = X_covars @ beta...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e                # compute and center the residuals\n                r = y - y_pred\n                residuals[(target, excluded)] = r - r.mean()...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e            res_1 = residuals[(i, j)]\n            res_2 = residuals[(j, i)]\n            corr = np.dot(res_1, res_2) \u002f (np.linalg.norm(res_1) * np.linalg.norm(res_2))...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003ePartial correlations correctly identify that $$U$$ is correlated with both $$X$$ and $$Y$$, and in turn that those are not correlated once we account for the effect of $$U$$:...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e{% include figure.liquid loading=\"eager\" path=\"assets\u002fpython\u002f2025-05-29-precision-matrix\u002fimg\u002fpartial_correlations.webp\" class=\"img-fluid\" %}...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e\u003cdiv class=\"caption\"\u003e\n    Scatter plots of the residuals of each pair of variables on 200 observations, each with the \u003cem\u003epartial correlation\u003c\u002fem\u003e between the variables and its associated P-value indi...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003eNote that while $$\\hat{\\rho}_{X, U} \\approx \\rho_{X, U} = 0.8944$$, $$\\hat{\\rho}_{X, U \\mid Y} \\neq \\rho_{X, U}$$. This is because $$Y$$ contains an additional noise term that makes the adjustment imp...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003eA downside of this approach is its computational complexity. For an $$n \\times p$$ input matrix:...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e- Memory complexity: $$\\mathcal{O}(np^2)$$, dominated by storing $${p \\choose 2} = \\mathcal{O}(p^2)$$ residuals, each of length $$n$$.\n- Time complexity: $$\\mathcal{O}(np^4)$$, dominated by computing ...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003eThis is quite computational intensive, which will become a problem in real-world problems. Can we do better? Enter the **precision matrix**, a nice mathematical object to do this at scale....","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003eThe variance takes values in $$[0, \\infty)$$, and measures how disperse the outcomes of the RV are from its mean. Notably, the **(scalar) precision** is defined as $$\\frac 1 \\sigma_X^2$$, so high vari...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003eThe **covariance** between two random variables, $$X_1$$ and $$X_2$$, is defined as:...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e$$\n\\text{Cov}(X_1, X_2) = \\mathbf{E}((X_1 - \\mathbf{E}(X_1))(X_2 - \\mathbf{E}(X_2))).\n$$...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003eObserve that if $$X_1 = X_2$$, $$\\sigma_{X_1}^2 = \\sigma_{X_2}^2 = \\text{Cov}(X_1, X_2)$$....","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003eThe covariance takes values in $$(-\\sigma_{X_1} \\sigma_{X_2}, \\sigma_{X_1} \\sigma_{X_2})$$, and measures the degree to which two random variables are linearly related. The **correlation** $$\\rho$$ nor...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003eThe **covariance matrix** of a set of random variables ties the variance and the covariance together. If $$\\mathbf{X}$$ is a column vector such that...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e$$\n\\mathbf{X} = \\begin{pmatrix}\n  X_1 \\\\\n  X_2 \\\\\n  \\vdots \\\\\n  X_n\n\\end{pmatrix}\n$$...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e$$\n\\mathbf{\\Sigma} = \\begin{pmatrix}\n    \\sigma_{X_1}^2        & \\text{Cov}(X_1, X_2) & \\cdots & \\text{Cov}(X_1, X_n) \\\\\n    \\text{Cov}(X_2, X_1)  & \\sigma_{X_2}^2       & \\cdots & \\text{Cov}(X_2, X_n...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003eSince always $$\\text{Cov}(X_i, X_j) = \\text{Cov}(X_j, X_i)$$, $$\\mathbf{\\Sigma}$$ is _symmetric_. It is, in fact, _positive semi-definite_ ([proof](https:\u002f\u002fstatproofbook.github.io\u002fP\u002fcovmat-psd.html))....","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003eBy normalizing the covariance matrix by dividing each item $$\\mathbf{\\Sigma}_{ij}$$ by $$\\sigma_{X_i} \\sigma_{X_j}$$, we obtain the **correlation matrix**:...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e$$\nP = \\begin{pmatrix}\n    1               & \\rho_{X_1, X_2} & \\cdots & \\rho_{X_1, X_n} \\\\\n    \\rho_{X_2, X_1} & 1               & \\cdots & \\rho_{X_1, X_n} \\\\\n    \\vdots          & \\vdots          & \\...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003eFinally, the **precision matrix** $$\\mathbf{\\Sigma}^{-1}$$ is the inverse of the covariance matrix, i.e., $$\\mathbf{\\Sigma} \\mathbf{\\Sigma}^{-1} = \\mathbf{I}$$. $$\\mathbf{\\Sigma}$$ is not guaranteed t...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e$$...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e    1                                           & -\\rho_{X_1, X_2 \\mid X_3, \\dots, X_n}          & \\cdots & -\\rho_{X_1, X_n \\mid X_2, \\cdots, X_{n-1}}      \\\\...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e    -\\rho_{X_2, X_1 \\mid X_3, \\cdots, X_n}      & 1                                              & \\cdots & -\\rho_{X_2, X_n \\mid X_1, X_3, \\cdots, X_{n-1}} \\\\...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e    \\vdots                                      & \\vdots                                         & \\ddots & \\vdots                                          \\\\...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e    -\\rho_{X_n, X_1 \\mid X_2, \\cdots, X_{n-1}}  & -\\rho_{X_n, X_2 \\mid X_1, X_3 \\cdots, X_{n-1}} & \\cdots & 1...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e$$\nD =\n\\begin{pmatrix}\n    \\frac 1 {\\sigma_{X_1 \\mid X_2, \\cdots, X_n}} &                                                   &        & 0 \\\\\n                                                 & \\frac 1 {...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003eThe entries $$\\rho_{X_., X_. \\mid \\dots}$$ in the middle matrix are our precious **partial correlations**....","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003eLet's revisit our motivating example equipped with our newfound knowledge: instead of fitting $$\\mathcal{O}(p^2)$$ linear models, let's reach the same result using linear algebra. First, we will estim...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e```python\ndef pcorr_linalg(X: np.ndarray) -\u003e np.ndarray: # (n, p) -\u003e (p, p)\n    \"\"\"\n    Compute the matrix of partial correlations from the covariance matrix....","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e    Returns\n    -------\n    np.ndarray\n        The matrix of partial correlations.\n    \"\"\"...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e    # showing how the sausage is made\n    # but could be replaced by covariance = np.cov(X, rowvar=False)\n    centered_X = X - X.mean(axis=0)\n    covariance = np.dot(centered_X.T, centered_X) \u002f (n - 1...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e    normalization_factors = np.sqrt(np.outer(np.diag(precision), np.diag(precision)))\n    partial_correlations = - precision \u002f normalization_factors...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003eThis implementation is not only more compact, but has a more favorable computational complexity:...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e- Memory complexity: $$\\mathcal{O}(p^2)$$, dominated by the intermediate matrices.\n- Time complexity: $$\\mathcal{O}(np^2 + p^3)$$, dominated by the computation of the covariance matrix and by the matr...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003eFurthermore, this implementation is [vectorized]({% post_url 2024-02-04-python-vectors %}) which further boosts performance. As a quick benchmark, on a random $$1000 \\times 100$$ matrix, the original ...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003eAs with many elegant results in linear algebra, things start breaking down when our covariance matrix is [ill-conditioned](https:\u002f\u002fen.wikipedia.org\u002fwiki\u002fCondition_number) or outright [non-invertible](...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003eIn [high-dimensional problems](https:\u002f\u002fen.wikipedia.org\u002fwiki\u002fHigh-dimensional_statistics), $$\\Sigma$$ is non-invertible (and hard to estimate in the first place)...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003eIn such cases, we could use [the pseudoinverse matrix](https:\u002f\u002fen.wikipedia.org\u002fwiki\u002fMoore%E2%80%93Penrose_inverse) instead of the inverse...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003eBut that's just a patch: we will get results, but we are outside of the theory and interpreting the results is not as straightforward...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003eHowever, when the matrix is ill-conditioned, there is a potential path to salvation: [regularization](https:\u002f\u002fscikit-learn.org\u002fstable\u002fmodules\u002fcovariance.html)....","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003eAdding a regularization step to the covariance matrix estimation will result in a better conditioned matrix. A common approach is _shrinking_ our empirical covariance towards another matrix, the _targ...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e$$\n\\hat{\\mathbf{\\Sigma}} = (1 - \\alpha) \\hat{\\mathbf{\\Sigma}}_\\text{MLE} + \\alpha T\n$$...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003ewhere $$\\alpha \\in [0, 1]$$ is a parameter and $$T$$ is the target matrix, a highly structured matrix that encodes our assumption about what a _true_ covariance matrix should look like. A possible and...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003eThe problem becomes then tuning $$\\alpha$$. A common way to compute the $$\\alpha$$ is the [Ledoit-Wolf shrinkage method](https:\u002f\u002fweb.archive.org\u002fweb\u002f20141205061842\u002fhttp:\u002f\u002fwww.econ.uzh.ch\u002ffaculty\u002fledoi...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003eAlternatively, we can use graphical lasso to estimate a sparse precision matrix. Conceptually, this is a bit easier to swallow: in many situations, most variables being conditionally uncorrelated is a...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e$$\n\\hat{\\mathbf{\\Sigma}}^{-1} = \\operatorname{argmin}_{\\mathbf{\\Sigma}^{-1} \\succ 0} \\left(\\operatorname{tr}(\\mathbf{\\Sigma} \\mathbf{\\Sigma}^{-1}) - \\log \\det \\mathbf{\\Sigma}^{-1} - \\lambda \\|\\mathbf{...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003eThe $$- \\lambda \\|\\mathbf{\\Sigma}^{-1}\\|_1$$ term will favor sparse matrices, with a strength proportional to the magnitude of $$\\lambda$$. While tuning $$\\lambda$$ is in itself a challenge, a common ...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003eLet's bring this point home by looking at a high-dimensional example (20 samples, 20 features)....","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e{% include figure.liquid loading=\"eager\" path=\"assets\u002fpython\u002f2025-05-29-precision-matrix\u002fimg\u002fhigh_dimensional_experiments.webp\" class=\"img-fluid\" %}...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e\u003cdiv class=\"caption\"\u003e\n    Ground truth and estimated covariance matrix, precision matrix and structure of a high-dimensional example. The data generation process involved 20 samples, with 20 features ...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003eDue to the extremely ill-conditioned covariance matrix, the precision matrix is completely off scale, with values ranging from -1.8e+15 to 1.0e+15...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003eWhile it is still far from the ground truth, it prunes away most of the spurious correlations and keeps most of the true links...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e[As expected](https:\u002f\u002fscikit-learn.org\u002fstable\u002fmodules\u002fcovariance.html#sparse-inverse-covariance), most of the true links are larger in absolute value, and further pruning it would return something clo...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003eMore than anything, this little exercise shows how hard this endeavour is, and serves as a good caution to high-dimensional statistics. Beware!...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003eUnder certain assumptions, the precision matrix helps us discover the internal structure of the data. When should we use what to estimate it?...","\u003cb\u003eCovariance and precision\u003c\u002fb\u003e\u003cbr\u003e1. **Empirical inverse (MLE):** fast and exact, but blows up if $$p$$ approaches $$n$$ or $$\\hat \u03a3$$ is singular. Use it when $$n \\gg p$$ and $$\\hat \u03a3$$ is well\u2011conditioned.\n1. **Shrinkage (Ledoit-Wol...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eBut for us, entropy is not (only) about m...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eBut for us, entropy is not (only) about messy bedrooms, but about messy _data_. That's why I will focus on _Shannon's_ entropy $$H(P)$$, which is a property of a probability distribution $$P$$. For a ...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eAs stated, using the binary logarithm, entropy is measured in [bits](https:\u002f\u002fen.wikipedia.org\u002fwiki\u002fBit); when using the natural logarithm instead, the unit of measure is nats. From here on, $$\\log$$ m...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eIn a nutshell, entropy is the average surprise we'll experience when observing a realization of $$P$$: if an outcome is rare ($$P(x)$$ is small), observing it should be quite surprising ($$\\log \\frac{...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eA more tangible interpretation of entropy links it to the _encoding_ of a message. Imagine we want to encode the outcome of a probability distribution. We observe an outcome and want to unambiguously ...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003e| Weather | Probability |\n| ------- | ----------- |\n| Cloudy  | 0.5         |\n| Rainy   | 0.4         |\n| Sunny   | 0.1         |...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003e\u003e For easier computations, let's assume that probabilities remain independent and constant over time. Which, again, isn't too far from my reality. More formally, the outcomes are independent and ident...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eEvery morning, I look out the window exactly at 9am, and send my friend the weather report. Our first instinct is probably to just text them \"cloudy\", \"rainy\" or \"sunny\" as appropriate. If we encode t...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003e| Weather | Probability | Codeword                                         | Codeword length |\n| ------- | ----------- | ------------------------------------------------ | --------------- |\n| Cloudy  ...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eIn the long term, the average message will take $$0.5 \\times 48 + 0.4 \\times 40 + 0.1 \\times 40 = 44$$ bits. Not a big deal I guess... But we can do much better! _Why waste time say lot word when few ...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eThe [Huffman coding](https:\u002f\u002fen.wikipedia.org\u002fwiki\u002fHuffman_coding) is a common solution to this problem which significantly shortens our average message by leveraging our knowledge of $$P$$:...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003e| Weather | Probability | Codeword | Codeword length |\n| ------- | ----------- | -------- | --------------- |\n| Cloudy  | 0.5         | 0        | 1               |\n| Rainy   | 0.4         | 10       ...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003e**Huffman coding** builds an optimal prefix code for a known distribution. Here's how it works for our weather example:...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003e   | Weather | Probability |\n   | ------- | ----------- |\n   | Cloudy  | 0.5         |\n   | Rainy   | 0.4         |\n   | Sunny   | 0.1         |...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003e2. **Merge lowest pairs:** Combine Sunny (0.1) and Rainy (0.4) \u2192 node with weight 0.5. Then combine with Cloudy (0.5) \u2192 final tree....","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003e   | Weather | Codeword |\n   | ------- | -------- |\n   | Cloudy  | 0        |\n   | Rainy   | 10       |\n   | Sunny   | 11       |...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eHuffman coding guarantees the shortest average length for any prefix code based on the true distribution....","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eThis is much better: we have gone from 44 bits to $$0.5 \\times 1 + 0.4 \\times 2 + 0.1 \\times 2 = 1.5$$ bits on average. Of course, for this to be possible, we need to have access to the true weather d...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eDespite Huffman being optimal, our message length will be the same on rainy days and on sunny days...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eThe core problem is that our messages have an _integer_ length, and we would need _fractional_ lengths to do better...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eThe most likely one is a streak of ten cloudy days, which occurs with probability $$0.5^{10}$$...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eConsequently, the Huffman coding assigns a much shorter codeword to this string than to the most unlikely string, a streak of ten sunny days:...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003e| 10-day weather | Probability | Codeword                         | Codeword length |...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003e| -------------- | ----------- | -------------------------------- | --------------- |...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003e| CCCCCCCCCC     | 9.77e-04    | 0111001010                       | 10              |...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eThe average length of this code is 13.64 bits, or 1.364 bits per day. Batching outcomes together allows us to spend only _fractions_ of a bit. And it's easy to see how, if we kept batching more and mo...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003e{% include figure.liquid loading=\"eager\" path=\"assets\u002fpython\u002f2025-07-14-cross-entropy\u002fimg\u002fentropy-batch_size_vs_avg_bits_per_day.webp\" class=\"img-fluid rounded z-depth-1\" %}...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eAnd this brings us to the key point: entropy represents the lower bound for the average message length required to _optimally_ encode each outcome of a random process. Even with the best encoding we c...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003e$$\nH(P) = 0.5 \\times \\log \\frac 1 {0.5} + 0.4 \\times \\log \\frac 1 {0.4} + 0.1 \\times \\log \\frac 1 {0.1} = 1.361 \\text{ bits}\n$$...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eAll this is fine, but a lingering question remains: what's the logarithm of the probability doing there? Why aren't we using any other transformation of the probability?...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eImagine the space of all possible codewords of a prefix code. If we decide to use the codeword \"0\", every other codeword needs to start by \"1\"; that choice cost us half of all possible codewords. Henc...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eWe just saw how knowing the underlying probability distribution gave us an edge in encoding the outcomes efficiently. However, here in the real world we rarely have access to _true_ probability distri...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eFor instance, we rely on very complex models to accurately predict the weather. But let's leave those aside, and use the simple model ($$Q_\\text{Barcelona}$$) and associated Huffman code I developed f...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003e| Weather | Probability | Codeword | Codeword length |\n| ------- | ----------- | -------- | --------------- |\n| Cloudy  | 0.2         | 11       | 2               |\n| Rainy   | 0.1         | 10       ...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eAs you can imagine, after moving to London, my model of the weather was not that useful. In fact, I often experienced _surprise_, as outcomes that should be rare happened often. In consequence, when u...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eEntropy quantified our average surprise when observing a distribution's outcomes while knowing the true distribution. Similarly, the **cross-entropy** measures our surprise when observing a distributi...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eJust like entropy, $$\\log \\frac{1}{Q(x)}$$ measures the degree of surprise we expect as per our model, which is weighted by the actual frequency with which we observe the outcome. It is also measured ...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003e$$\nH(P_\\text{London}, Q_\\text{Barcelona}) = 0.5 \\times \\log \\frac 1 {0.2} + 0.4 \\times \\log \\frac 1 {0.1} + 0.1 \\times \\log \\frac 1 {0.7} \\approx 2.54 \\text{ bits}.\n$$...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eThis is higher than the average message length of 1.9 bits. Contrary to entropy, which is a hard-limit, our model _can_ do better than cross-entropy. This is because the cross-entropy leverages (optim...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003e| Weather | P   | Codeword length | Q-optimal codeword length | Extra ($$+$$)\u002fSaved ($$-$$) bits |\n| ------- | --- | --------------- | ------------------------- | -------------------------------- |\n| ...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eNotice how we're saving a ton of bits on cloudy and rainy days; we got lucky. If we batch our weather reports, we get closer to encoding individual outcomes with fractional bits. Using the 10-day Barc...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003e{% include figure.liquid loading=\"eager\" path=\"assets\u002fpython\u002f2025-07-14-cross-entropy\u002fimg\u002fcrossentropy-batch_size_vs_avg_bits_per_day.webp\" class=\"img-fluid rounded z-depth-1\" %}...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eAfter a few years in London my model became quite accurate, to the extent that $$Q_\\text{London} \\approx P_\\text{London}$$:...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003e$$\nH(P_\\text{London}, Q_\\text{London}) = 0.5 \\times \\log \\frac 1 {0.5} + 0.4 \\times \\log \\frac 1 {0.4} + 0.1 \\times \\log \\frac 1 {0.1} \\approx 1.36 \\text{ bits}.\n$$...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eThis is an important result (the [Gibbs' inequality](https:\u002f\u002fen.wikipedia.org\u002fwiki\u002fGibbs%27_inequality)):...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eSince entropy is the lower bound for cross-entropy, the difference between both informs us about how well our model reflects the true distribution. This difference is also so important that it has its...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eIt can be interpreted as the _cost of being wrong_: how many extra bits we need to spend because our model departs from the true distribution....","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eUltimately, this is why I went down this rabbit hole. We\u2019ve covered distributions, processes, and encoding. But machine learning is one of the most important applications of cross-entropy via the [cro...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eLet's bring this point home by revisiting our weather model one last time. In this case, we want a model to predict tomorrow's weather using some sensible variables (like today's weather, temperature,...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003e\u003e Note that $$p$$ is the one-hot (empirical) distribution on the observed class, not the true generative $$P$$, and $$q$$ is just the model's prediction for this example, not the overall distribution ...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003e$$\n\\mathcal{L}(p,q(x))\n   = 1 \\times \\log\\frac{1}{q_C} + 0 \\times \\log\\frac{1}{q_R} + 0 \\times \\log\\frac{1}{q_S}\n   = \\log\\frac{1}{q_C}\n   = - \\log q_C.\n$$...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003ewhere $$K$$ is the number of classes, and $$y$$ is the index of the true class ($$1$$ in our example, corresponding to class $$C$$)....","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eThe model will consequently update its parameters to minimize this loss, also known as log-loss. Minimizing it is equivalent to [maximizing the probability of the data](https:\u002f\u002fen.wikipedia.org\u002fwiki\u002fM...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003e$$\n\\mathcal{L} =\n   \\mathbb{E}_{(x,y) \\sim \\mathcal{D}} \\bigl[-\\log q(x)_y \\bigr] =\n   -\\frac{1}{N}\\sum_{i=1}^N \\log q(x^{(i)})_{y^{(i)}}.\n$$...","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003eAnd that\u2019s the objective in its entirety: to adjust the model's parameters until it is, on average, least surprised by the correct answer. At least until it sees the test set......","\u003cb\u003eCross-entropy. Intuition and applications.\u003c\u002fb\u003e\u003cbr\u003e- [Visual Information Theory](https:\u002f\u002fcolah.github.io\u002fposts\u002f2015-09-Visual-Information\u002f)\n- [The Key Equation Behind Probability (video)](https:\u002f\u002fwww.youtube.com\u002fwatch?v=KHVR587oW8I)...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eHowever, large swaths of data ca...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eHowever, large swaths of data can't naturally fit in a table. This is the case with corpora of text, DNA sequences, or songs. In such cases, finding _equal_ elements is still easy by, for instance, ch...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eOur goal is to find all documents in a collection of $$N$$ items (like documents, images, or songs) that are similar, though not necessarily equal, to our query item. We will represent each item in a ...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e1. **Embedding the items**, that is, finding a $$D$$-dimensional, meaningful vector representation of each item\n1. Computing the **similarity between items**\n1. Efficiently **finding the nearest neigh...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e**Embeddings** are just vectors that represent a piece of content\u2014a song, a text, or [a piece of DNA]({% post_url 2025-05-02-hf-transformers %}#embedding-dna-sequences)...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eYou can think of embeddings as the document's coordinates in some an arbitary, very high-dimensional space...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eThey are usually the activations of a given internal layer of a pre-trained model...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eFirst, that all the embeddings have the _same dimensions_, i.e., they map to the same space and we can neatly arrange them into a matrix...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eAfter all, the model had to learn a good representation of the data during training.\u003cd-footnote\u003eAs long as the training data is representative of our use case\u003c\u002fd-footnote\u003e This also ensures that seman...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eEmbedding the items provides many advantages: where we had atomic units of unstructured data, we now have numerical representations that can be mathematically operated on. For instance, we can go from...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e\u003cdiv class=\".l-body-outset\"\u003e\n    \u003ciframe src=\"{{ '\u002fassets\u002fpython\u002f2025-08-16-rags\u002fplotly\u002fposts_umap.html' | relative_url }}\" frameborder='0' scrolling='no' height=\"550px\" width=\"100%\"\u003e\u003c\u002fiframe\u003e\n\u003c\u002fdiv\u003e...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e\u003cdiv class=\"caption\"\u003e\n    Scatter plot of the two UMAP dimensions from the 384-dimensional embeddings computed by applying the \u003ca href=\"https:\u002f\u002fhuggingface.co\u002fsentence-transformers\u002fall-MiniLM-L6-v2\"\u003ea...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eWe can see that posts with similar tags are neatly close to each other. But the embeddings offer a bit more nuance than that. Posts about statistical methods live near those about machine learning. Th...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e# Small, high-quality model that is lightweight on CPU\nMODEL_NAME = \"sentence-transformers\u002fall-MiniLM-L6-v2\"...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003edef compute_embeddings(\n    texts: List[str], model: fastembed.text.TextEmbedding\n) -\u003e np.ndarray:\n    \"\"\"Compute embeddings using fastembed models.\"\"\"...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eI just glossed over a very important detail: embedding a whole document _is a bad idea_. We can see an embedding as a lossy compression of our document. Since the dimensionality of our embeddings is f...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eInstead of dealing with the whole document, it's better to split each post into semantically coherent chunks (e.g., paragraphs) and then embedded the chunks individually...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eFirst, it tries to get chunks of the right size splitting the documents into paragraphs...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eAnd, if these are still too long, we keep going, splitting them into subsentences, words and, finally, into individual characters if absolutely needed...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eFurthermore, each chunk includes a small overlap with the preceding and the following chunk, to ensure that some context is also passed along....","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e```python\ndef split_text(\n    text: str,\n    split_chars: List = [\"\\n\\n\", \"\\n\", [\". \", \"! \", \"? \"], \"; \", \", \", \" \"],\n    max_size: int = 500,\n) -\u003e List[str]:\n    \"\"\"Recursively split text into chunks...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e    if len(text) \u003c= max_size:\n        return [text]\n    elif not split_chars:\n        return [text[:max_size]]...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e    splitter = split_chars[0]\n    splitter = (\n        splitter if isinstance(splitter, str) else \"|\".join(map(re.escape, splitter))\n    )\n    splits = []...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e    for chunk in re.split(splitter, text.strip()):\n        splits.extend(split_text(chunk, split_chars[1:], max_size=max_size))...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eI applied this strategy to my 32 blog posts, obtaining 1,337 chunks of text. I then embedded each chunk individually. Here is the UMAP plot of the resulting embeddings:...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e\u003cdiv class=\".l-body-outset\"\u003e\n    \u003ciframe src=\"{{ '\u002fassets\u002fpython\u002f2025-08-16-rags\u002fplotly\u002fparagraphs_umap.html' | relative_url }}\" frameborder='0' scrolling='no' height=\"600px\" width=\"100%\"\u003e\u003c\u002fiframe\u003e\n\u003c\u002f...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e\u003cdiv class=\"caption\"\u003e\n    Scatter plot of the two UMAP dimensions from the 384-dimensional embeddings computed by applying the [all-MiniLM-L6-v2](https:\u002f\u002fhuggingface.co\u002fsentence-transformers\u002fall-MiniL...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eThe results here are now more nuanced. In general, the points are grouped by their post of origin, denoted by their color, which is a good sanity check. On the top right we find chunks coming from my ...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eIf we wanted to find equal documents, an efficient solution would be relatively straightforward: hash them and see which ones fall in the same bucket. But that's not the task we've embarked on. If the...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eMany distance and similarity measures have been defined for different kinds of data. In the vector spaces in which our embeddings live, the most popular ones are:...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e| Measure                |               Formula               | Meaning                                                        | Range                 |...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e| ---------------------- | :---------------------------------: | -------------------------------------------------------------- | --------------------- |...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e| **Cosine** similarity  | $$\\frac {u \\cdot v} {\\|u\\| \\|v\\|}$$ | Extent to which $u$ and $v$ point in the same direction        | $$[-1, 1]$$           |...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e| **Dot product**        |            $$u \\cdot v$$            | Same as cosine, but multiplied by the magnitude of $u$ and $v$ | $$(-\\infty, \\infty)$$ |...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e| **Euclidean** distance |   $$\\sqrt{\\sum_i (u_i - v_i)^2}$$   | Distance between the tips of $u$ and $v$ in Euclidean space    | $$[0, \\infty)$$       |...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eThe cosine similarity is a common choice to measure semantic similarity of embeddings. Although I haven't found a satisfying explanation for the why, the lengths of the vectors do not seem to carry in...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e```python\ndef cosine_similarity(X: np.ndarray) -\u003e np.ndarray:\n    \"\"\"Compute the cosine similarity matrix between the rows in X.\"\"\"\n    X_norm = X \u002f np.linalg.norm(X, axis=1, keepdims=True)\n    return...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e\u003cdiv class=\".l-body-outset\"\u003e\n    \u003ciframe src=\"{{ '\u002fassets\u002fpython\u002f2025-08-16-rags\u002fplotly\u002fparagraph_similarity_heatmap.html' | relative_url }}\" frameborder='0' scrolling='no' height=\"550px\" width=\"100%\"...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e\u003cdiv class=\"caption\"\u003e\n    Pairwise cosine similarities between all 1,337 text chunks.\n\u003c\u002fdiv\u003e...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eTo that end, I will embed it and compare that embedding against that of all my chunks. Here are the five chunks with the highest cosine similarity:...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e| Post title  | Cosine similarity | Text sample                                       |...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e| SHAP values | 0.514             | Machine learning models like linear regression... |...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e| SHAP values | 0.505             | - Interpretable Machine Learning: Shapley valu... |...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e| SHAP values | 0.403             | To establish the connection to Shapley values,... |...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e| SHAP values | 0.391             | Let's understand SHAP values better by looking... |...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e| SHAP values | 0.388             | Global explanations can be derived by aggregat... |...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eI would say the search found some relevant chunks. This operation has a time complexity of $$O(MN)$$ for the simple query, where $$N$$ is the number of posts and $$M$$ the average number of chunks per...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eIn the past, [I described]({% post_url 2017-11-04-finding-similar-items %}) a method to find similar items using **local-sensitivity hashing** (LSH)...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eThe idea is to hash the items in such a way that similar items are likely to fall in the same bucket...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eThis way, we can quickly scan over the corpus and find the items that are similar to our query...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eHere I will focus on **hierarchical navigable small world graphs** (HNSW), a more modern method that is the backbone of vector databases...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e[**Skip lists**]({% post_url 2024-02-15-data-structures %}#skip-lists) are a data structure consisting of a set of [linked lists]({% post_url 2024-02-15-data-structures %}#linked-lists), each one cont...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e{% include figure.liquid path=\"assets\u002fimg\u002fposts\u002f2025-08-16-rags\u002fskip_list.drawio.png\" class=\"img-fluid\" %}...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e\u003cdiv class=\"caption\"\u003e\n    Example of search path for a value (9) in a 5-layered skip list.\n\u003c\u002fdiv\u003e...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eThe topmost list contains only a few items, while the bottommost list contains all the items. Each item in a list points to the next item in the same list, and also to the next item in the lists below...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e[Small world graphs]({% post_url 2025-02-09-graph-properties %}#small-world-graphs) are graphs with two key properties: small mean shortest-paths and high clustering coefficients....","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e**Navigable graphs** are graphs in which we can find a path between any two nodes via a greedy strategy that chooses the neighbor closest according to a [distance function](#the-distance-measure)....","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eHNSW graphs are _indexing_ structures, i.e., every time we add a new item to our collection, we will update the graph. The graph is built in such a way that it allows us to quickly find the nearest ne...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e{% include figure.liquid path=\"assets\u002fimg\u002fposts\u002f2025-08-16-rags\u002fhnsw.drawio.png\" class=\"img-fluid\" %}...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e\u003cdiv class=\"caption\"\u003e\n    Example of search path for query item in a 3-layered HNSW graph.\n\u003c\u002fdiv\u003e...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eVector databases implement the above to efficiently store vector embeddings and find similar items...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eOne of their main applications relates to LLMs: **retrieval augmented generation** or RAG...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eTypically, when an LLM produces an output, it uses two sources of information: its _memory_ and the _query_...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eThe LLM's memory consists of large swaths of patterns learnt during training and stored in its weights...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eThe query is the user request, containing a context and potentially some additional information...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eVector databases provide a way to add a third source of information, allowing the LLM to efficiently retrieve relevant items from a corpus and leverage them in its answer...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003eThe idea of RAGs is quite simple. First we embed the corpus and store the embeddings into a vector database. Second, once the query is received, we embed the it and find its closest neighbors from the...","\u003cb\u003eHow do vector databases work?\u003c\u002fb\u003e\u003cbr\u003e- [My previous, pre-LLM write-up on this topic]({% post_url 2017-11-04-finding-similar-items %})\n- [Embeddings: What they are and why they matter](https:\u002f\u002fsimonwillison.net\u002f2023\u002fOct\u002f23\u002fembeddings\u002f)\n- ..."],"x":{"dtype":"f4","bdata":"tdZMP0+9VT+Zqms\u002fIY6CPx6\u002fhD9zwqE\u002fFKOIP+A0jz+HYHA\u002fCzZfP0DBYz+iS3Y\u002fnoFlP0r4bD8Z9Jo\u002f\u002fiChP9FTvj+4B6Q\u002fvxmgPz2ylz8vI4c\u002fTQdOP+mRVj+sbTA\u002fiVomP5ab1j\u002fu5lBASLlcQAZ5XEANWU1AgJpRQIRhYkBn\u002fVFAjl5RQMMkYUDc+WFA3zViQLSBT0CK7V5AQehRQM\u002f\u002fbUCxZU9AYqRRQO9mX0DMGVNApXlhQPwGYkBMdWBAElliQBhJYUCqflBAjApMQDMCBT8a0BA\u002ftfsKP93SCj\u002fOSfI+gP7tPt9h+D5ZJ3I\u002fCxJqP01EXz9H\u002fKs\u002fo+h9P6s+dj9X71E\u002fSil6PyUKTz\u002fbt2Q\u002f7OdlP9HuVj9ae3U\u002fcMVHPzi0VD9hPYQ\u002fuKECQK1+EUAUXBZArP4IQBI3FUDsUQxAkuf\u002fPzIsCECQ6AZAAUwCQKWv\u002fT+xYQdA2doLQPdMAEDud\u002f0\u002fx7oUQH1fEkDzYRVAgyDfPwXe5D8+tuI\u002fqUT2PwEh2T\u002f0Rr0\u002fRtKGP+78zz8cnBRAqTbyP\u002fsA+kBupA5BxT8PQUdFEEExkgZBRIQQQSL\u002fDkEYfhFByYkWQVKHFUGHiB1BJ+AWQXlBGEGSmxBBThkRQbkPFUHdmRNBQDsTQdaDFEFsbBFBiLkPQYQIVUCz+cdAqEreP38H7D+pOtM\u002f70baP0eU2T9zSeY\u002fNoH2P+268j+9\u002f88\u002fSVjNP8VOvD9r5yxBNg0uQW0kLUEPeq4\u002f79G2P8KvyT+\u002fewxBmjK5P3xL1T9B1eU\u002fWEzXP+INvz8CmuI\u002fF2HUPy492D9M+cM\u002f15i6P6aQ2j8kIKw\u002f9G+4P6Ga3UAi67k\u002fVpe9PzQarT\u002fwXL0\u002fWua1PwPuvj+QxWZABNSyPwt7sz\u002fvLq4\u002fes6wP9En9T9XpwI\u002fP0\u002fnPzplhD8tWvQ\u002fghnHQKAEyEDFd8ZAOnfGQFuOx0AyI8ZA7drQQDcF4UAETOZAdrPeQOrU6kBzYRlAgkvlQAQt2kC6otpAWSvaQHHM5EA4wOdAx\u002fI6QXw3PEGKcN1AEBncQOQN40A1V9xAsgzqQN5a3kDR3ddA+ETYQPI220BodN5AhvrrQKK\u002f7UAa0vBABcfuQBsp7kBVoOpAkazmQMGy7EBtofBAXQXtQCkfy0DtGsxAWibOQMdB1EAXhNVAAD7UQC8G10AnO9dApZ\u002fWQA+a1EAJjddAxaLWQHnly0A5AdBAz5jRQMqO0UAd1NFAmjzIQMZnyUD\u002f1cZAPjrJQLGMykCo28lAow0gQRlfIEGOwidBKcAwQRkpPUF61CxBX0oxQZJlHUF85TRBl5MlQcf9OEG1tjVBu\u002fE1Qa12H0Hh3R9BGi8uQQI1LkF9Cz5BrLUuQQnoPUE5OCBBl9IZQRHaF0HhwhlBSLkcQb7wGUEWARhBLWgYQdNDGUFV2hdB0v0bQXiBGEFpIx5BV0QcQaEgHEH+oRtBiUwbQah5HUFXbSFBOLggQbRYGkGDjRtBhzoiQTUpIEGsDx9BSCkhQaybIEFjAR9BB+wpQeGuHkGzeh5BvY8eQcbAHkFOBh5BeukdQaTkJEED5yZBPXolQTW+5ECMUClB\u002fJInQYqGKEHM8DJBWMwyQfWRPkGsKyxB\u002fMYkQUoyJUEBCiVBZWgjQVf8JEHaQiZBfmElQZBdLEENgitBQiYsQU7QK0E+mytBfqYrQX\u002fiK0H33CxBk8YrQa43JkEPDSVBWfIjQawqJ0HHgCRBcIsjQQ2IM0FSGiBBqY0iQdUIGUH+5xRBN4AXQVI4HUHH\u002fhdBn\u002fEXQbk0F0F1XxdBRD4ZQVs1HEFFVDRB+8AZQVWtFUGNZxdBAKAXQWHwF0GqhBdBT4w5QZ35N0HKczpBXGo6QZwPGkEAlRhBHCkUQQFsE0F9MBNBVrATQbkDEkFV7RJBMKMSQfcoE0GMiBJBh+ESQZZvEkFxdRJBs7oUQVHMQ0Fl7hNBbBwTQZQqEkEHKhFBA4cSQfPKEEFUzkVBUVZFQWGURkGfGEZB5y1GQcs9RkHCSjtB7y4YQZdfO0H8JztBCZ8YQQgGGEEIOxdBGrc4QVymOkEFYxlBqhkVQYSkO0HC+DpBmwAYQQFoFkG\u002fFDtB0cM5QbNQO0G+JjtBMcY7Qc14O0F9bjtBbxw8Qex2O0HNfUFBMQc8QQUwJUFhGSBBC30eQbTlHEF7BRxBIr8bQVAsJ0FRXyBBVuQmQSRbHEHLjzhBipc8QRUvP0FxZDlBWBU8QcadQUHKQRxBeFNCQbxPIUHkICFB1lZCQTumQkGPXiFBLOY+QRq5QEGoxUBB01BDQTIPQkG6CkJBzaQoQRkBJ0FvaylBFfsoQQXVKEHsUUJBAjkuQbh+LUGGMitBVxQhQeYTMkHAODNBB8c3QUhTHkFXXR5BOtsjQUupMUEOHCNBE58kQZZGPEFTDzdB7cA3QU03UkFjHitBewsnQaVyI0FEIC1BzQ4xQQl4KUFrVDNB7HQuQbyUKkFUcjFB0jNCQS0zS0GNv0pBCmhJQZy1PUEcDzxBCKYzQR7mOEHs+j9BcAYyQaz1PEGH9jNBQeYzQUTOM0H0yjlBwhI2QYfwNEG\u002fazRBp4IrQYRMK0HIRSxBzXMuQVbwKkESyzFBmTwqQWhrKUGABChBDVofQdyRJkG2vyZB7Q8lQbs7JUH6EiVBqN8kQQ29I0Gx6yRB3QolQRupJEHbAylB+1InQe7lJUE1fyhBq3ooQZY3K0GzGidBNkEnQRaZLUHjLTRBJA42QWTFMEE8bDFBohU2Qa6lK0H2BkJB\u002fHVBQW8cQUH\u002fsUFBy81CQdEpQEHAwxxBSolCQQN2IUEnGSFB92NCQYOtQkFhmiFBrE00QfkCNEHWiDRB0+U0QcCTLkHM3zNBZcs3QeuyL0HnXC9B51grQff8KEE30CdB6FBDQclYKUGJdy1BuO0tQVqILEGkIi5BL2E0QfU3NEGSijRB23szQYTmT0HOAC9B54A7QXYQJUFBOShBSt1BQasMQkHN2jNBt4I\u002fQdGlPUE8Nz5Btrw7QRC7RUEHtkJBzgpEQbDeQ0HPcUNBuW1GQbRvRUHyzzZBdWE2QSCsN0HM2ThBG40pQV8LSUHHCUlBXpY4QYcGOEE140FB\u002f+o+QbGePkH20D5BJ1M8QQJ5QUE94kFBWgI6QYbCOUHYezxB6+s7QS7\u002fO0EMnzxBpjg8QcnXPEFeHElBfhs8QcPdM0HBUzhBPrM8QcYSPEFY7zpBIRI9QbQcPEFeCEBBMro7Qaq1QEF4HENBFjc4QSa5NkHaTE9B45FBQSAuOEGtMChBKSdIQZu7RUHM0TtBxItBQXlKREEBJE5BpV9OQXU9TEG9Dk1BYcBMQW2vTkEq80xBkFBNQek\u002fT0EATU9BpI1EQemVP0EicEBBCqQ\u002fQSgePUHZJTxB96k\u002fQdLsRkFgIkRBLUBBQUnLPEGnlTlBc5ktQeYqQUEyPk9BnFVIQQkZRUGYV0VBGaZRQXMfUUFsq1FBBvdSQbfBUUHBG01BMxdSQQ7zUEG4DVBByzdRQQ2wUEETok9Bun5SQb3jT0Hh1VBBsYpTQSWgUUHmDVJBfJpVQeFSUUGFzFFB9zlVQVr6UUEdrFFBryJVQVXvUUHAPVFBfodXQVkxWEGSQVFBNPdRQazkV0GzxVdBUU9RQRcyU0EVHFhBT75XQS0GWEGTPU5BDltPQRF\u002fUEEdT09ByNpMQd3fTkE+zk5BmhEwQVZcNUHHY0lBtUpMQUUrGEG3uTJBWIZGQUfJQ0GIPU1B0FtIQS\u002ftSEHcx0hByc1OQXE0SUF0eUxB8R1IQYI9REHqoEhBSupIQRKgTkFkqjxBChJDQchJQUHC1DtBS\u002fE9QS4MT0ElI0ZBoTRJQfUEREH9qkNBYF9BQf6SPEEHxT1BjTwoQZUzJ0HfoCRB5dkmQQ0ZP0FPqkNBQlpFQVDEP0EgSUFBnSxBQVOSQ0EecRZBNXEfQcNJRUFjCkVB3q1EQXNFQ0Hq8UJBQ\u002fZCQROTQ0HzxBpB2AYZQRPML0HQDDFBpR4YQdyUG0EJETRB+iU4QYgYNUFSnjZBRyM4QbVlOEHChDhBTfA4QeKdOEHIYzlBpgc4Qb1gOkFbVDtBAww6QY3cN0GMqjdBd3o6QUoMQEHi1D9BZ5k2QaqyP0FyRy9Bxs03QaMBREEwLy1BIegiQfnqSUGnE0lBLzJIQQEESEGsMUdBqxJIQc6RSEEs\u002fEpB2MhLQWgkS0GeFk5BZd1LQXUVTUHCn09BxfNLQdUJT0HpSk5B5+P9QKk7TkGVVktBP\u002fZKQRhUTEG3Bk5BWUVNQSFpRUF\u002fakNB65FLQSJNTEGG9ExB8NtLQbtITEFt\u002f0xBvFRPQYwzT0GK2E1BgxpPQRA2T0GQEkxBVvNMQdesT0E6ok5ByqtOQXR0S0FOZ0xBVbdMQUujTUHiZEBBm1BNQRUDUEFJX0ZBOeRHQdgALkFtAk9BOj5JQXumR0HoD0pBFelJQR+ESEEesUtBv1xHQXAQR0GluEBBxDBOQejjR0ENm0VBzXJCQeZ8QUFYCEJB3rxCQb5oQkFSF0FBCmBBQSsyQEE7l0NBfF8\u002fQXYQQEEzvT9BLAtAQUDMQEEuqkJBwaZAQSwISEEE+j9BogFBQZ3NQUFRaUpBfvE\u002fQRqSBUE3gUJBkbtBQeyxQEFsREZBLHhFQVbMRUGVkkVBMK1GQT29PkFb0DxBLTE\u002fQfCJREFIB0VB2PNBQc8aRkFSBkZBFsNGQep2RkHwm0VB825GQYygRkEx+kFBj4Y\u002fQQacPkGmMz9BrxZFQZ7RR0GLlEhBlwpIQSdcSEFjSUdBqvlEQSSeRUFvVkNBSyZBQbb\u002fP0HNREFBiH9CQSz2QkHNj0tBFe9EQbroREHq6UlBjFP4QEPAS0G1o0pBe31GQaGWQ0H6hEVBm2ZFQdwCREGrc0ZBzddCQZG0QUHBD0FBDmNAQRU6QUEpEd9AWhnfQMcl4UDY0t5AAo7fQJfo3UBGLt9AJhHbQMwY3UAuAeFAMtnMQJgH0kAoxttAJW7eQPsV30D2l+BAQh3ZQAM620DOB+BAkvjgQG2H40Cv6OBA7BzeQNOh3kDDKN5Ab9ndQKvq0UAChNRA1kLWQJ7H00DXvxdBArTVQKXr1kAUsNpALeLQQIbCyEA808pAgT7LQGV\u002fyUA6J8tAtLbJQB0myECvuMlAJ0jIQMikyEC3t85ADRnVQGDQ3ECu0txA9XzYQLnl1EDOndFAFJvYQLb81EClKNpAfyfaQE5120DOBddAB5vVQCsQ0kAz\u002fdlAM1zXQNSl2ECJyN1A+7bdQHdz1kD0Cs9A4g7aQF4C7UDFGQ1BpD0LQfitzkCVMdRAs6TQQFEiz0Bj481ANAvRQAlC20A\u002fY95A1rfeQNjo1kDit9hA24nbQAwd1EC3Mc9ARCLSQG3R0UA06NFA7vXaQJrx1UDUXdhAy8DPQGsAyUDVorRAvke+QJkIwUCStPVAH3XEQKzMwkAZasBAMbXDQP8vw0AXELRAnPu1QAN9s0B387VAC5G0QPv0wEABesFAyQ+6QD4y9kBtC8pAkk7CQPDot0DeQs1Am5\u002fHQBUNukCz7ttAYs3bQLvO3EAfeuBAnwzfQNfj5UABnbhAB4e5QMldukAb8cNAchm1QJXvtEB8mXFAiWW2QI05tUCLD61AwcYWQT8VpEA7LbRAQ4+vQPZzRECGjRZB9I+sQD8LrUDlBxdBBR6qQI+1sECCBRdBUsOyQJLTtECnnLZA\u002fpezQDHAtEAhpLJAGlK4QMKeuEDeUbhAd0K2QA+ot0DpQdRAzenVQBc25kCDcHlAPoLQQGF5tECDxrJA1AndQHnkzkDARtFAT+TSQHQbz0D98BZB38nTQDT13ED0q9xANv\u002fdQICE6ECYgd5AInTgQG4O30Db19NAHZfSQARu00Cs79JAuCHSQJg\u002f1ECx8dNAJ2\u002fSQJXbykDoDMtArSjMQBiwyUD6U9tAadrCQAnhIkE4P8lANNTgQKVp3kBLbM1A0vbJQEnY00DblM1A5FDLQA3H4EAmOeFA8rrhQI1E5EDCSONARGbkQJNU5UDUKixBQsDgQP1j4kD4CORAjlbhQCuq10DvC91Al0PfQM878kAYhPBA\u002fp76QNNM+UCNcehAUMLnQFsW6UCr6eJAuYLnQO1n5kDep+NAns3nQNF65kAwI81AEfLXQF3X2UC8rNpAS\u002fnTQFtt2kB0571AEJbtQCVg9EBLeOpAcHfYQPCP+j8jttxAkG7rQNq+6kBL+BdBAWbNQJWqxUBYBsNAkqHAQN9KwUBW0cBAqPvfQChf9EAIj+1AL2TNQDXME0HQ1hJB+eAXQfOvFkE6JhJBJ3cRQSP9D0H4g0BBoZcNQSehD0E9WBJBAVATQTsoF0GlmRVBu\u002fwSQSVuMEH8wTVBS60MQX59C0EK+wtB54oLQVXtCkHcFwxByCAMQaiJCkEhxgpBJsEKQdL\u002fDkEdVw5BKzgJQdYbCEEyngpBmGIIQcegCUGTqwhB+S8IQZmfDEFFyw1BbWoQQa\u002fpD0GA8BBBBykRQRJYMEFEcDdBg0cQQZ5wCUEFfQZBukAGQb0Iy0D55wZBRMYIQcPwCEGzAQhBDGUHQe6nB0FdjwdByCsHQfTNz0DdrxdBNTUWQaYICkHgQgpBb2gHQeyp90APtQdBTOQHQRPBvkDJJ75AWI+8QCYfv0Dudr5AtqSwQKkhs0BPkbFAAX6xQLEDuED0M7ZAYIi0QI8QtkAQ6rFA4fdDQUI3s0DS3LVAJgW2QJUatUDX17dAgciwQIYjtUBgmq9AXeltQG5pY0DiSLlA2dUWQUBCuUD7vrdAnaa8QAcPvEDtUr9Auh+zQFIwsUARzLZAA32+QNfmvUD0YbNA8Ci4QFnWtEBBqbhASIUWQVPhtUAxGLVAX562QMhMukARJLdAhFe+QD08s0DSpLhAH6K5QDvx1kB1F81Awsa8QJ8N0ECZWMNADt8zQaA8EUH\u002fPg1BmxMNQa+m+0BZvwFBWVzLQOCoAEFA\u002fQJByDoEQX9v9UBzF\u002fZAfewEQdMay0CeA+lA2v39QFXn\u002fUAij\u002f9AclUAQfD29kAyMgFBulMBQXZhAEHi\u002fgBBmJz4QA8N9UC4cfdANxYCQej5EEH3fAhB1\u002fcKQVSoZkCx\u002fwVBi34GQSKVBkEDawZBZzYRQau5+EAKngJBDWECQbrgBEGh77VAa9CxQA4RtUBMg7VAjGixQCSFD0FqshBBowEaQWvWD0HA6D1BSUAbQdIsGEHx3hpBwlcwQbg9QEFZmkFBZPw3QTs2GEG3AxxBWOkLQY9YEEGCIh1BHAMcQcD\u002fEUFQJQ9BQMwOQfCECUE="},"y":{"dtype":"f4","bdata":"Ln4cQEIlE0B2bR5AVyEfQAEiFUCVMx5At8cRQLTGFkCYyBhAPyUaQBd6HUAYGyJAUrgeQAspIED\u002fOyBAARwiQB8BG0B7Sx5AJi8fQDqbG0CPLhdApR0QQLtiDkC\u002fzxJAlsoVQI1EIEDc6BxAFHgyQOCLL0BTXBpApLceQEm9KkDkIh5AcfgdQBc1NEDh6zNAGcU2QLlkIUBh1jVA7\u002fcdQFArK0ASNRtAtOQdQM09K0DtYB9AJgwzQKFzNECLiDVAPcQ1QFj0MkDq5xxALUoeQB+AF0A1TRtAgp0XQHjMFkBQphdAcoMXQF1mGkDpHQZASBsNQGZnA0AEFoU\u002f8WD0P4JW\u002fz8uOhFAMxQEQGaiCkBQ7A9A56IHQA9ZC0BTN\u002fU\u002fwl8EQHG6A0CRQAtAo73OP4pNxD9XisE\u002fDLm5PxJLxj+Q6MU\u002fGR3LPyHmzD83ess\u002fnk3QP4FoxT+mrcE\u002fGNHPPymdyz+BQsk\u002fgjfGPzB+yD+Dr8I\u002fsAfAPyLZyj+Gq8U\u002fbe7EP\u002fyA0T99758\u002fN2EIQCIR1D8KmsQ\u002fM6fQP+y7m0DwvZtAcwacQCPpl0Bv6YhAuEeYQJ7qmkDe85xA2JOkQE8ypEBhI9VA3tumQB7RpUDKypdAzzGaQIyKo0D9KJdAzZaXQLfemEC6aZpAz0acQGHfmT\u002f2leg\u002fT4qJP89Pgz87kp0\u002fWuShP4G5iT+AUZs\u002fOtnLPzpKzz8iMno\u002fvuJQP9xkYT9lj0lAwHdMQMJoSkALrFU\u002f0qhePwNdhT\u002f4jq4\u002fpcdEPyoadj+Y2YU\u002f2jJyP5+iUD\u002f++UY\u002fgJtNP1TtVD\u002fw118\u002f\u002faaFP+GTZD8Uwls\u002fBexOP5ZJhj\u002fp72w\u002fbvCGP4a5PT9yYVE\u002fyP5IP3SWTz\u002f+Hx1ACfxSP9StWD\u002frbV0\u002fJupUP\u002fGYQz8YTxJA53RQP\u002fai3D8ZN1c\u002fuwsRQKLIEkDYqRJA20sRQM7lEUA21wtArJgkQNooFEBADQ1AHoEeQPD0BUADkpc\u002fzsUNQAjnJkC7zCNAdnAlQH9PEkD6AgpArrG4P6mzrT9GgSZAKlIjQBLrFkBMKCBAPwoEQETmGUDl7CRAQBchQDy1H0CEfR5A2sUGQGq\u002fCUCUbQBALqgDQEBcA0ApwgVAf4YNQGxeBUCCJANA6QkHQObQDUDsnBZAgoMOQBOKLEB49zFA2SQzQGTQMUBQ0jNAhis1QOqrNEB\u002fgDZAn+w0QOddHEAAsCVARqApQBQSKkDfRSVANA4cQDotG0CDSiBAYqAlQBO0JUB5XidAzq4GQSCzB0GU8PBAEIn9QG8I+0DzAPxAE9n+QNPkBUH5xgNBDevdQIpxA0GMegVBuKf2QMRHBUHTrQpB0Zn7QKji+EC7DvVAcLP5QM50+UCeIQZBnnIMQWdkDUGPMgxBIsQJQbVfDEHv+Q1BLUQNQYSKCkHUHA1BpXkFQWc0DUEGGQdBg3cGQXMyCkEQBgVBCmgEQZ8RBkGk8ItAthH4QNlbC0F+GQNBidr8QEACDEHCRhFBDu8QQSpzDEEGEhFBBv4PQQYkEkFAbBJB8XYRQT5ZEkHeVxJBceMRQQM+\u002fEC0nwJBdAoAQQ0EA0HXwexAS0rdQLJMAkFulP5AnUcAQV7j9UAbNQJBBrflQDMnCEHrsAlBOloJQc+\u002fCkFddA1B3msLQW0KF0HMbRhB9bQXQT\u002f3F0Fb9BdBUdgXQcFxF0EBfxZB290XQQ+T20CbxdBANDDTQCge70CPHfdAoSMFQU1m5kByOwNB\u002ftoBQVQ1\u002fECnYORAytbiQIuj1UBPHd5AgSvYQAqj4ECcaNdAW9fXQNOQ3ECAkfVA5anUQFnB3kCwOcdANZLXQIuY2kBL7tZAknSpQP1VsUDslaxAuUSrQDrK5EDgPgBBIovyQDR650Dd1fBASw3zQEPa6UAJW+5Aw6DuQKnW8EA6j+9AeqHvQDpk8EBAUupAQE3uQK8a8kDzC+lAMRTsQFQU6UAZ4uZAn+noQBDN60Dv1u1AQzgEQYxvBEHaGwRBa8cEQR+lBEGvxxhBeGz+QLBvGUFgGRlBUaD\u002fQCpn+0Cv0\u002ftAl\u002fQWQf17GUG+TPxA4+H1QBdfGkHajxhBuBsAQQ\u002f5+EAAABlBHHQXQUBMGUFKbhlBVSkJQUXXGEGHgRdBa8QGQZqfGUHihv5AQC8YQUB69UCMS9JA5pnTQKji1UAmT9lAlbLXQHyv4UClj95AZLTkQOSR0UAlNq1AWja0QIYhuUDAmq5AosuxQCqyvkDv+gpBree+QJepi0C1\u002f4pA4mO\u002fQK\u002fKv0Co0YtA5NT9QCrp+kBihvxAmb33QBdF+0DIX\u002ftA6ofzQHFtAkGgWgJBOIwBQW1TBEFwSv1AY9cMQfNZDEGugQlB\u002fd8GQUVY3UAFB+BAQzrlQOxX20D+MdVAksTjQNXZ20AtGNZAN2HSQD3z50BPXuNAMrLhQC8C70BbudNAXgTPQBd72kA4seNAtlrZQCz40EDWbtxAr7vjQKFm3UD82tlAidPiQPMz5ECZY+NAlN3eQLv36UB\u002fjLdA5o7iQEzt4UArcuNAsqXjQO\u002f\u002f5EDZ3ORAqS\u002fwQBjG4UAhYudARI3oQIIh7UAPSOxAxZHhQGac4kAyAeBAAQfdQAVAyUB3wOFA4MjEQEPJyUBVO8tAH6nQQC4uy0AqscZAglTKQDMtxUAHi8RAnt\u002fMQLICwEBEgcRA0rjOQPudzUByAM9A4c\u002fLQOJdy0D\u002fcMpAeazPQBBayECo4wNBcYACQbD2CUFkngNBDOz9QMbADkE+Pg5BGn7+QO1aBkHaXftABar8QNHIAUH3G\u002f9AfDwAQUdMvEArnwpBjt+9QBVvikDp5IpAAwi+QPYLwEDVYIpAGHj8QNi9+EAjNflAK\u002ff4QJXyAEGTJPBAk3D0QPPU9UBfLQ5BTWMIQTPZA0HGdgRBB40AQeewAkFxZbhA39njQJwY4UBX0uVAkIb1QOV89kA+pflAOKL2QCNQAkEfkANB1ZwAQWR30UBT4ABB3namQPR2rUBtZXZAZZmxQPY9tUDofrJAvuGlQOsp60CwO8ZAPuHCQHx0yUAf1sZABczrQKZ960Dfj6NAJ\u002fWZQNc6jkCwXI1AnxrqQJEi9UAT1u9AujWPQJQRj0DS9IlATRGHQMZBiEB8685AC6SAQOvBiUAd5FBAoPlUQHjeY0A9ZHJAdGVyQH5TeUAeg3NAJNGZQMuAkEBVHTtA00tgQJEdS0COqmZAw\u002fltQMR3dUAjtXFA5mObQFVRnUDCWZNApmmdQK6Qj0AtzUlAakqnQJs\u002fpECpGuZA6f+kQFuJpEClCMFAmx6gQL1fnEAqX4FAX4fIQBNgj0BLueJAqZHlQBBv6UBlU+ZAwizrQLqK6kDa6epAJ3TnQASi6kAYEuxACoLzQH0vmkCBqqBAfXGjQKjJpEDMtspAcRmjQAmsnUAhq59A4Q+LQPuin0CNgqBAJSyrQGbfrECyHeVAdpGfQD6rp0CkIadAxTDzQIqm8kDpHgFBWx31QM478EA4wAFBxIECQQH8A0Ho+QFBs1sDQQSgA0G0BwNBhEgDQb2JBEHyoOhAGZ\u002fuQAIb+kAY9PBALpfyQG\u002fQ70C3p\u002fJAxrL0QM8k7UCY2u9A3L7zQHPG7kBa6wBBdir8QJ7r+0Dg\u002fgFBFG7+QAqP\u002fEBgM\u002f1Awq4CQeh+AUHFQfxA68D+QLWu\u002fUBYswNBAW0EQeCiA0FjggNBumsDQTOwoEDW6KBAZEOnQCjjpUDMKaRAqk6iQMinzEBmsaRAsf9vQK\u002fRmEDkPaBAtaqjQHTVoUATeadACZmgQHpNoEBTtqNAHfuzQK2JokBQ2qxA5kuvQHx4oEB9y6VAGbuoQCghqEAkV6JAgDqhQA90oEBAWiNAbVQTQLeBKUC9XXtAKBmDQHL5g0CaqolAiAC4QPppv0DGlr1AZfi\u002fQDxskkBIIIFA1GhlQH61lkAsQItAwzaMQLnVeECCG6JAB7ikQEm1eUB8QX1AWRCAQJ3mhUDIcYJAo+6BQOfQgEDlp9JAjc\u002fTQGkVkUDv2Y9AiI8yQJHoPEDzZpBAK7SMQGaonkB2hdVAT6zUQOOo1UBajttAEKrZQJ0l1kCj+9VAlX\u002fUQCdX3UCw8t5AZfDYQJdWsUAHUrtAXDjWQC6d4ECjteBAGEfyQD+M9UAM1epAZZ7UQA2LLEDpVEtAKtWpQKJG1T4gLDFAmWgvQGVrMUBPjitAGLI8QBqINkDgqSpAZ2saQCNNE0B24RdAYYUUQGuIGUBxpwxAUHkTQDFmGkDUNSJADegGP61cJ0C3pDJArbU5QBCENEC\u002f2yxABGowQMaDH0Bx1iBAKvExQObpHUDIBhZA9aM2QEMxN0BUyjJAtespQCahL0AMywxAUqMJQEqQCECMEQtAnhMSQAyJCkDaHAZAfsoaQC\u002fsF0B1UidAVc8nQAAPO0CkUz1AdQ42QKygKkD2X8g\u002foZrLPzuORkA5QgVAFh7SPwOVrD8N4NQ\u002ffg\u002fXPy4LzT8+QfA\u002fqU2sPzAtzj8CP5k\u002f3rH\u002fP+BZtT9PjqA\u002fyq+nP\u002fUipj8mTp0\u002fmVNvP\u002fo5wj+SDbw\u002fyejBPw1goj+cg9E\u002f3cmiP0Xtnj\u002fhv5w\u002fA+OdP2rCoj8vKaw\u002f25WjP9XiE0CvuLk\u002fz0zuPyaS\u002fj8EQuA\u002fTjuVPxjRwD8SwQNAYYL9Pxha8D8ZCQM\u002fBFUDPx5a9j6DKPY+jt3UPk76Oj9v+CE\u002fsukmPyzzAz+CEZ0\u002fx5UrP5l59D5Y5N8+glzPPp1T2j7kNv0+0zXsPisu1T6rQ6A\u002f9UOTPxXyiz8Jv48\u002fsnbvPtS7zz5FHsY+jhLHPqQy6D6H9AQ\u002fkRISP23GOD8Pjxw\u002flNBFP5Ur8j9H7C5AIL0yQJ7EOEDSvgBA4cALQG7tCEBmfQVAJLz0P15qBECB0gBABkwQQAvHFEA1tRJAnycPQIZ7G0DSZSNANkgXQGTWJkAtyC9AvwUuQEDnKEDUnZk\u002fyP6dPxGEsz8Bz5w\u002fmLmZP3fYiT82PoI\u002f3ANKP\u002ftWST9iPlc\u002fXiOHPzARlT\u002fJbYY\u002fdZ57P0IjdD84p4o\u002fOX2+PxVFqj+fQJQ\u002fNnCRP7LPlD\u002fNmpA\u002fyW93P0wfgT8ebpk\u002fdCqAP2JZBEEpa9VAsMHNQKlM10BObDFAo4QEQWKiBEGEDgRB8wn8QIi2BkGi+QVBNxcGQVEoBkG33gVBiy4GQTv3BkGKNgZByy4HQV6SBkFlwAFB3YztQFEFAUGJjABBa83\u002fQAlp80C0eP1AD1IFQUfbBUFZkgVBLF4FQTpUBkE5bQZBg5AGQalrA0HYOQVB15cBQZgkAkEI5dBA\u002fy0BQWMWA0GSJQFBrJX\u002fQO59+0ATSuxA6KXvQFC8AEGevf9AM9QAQSQJAkG9+wFBnkUBQbl2AUGS1QFBV5oAQfHuAkFiHwBB6moBQQ3J40ByBv5A0dj8QFJc+EDtMP1AC0YBQfgS5EAEbNZAv5wDQdS5BkGZzPE+F\u002fQXPuGnfz1c4gQ\u002ffJmYPN51Qz0yE7E9AkNEPWyC5TzG0g8\u002fyahSP1F6ED8kHAY\u002foUb8PrlK9j1R+go+vmz8PrnMDD8l+Uk\u002f7ffgPaPQrj6Gnkc\u002fNnYjP2MKCT+LUAQ\u002fC5ITP1NBBT8euPs++VEHP0q18D64uQw\u002fsnzpPhwaAz\u002fxcDE\u002fCEURPwdFPz9tWvY\u002fym\u002frPtz86D5+My0\u002fCTIsQNTTVT\u002fjhgM\u002fP8MjP++g7j99oitAtTxGPxjIIj8C9SxAvP5AP4UKJj+cny1APRghP4zPAT9I+ro++PjXPi5SuT7bqAk\u002ftvuoPmvp5j4Fy7s+i5y9PrdQrz5VbVI\u002fwct4Pws5AD\u002fGSOo\u002fhdpMPw2SvT5K4w8\u002fJbfrQBX1t0BsMa9AmnixQHufskBRAy9A0VnJQNGPskBJ7K5AlA2wQKKatUBdHblANT29QOmIwUBG16VAjpSkQIJPpUDiPKVAeVOjQGL5oUBA9aFAVRukQEH1skCO2a5ApgG2QHIatUCtHctAgIOyQGBcAUHCe7NAGEjCQKKXv0CGQ7ZAdm21QOEYsEDfG7hAivS1QPYZtECXVrdAEcC\u002fQMmDwkCMt8NAlw7CQPVtw0Cm+UlAPN+xQKktrkCEmaxAhr+6QECct0B3l7VAP7W4QIlaykAwvspA8yPJQCaLy0Df7cJAZry\u002fQLwhwEC05b5ANPzCQPNFwkAOHMNAluXCQPrlwkA95LRABs60QNcvsUD0\u002fahAyoSqQIWhrEDbvalAtcCkQAhXnUAoZ6BAqfGoQFnDzz8ir6VAGeKVQOD1kkBCmTBAzT+1QIMIsEBva7JA1E+yQGbQsEDhWLBAK8WiQDnenEByoZ1AQxy3QJyzrz91oqk\u002fS+wvQJEIKEBea6Y\u002fPx+VPwg0eD\u002fC5+FAxHSjP+ZbpT+aMJw\u002fiM6rP2rXLECtExRARVmyPw5kqUCeq6RA45qsPyjQTT\u002flRz0\u002fG3I9PzniMz\u002fjJ0A\u002f\u002fE1MP4IxID+z\u002fEI\u002f9tZFP40OYj\u002fkWWE\u002f9PhnP\u002fyIhz8g1zY\u002f8JsdPza5Hz+K5Rg\u002fgt9\u002fPyraTD+B7aE\u002f4PSKP6fhej\u002f7C5Q\u002fLbqNPzdvwUC68KFAY17BP94Gpj819Y0\u002fUnuSP1cobD8+NI0\u002fDHmUP3yQhz\u002fZPI0\u002fAduKP1r1hz+JNYQ\u002fvXyNP2GhXj8xoC9Am0goQBfXkD\u002fy+uc\u002fCmmlP4stlT9Xg48\u002fLXeSP1NMUcCgAFLA76RTwLgoUMAzW1PAjyd8wPKJdcBkWXzAUop8wLMzbMB6y3TASW93wGk9dsBAlHrAMxWFQBAkesBCgnbAuD1vwBnod8A07G3AtVx\u002fwOLHfsAQdYHAzrkiQDgqMUCgGmzAQi8rQLCyXMBe\u002flTApU9XwLfaZMDbt03AHi13wADjfMBhcGnAKc5OwHFBUMC\u002f52HA8h5twLEAdMAQoGLAxUUrQKdAVMDHjlfA9btQwNPhSsCJJlHA+ktOwC4YdcD2t0TABTpIwJdh8T9X3v0\u002forw\u002fwMK78D8L9EnAJufNQDC5n0Cf051A1deeQIq\u002fokBtt59AqCajQPeXoUARUJ5Ad82eQM3jm0Bz3p5AeBmeQPgduECOvsBA53KhQMWGp0A8N6xAGlGmQA+dqECFx7dAwq+6QLpquUBot7hAPOigQFucm0DFNp1AMfeUQFMQnkDZjpJAH92JQPZJH0CWHYhAu92FQBxmiEDHJI1AOQqePywymUBWYpRAUgKVQAIBk0AP39o+usv8Pqu+0T65kb8+axjjPmDVnEA0ZKBARPPGQHz2oUDzmjhABABGQJNVMUDJcTpAL4uOQIjYLkBmpi5ASZg8QD5AMkA5+TlA9vCeQJJ2p0BRs7lAfrS5QMaBqEBm8aVAhC2kQIeEmkA="},"type":"scatter"}],                        {"template":{"data":{"barpolar":[{"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scattermap":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermap"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"white","showlakes":true,"showland":true,"subunitcolor":"#C8D4E3"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"white","polar":{"angularaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""},"bgcolor":"white","radialaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"yaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"zaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"baxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"bgcolor":"white","caxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2}}},"xaxis":{"showgrid":false,"zeroline":false,"showline":true,"ticks":"","showticklabels":false,"linecolor":"black","linewidth":1,"title":{"text":"UMAP 1"}},"yaxis":{"showgrid":false,"zeroline":false,"showline":true,"ticks":"","showticklabels":false,"linecolor":"black","linewidth":1,"title":{"text":"UMAP 2"}},"margin":{"l":0,"r":0,"t":0,"b":0},"colorway":["#4C78A8","#F58518","#E45756","#72B7B2","#54A24B","#EECA3B","#B279A2","#FF9DA6","#9D755D","#BAB0AC"],"showlegend":false},                        {"responsive": true}                    ).then(function(){

var gd = document.getElementById('umap_plot');
if(gd) {
  gd.on('plotly_click', function(data) {
    var pt = data.points[0];
    var url = pt.customdata;
    if(url) { window.open(url, '_blank'); }
  });
}

                        })                };            </script>        </div>
</body>
</html>
