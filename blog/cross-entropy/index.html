<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Cross-entropy. Intuition and applications. | Héctor Climente-González </title> <meta name="author" content="Héctor Climente-González"> <meta name="description" content="The secret sauce of machine learning"> <meta name="keywords" content="machine-learning, genetics, pharma, drug-discovery, climente, climente-gonzalez"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%AA%A2&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hclimente.github.io/blog/cross-entropy/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Cross-entropy. Intuition and applications.",
            "description": "The secret sauce of machine learning",
            "published": "August 01, 2025",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Héctor</span> Climente-González </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Cross-entropy. Intuition and applications.</h1> <p>The secret sauce of machine learning</p> </d-title> <d-article> <p>In pop science, <strong>entropy</strong> is considered a measure of <em>disorder</em>: a system has high entropy when it is disordered (e.g., my college bedroom), and low entropy when it is ordered (e.g., a chocolate box). This meaning probably has its roots in thermodynamics, where my college bedroom was evidence of the universe getting ever closer to its <a href="https://en.wikipedia.org/wiki/Heat_death_of_the_universe" rel="external nofollow noopener" target="_blank">heat death</a>.</p> <p>But for us, entropy is not (only) about messy bedrooms, but about messy <em>data</em>. That’s why I will focus on <em>Shannon’s</em> entropy \(H(P)\), which is a property of a probability distribution \(P\). For a discrete random variable:</p> \[H(P) = \sum_x P(x) \log_2 \frac{1}{P(x)}.\] <p>As stated, using the binary logarithm, entropy is measured in <a href="https://en.wikipedia.org/wiki/Bit" rel="external nofollow noopener" target="_blank">bits</a>; when using the natural logarithm instead, the unit of measure is nats. From here on, \(\log\) means \(\log_2\).</p> <h1 id="what-does-entropy-really-mean">What does entropy <em>really</em> mean?</h1> <p>In a nutshell, entropy is the average surprise we’ll experience when observing a realization of \(P\): if an outcome is rare (\(P(x)\) is small), observing it should be quite surprising (\(\log \frac{1}{P(x)}\) is large); if it is very common, the surprise should be low.</p> <p>A more tangible interpretation of entropy links it to the <em>encoding</em> of a message. Imagine we want to encode the outcome of a probability distribution. We observe an outcome and want to unambiguously communicate it to a friend. For instance, let’s say the weather in my city follows the following probability distribution:</p> <table> <thead> <tr> <th>Weather</th> <th>Probability</th> </tr> </thead> <tbody> <tr> <td>Cloudy</td> <td>0.5</td> </tr> <tr> <td>Rainy</td> <td>0.4</td> </tr> <tr> <td>Sunny</td> <td>0.1</td> </tr> </tbody> </table> <p>(Yes, I live in London.)</p> <blockquote> <p>For easier computations, let’s assume that probabilities remain independent and constant over time. Which, again, isn’t too far from my reality. More formally, the outcomes are independent and identically distributed.</p> </blockquote> <p>Every morning, I look out the window exactly at 9am, and send my friend the weather report. Our first instinct is probably to just text them “cloudy”, “rainy” or “sunny” as appropriate. If we encode these strings in <a href="https://en.wikipedia.org/wiki/ASCII" rel="external nofollow noopener" target="_blank">ASCII</a>:</p> <table> <thead> <tr> <th>Weather</th> <th>Probability</th> <th>Codeword</th> <th>Codeword length</th> </tr> </thead> <tbody> <tr> <td>Cloudy</td> <td>0.5</td> <td>011000110110110001101111011101010110010001111001</td> <td>48</td> </tr> <tr> <td>Rainy</td> <td>0.4</td> <td>0111001001100001011010010110111001111001</td> <td>40</td> </tr> <tr> <td>Sunny</td> <td>0.1</td> <td>0111001101110101011011100110111001111001</td> <td>40</td> </tr> </tbody> </table> <p>In the long term, the average message will take \(0.5 \times 48 + 0.4 \times 40 + 0.1 \times 40 = 44\) bits. Not a big deal I guess… But we can do much better! <em>Why waste time say lot word when few word do trick?</em> For instance, we could associate each string to an integer or an emoji (8 bits). But we can do even better than that, we can generate our own <em>codewords</em>. To this end, we need to generate a <a href="https://en.wikipedia.org/wiki/Prefix_code" rel="external nofollow noopener" target="_blank">prefix code</a>, i.e., one in which no codeword can prefix another.</p> <p>The <a href="https://en.wikipedia.org/wiki/Huffman_coding" rel="external nofollow noopener" target="_blank">Huffman coding</a> is a common solution to this problem which significantly shortens our average message by leveraging our knowledge of \(P\):</p> <table> <thead> <tr> <th>Weather</th> <th>Probability</th> <th>Codeword</th> <th>Codeword length</th> </tr> </thead> <tbody> <tr> <td>Cloudy</td> <td>0.5</td> <td>0</td> <td>1</td> </tr> <tr> <td>Rainy</td> <td>0.4</td> <td>10</td> <td>2</td> </tr> <tr> <td>Sunny</td> <td>0.1</td> <td>11</td> <td>2</td> </tr> </tbody> </table> <details><summary><strong>Huffman coding</strong></summary> <p><strong>Huffman coding</strong> builds an optimal prefix code for a known distribution. Here’s how it works for our weather example:</p> <ol> <li> <p><strong>Start with probabilities:</strong></p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>Cloudy: 0.5
Rainy:  0.4
Sunny:  0.1
</code></pre></div> </div> </li> <li> <p><strong>Merge lowest pairs:</strong> Combine Sunny (0.1) and Rainy (0.4) → node with weight 0.5. Then combine with Cloudy (0.5) → final tree.</p> </li> <li> <p><strong>Assign bits:</strong> Traverse tree, assigning 0/1 at each split.</p> <table> <thead> <tr> <th>Symbol</th> <th>Codeword</th> </tr> </thead> <tbody> <tr> <td>Cloudy</td> <td>0</td> </tr> <tr> <td>Rainy</td> <td>10</td> </tr> <tr> <td>Sunny</td> <td>11</td> </tr> </tbody> </table> </li> <li> <p><strong>Average length:</strong> \(0.5 \times 1 + 0.4 \times 2 + 0.1 \times 2 = 1.5 \text{ bits}\)</p> </li> </ol> <p>Huffman coding guarantees the shortest average length for any prefix code based on the true distribution.</p> </details> <p>This is much better: we have gone from 44 bits to \(0.5 \times 1 + 0.4 \times 2 + 0.1 \times 2 = 1.5\) bits on average. Of course, for this to be possible, we need to have access to the true weather distribution. Otherwise, our bit allocation won’t be optimal or we might converge to a suboptimal code.</p> <p>Are we satisfied yet? Not quite. Despite Huffman being optimal, our message length will be the same on rainy days and on sunny days. However, rainy days are 4 times more common! The core problem is that our messages have an <em>integer</em> length, and we would need <em>fractional</em> lengths to do better. But we can do better if we make some compromises. Imagine we only want to batch-send the weather report every 10 days. Then, there are \(3^{10}\) possible sequences of 10 days. The most likely one is a streak of ten cloudy days, which occurs with probability \(0.5^{10}\). Consequently, the Huffman coding assigns a much shorter codeword to this string than to the most unlikely string, a streak of ten sunny days:</p> <table> <thead> <tr> <th>10-day weather</th> <th>Probability</th> <th>Codeword (length)</th> <th>Codeword length</th> </tr> </thead> <tbody> <tr> <td>CCCCCCCCCC</td> <td>9.77e-04</td> <td>0111001010</td> <td>10</td> </tr> <tr> <td>CCCCCCCCCR</td> <td>7.81e-04</td> <td>0000101101</td> <td>10</td> </tr> <tr> <td>CCCCCCCCRC</td> <td>7.81e-04</td> <td>0000101110</td> <td>10</td> </tr> <tr> <td>…</td> <td>…</td> <td>…</td> <td>…</td> </tr> <tr> <td>SSSSSSSSSR</td> <td>4.00e-10</td> <td>0111001001100000001100111110100</td> <td>31</td> </tr> <tr> <td>RSSSSSSSSS</td> <td>4.00e-10</td> <td>11000100010100011010000101101111</td> <td>32</td> </tr> <tr> <td>SSSSSSSSSS</td> <td>1.00e-10</td> <td>11000100010100011010000101101110</td> <td>32</td> </tr> </tbody> </table> <p>The average length of this code is 13.64 bits, or 1.364 bits per day. Batching outcomes together allows us to spend only <em>fractions</em> of a bit. And it’s easy to see how, if we kept batching more and more days together, each single day would require less and less bits.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/python/2025-07-14-cross-entropy/img/entropy-batch_size_vs_avg_bits_per_day.webp" sizes="95vw"></source> <img src="/assets/python/2025-07-14-cross-entropy/img/entropy-batch_size_vs_avg_bits_per_day.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>And this brings us to the key point: entropy represents the lower bound for the average message length required to <em>optimally</em> encode each outcome of a random process. Even with the best encoding we can come up with and incredibly large batches, we can’t compress the message below the entropy limit. In the case of our distribution:</p> \[H(P) = 0.5 \times \log \frac 1 {0.5} + 0.4 \times \log \frac 1 {0.4} + 0.1 \times \log \frac 1 {0.1} = 1.361 \text{ bits}\] <p>The Huffman encoding was doing pretty well after all!</p> <details><summary><strong>But <em>why</em> logarithms?</strong></summary> <p>All this is fine, but a lingering question remains: what’s the logarithm of the probability doing there? Why aren’t we using any other transformation of the probability?</p> <p>Imagine the space of all possible codewords of a prefix code. If we decide to use the codeword “0”, every other codeword needs to start by “1”; that choice cost us half of all possible codewords. Hence, if we are going to spend that precious codeword into one outcome, it better happen at least half the time. Note that \(- \log 0.5 = 1\). Similarly, a codeword like “00” still allows for words prefixed by “01”, “10” and “11”; it cost us only one fourth of the space. Note that \(- \log 0.25 = 2\).</p> <p>\(- \log P(x)\) gives us the optimal length of an outcome’s codeword.</p> </details> <h1 id="cross-entropy">Cross-entropy</h1> <p>We just saw how knowing the underlying probability distribution gave us an edge in encoding the outcomes efficiently. However, here in the real world we rarely have access to <em>true</em> probability distributions, if such a thing even exists. At most, we have access to our best guess of what the true probability distribution is. And these guesses are rarely completely correct.</p> <p>For instance, we rely on very complex models to accurately predict the weather. But let’s leave those aside, and use the simple model (\(Q_\text{Barcelona}\)) and associated Huffman code I developed for Barcelona’s weather:</p> <table> <thead> <tr> <th>Weather</th> <th>Probability</th> <th>Codeword</th> <th>Codeword length</th> </tr> </thead> <tbody> <tr> <td>Cloudy</td> <td>0.2</td> <td>11</td> <td>2</td> </tr> <tr> <td>Rainy</td> <td>0.1</td> <td>10</td> <td>2</td> </tr> <tr> <td>Sunny</td> <td>0.7</td> <td>0</td> <td>1</td> </tr> </tbody> </table> <p>As you can imagine, after moving to London, my model of the weather was not that useful. In fact, I often experienced <em>surprise</em>, as outcomes that should be rare happened often. In consequence, when using this code in London, my average message took up 1.9 bits.</p> <p>Entropy quantified our average surprise when observing a distribution’s outcomes while knowing the true distribution. Similarly, the <strong>cross-entropy</strong> measures our surprise when observing a distribution’s outcomes while only having a <em>model</em> of the true distribution. If \(P\) is the true distribution and \(Q\) is our model:</p> \[H(P, Q) = \sum_x P(x) \log \frac{1}{Q(x)}.\] <p>Just like entropy, \(\log \frac{1}{Q(x)}\) measures the degree of surprise we expect as per our model, which is weighted by the actual frequency with which we observe the outcome. It is also measured in bits.</p> <blockquote> <p>Note that order matters! \(H(P, Q) \neq H(Q, P)\).</p> </blockquote> <h2 id="why-theory-and-practice-can-differ">Why theory and practice can differ</h2> <p>The cross-entropy of my model \(Q_\text{Barcelona}\) on the London weather is:</p> \[H(P_\text{London}, Q_\text{Barcelona}) = 0.5 \times \log \frac 1 {0.2} + 0.4 \times \log \frac 1 {0.1} + 0.1 \times \log \frac 1 {0.7} \approx 2.54 \text{ bits}.\] <p>This is higher than the average message length of 1.9 bits. Contrary to entropy, which is a hard-limit, our model <em>can</em> do better than cross-entropy. This is because the cross-entropy leverages (optimal) fractional lengths, but our Huffman codes use non-fractional lengths, underestimating some outcomes and overestimating others:</p> <table> <thead> <tr> <th>Weather</th> <th>P</th> <th>Codeword length</th> <th>Q-optimal codeword length</th> <th>Extra/Saved bits</th> </tr> </thead> <tbody> <tr> <td>Cloudy</td> <td>0.5</td> <td>2</td> <td>2.32</td> <td>\(-0.32 \times 0.5 = -0.16\)</td> </tr> <tr> <td>Rainy</td> <td>0.4</td> <td>2</td> <td>3.32</td> <td>\(-1.32 \times 0.4 = -0.53\)</td> </tr> <tr> <td>Sunny</td> <td>0.1</td> <td>1</td> <td>0.51</td> <td>\(0.49 \times 0.1 = 0.05\)</td> </tr> </tbody> </table> <p>Notice how we’re saving a ton of bits on cloudy and rainy days; we got lucky. If we batch our weather reports, we get closer to encoding individual outcomes with fractional bits. Using the 10-day Barcelona code to report London weather, the average length of my message was \(2.53 \text{ bits}\), which is much closer to \(H(P, Q) \approx 2.54 \text{ bits}\). The cross-entropy <em>is</em> a lower bound if and only if we achieve the optimal coding for \(Q\).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/python/2025-07-14-cross-entropy/img/crossentropy-batch_size_vs_avg_bits_per_day.webp" sizes="95vw"></source> <img src="/assets/python/2025-07-14-cross-entropy/img/crossentropy-batch_size_vs_avg_bits_per_day.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h1 id="why-does-this-all-matter">Why does this all matter?</h1> <p>After a few years in London my model became quite accurate, to the extent that \(Q_\text{London} \approx P_\text{London}\):</p> \[H(P_\text{London}, Q_\text{London}) = 0.5 \times \log \frac 1 {0.5} + 0.4 \times \log \frac 1 {0.4} + 0.1 \times \log \frac 1 {0.1} \approx 1.36 \text{ bits}.\] <p>This is an important result (the <a href="https://en.wikipedia.org/wiki/Gibbs%27_inequality" rel="external nofollow noopener" target="_blank">Gibbs’ inequality</a>):</p> \[H(P, Q) \geq H(P, P) = H(P)\] <details><summary><strong>Kullback-Leibler (KL) divergence</strong></summary> <p>Since entropy is the lower bound for cross-entropy, the difference between both informs us about how well our model reflects the true distribution. This difference is also so important that it has its own name: <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="external nofollow noopener" target="_blank">Kullback-Leibler (KL) divergence</a>.</p> \[D_{KL}(P || Q) = H(P, Q) - H(P)\] <p>It can be interpreted as the <em>cost of being wrong</em>: how many extra bits we need to spend because our model departs from the true distribution.</p> </details> <p>Ultimately, this is why I went down this rabbit hole. We’ve covered distributions, processes, and encoding. But machine learning is one of the most important applications of cross-entropy via the <a href="https://en.wikipedia.org/wiki/Loss_function" rel="external nofollow noopener" target="_blank">cross-entropy loss</a>. During model <em>training</em>, many machine learning algorithms minimize the cross-entropy between the learned probability distribution (like <a href="https://en.wikipedia.org/wiki/Discriminative_model" rel="external nofollow noopener" target="_blank">\(P(Y \mid X)\)</a> or <a href="https://en.wikipedia.org/wiki/Generative_model" rel="external nofollow noopener" target="_blank">\(P(X, Y)\)</a>) and the one observed in the data.</p> <p>Let’s bring this point home by revisiting our weather model one last time. In this case, we want a model to predict tomorrow’s weather using some sensible variables (like today’s weather, temperature, humidity and wind). After some complex calculations it emits a probability vector \(q = [q_C, q_R, q_S]\). Say, for a given day, it predicts \(q = [0.35, 0.6, 0.05]\). Then, the day arrives, and we observe the true outcome: \(p = [1, 0, 0]\). Turns out our model was quite wrong!</p> <p>Here \(p\) is the one-hot (empirical) distribution on the observed class, not the true generative \(P\), and \(q\) is just the model’s prediction for this example, not the overall distribution \(Q\). Yet, the cross-entropy loss will have the familiar form:</p> \[\mathcal{L}(p,q) = -\sum_{i=1}^K p_i \log q_i = -\log q_y.\] <p>where \(K\) is the number of classes, and \(y\) is the index of the true class (\(1\) in our example).</p> <p>The model will consequently update its parameters to minimize this loss, adequately called cross-entropy loss (aka log-loss). Minimizing it is equivalent to <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" rel="external nofollow noopener" target="_blank">maximizing the probability of the data</a>. During training we minimize the average of this loss over the dataset \(\mathcal{D}\):</p> \[\mathcal{L} = \mathbb{E}_{(x,y) \sim \mathcal{D}} \bigl[-\log q_{y\mid x}\bigr] = -\frac{1}{N}\sum_{i=1}^N \log q_{y^{(i)}\mid x^{(i)}}.\] <h1 id="further-readings">Further readings</h1> <ul> <li><a href="https://colah.github.io/posts/2015-09-Visual-Information/" rel="external nofollow noopener" target="_blank">Visual Information Theory</a></li> <li><a href="https://www.youtube.com/watch?v=KHVR587oW8I" rel="external nofollow noopener" target="_blank">The Key Equation Behind Probability (video)</a></li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'hclimente/hclimente.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Héctor Climente-González. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>