<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> SHAP values | Héctor Climente-González </title> <meta name="author" content="Héctor Climente-González"> <meta name="description" content="A model-agnostic framework for explaining predictions"> <meta name="keywords" content="machine-learning, genetics, pharma, drug-discovery, climente, climente-gonzalez"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%AA%A2&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hclimente.github.io/blog/shapley/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/img-comparison-slider@8.0.6/dist/styles.min.css" integrity="sha256-3qTIuuUWIFnnU3LpQMjqiXc0p09rvd0dmj+WkpQXSR8=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-bundle.min.css" integrity="sha256-yUoNxsvX+Vo8Trj3lZ/Y5ZBf8HlBFsB6Xwm7rH75/9E=" crossorigin="anonymous"> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Héctor</span> Climente-González </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">SHAP values</h1> <p class="post-meta"> Created in April 01, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/feature-selection"> <i class="fa-solid fa-hashtag fa-sm"></i> feature_selection</a>   <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> machine_learning</a>   <a href="/blog/tag/feature-importance"> <i class="fa-solid fa-hashtag fa-sm"></i> feature_importance</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>SHAP values are a model-agnostic method to quantify the contribution of any given feature to a model’s prediction. They offer both local (per prediction) and global (overall) interpretations.</p> <h1 id="shapley-values">Shapley values</h1> <p>SHAP values have their roots in game theory, specifically in <strong>Shapley</strong> values. Imagine a group of players collaborating to achieve a payout. The Shapley value is a method to find out how to fairly distribute the total earnings among the players. Or the blame, if the payout was negative!</p> <p>A core concept of Shapley values is <strong>coalitions</strong>: given \(n\) players, a coalition is a subset of the players. Another concept is the <strong>characteristic function</strong>, \(v: 2^N \rightarrow \mathbb{R}\), which returns the total payout for any given coalition (its <em>worth</em>). Here, \(N\) is the set of all players. The last concept is the Shapley value itself, the amount \(\phi_i\) that player \(i\) receives. It is computed as the average of the marginal contributions of player \(i\) to all possible coalitions that do not include it. More formally, for a game \((v, N)\):</p> \[\phi_i(v) = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|!\; (n-|S|-1)!}{n!} (v(S\cup\{i\})-v(S))\] <p>These values satisfy four key properties, which collectively ensure fair attribution: efficiency, symmetry, dummy player, and additivity.</p> <h2 id="efficiency">Efficiency</h2> <p>The grand coalition is the coalition of all players, \(N\). Efficiency means that the sum of all Shapley values equals the value of the grand coalition, i.e., the entire payout is distributed among the players:</p> \[\sum_{i \in N} \phi_i(v) = v(N)\] <h2 id="symmetry">Symmetry</h2> <p>Two players \(i\) and \(j\) are symmetric if their marginal contribution to any coalition not containing either player is the same. That is, if \(v(S \cup \{i\}) = v(S \cup \{j\})\) for any coalition \(S \subseteq N \setminus \{i, j\}\), then symmetry implies that players \(i\) and \(j\) receive the same Shapley value: \(\phi_i(v) = \phi_j(v)\).</p> <h2 id="dummy-player">Dummy player</h2> <p>If a player \(i\) does not change the value of any coalition they join (i.e., \(v(S \cup \{i\}) = v(S)\) for all \(S \subseteq N \setminus \{i\}\)), they are a dummy player. The dummy player property states that such a player’s Shapley value is 0.</p> <h2 id="additivity">Additivity</h2> <p>If two games with characteristic functions \(v_1\) and \(v_2\) are combined into a new game \(v_1 + v_2\) (where \((v_1+v_2)(S) = v_1(S) + v_2(S)\) for any coalition \(S\)), the Shapley values are additive:</p> \[\phi_i(v_1+v_2) = \phi_i(v_1) + \phi_i(v_2)\] <h1 id="shap-values">SHAP values</h1> <p>Machine learning models like linear regression are <em>interpretable</em>, as the model parameters indicate how each input feature contributes to the prediction. However, many complex models like neural networks or random forests are less directly interpretable: their output is a complex, non-linear combination of the input features. SHAP values (<a href="https://arxiv.org/abs/1705.07874" rel="external nofollow noopener" target="_blank">Lundberg and Lee, 2017</a>) provide a framework to quantify the contribution of each feature to a specific prediction for <em>any</em> model. SHAP stands for SHapley Additive exPlanations, highlighting their connection to <em>Shapley</em> values.</p> <p>Intuitively, SHAP values quantify how much each feature’s presence changes the prediction. Some features will have a negative contribution (pushing the prediction lower) and others a positive contribution (pushing the prediction higher). The sum of a feature’s SHAP value and a baseline value (typically the average prediction) approximates the model’s output.</p> <p>To establish the connection to Shapley values, we map the game theory concepts to the machine learning context:</p> <ul> <li>The \(n\) players become \(n\) <em>predictive features</em>.</li> <li>The game is the <em>trained model</em>.</li> <li>The payout for a coalition of features is the <em>model’s prediction</em> when only those features are known.</li> </ul> <p>The Shapley value \(\phi_i\) for feature \(i\) in this context is then calculated as:</p> \[\phi_i = \sum_{S \subseteq F \setminus \{i\}} \frac{|S|!\; (n-|S|-1)!}{n!} (f(\mathbf{x}_{S\cup\{i\}})-f(\mathbf{x}_S))\] <p>where \(\mathbf{x}_S\) is the input datapoint including only the features in \(S\); \(F\) is the set of all features; \(n\) is the total number of features; and \(f_S\) is the model trained only on the features in set \(S\). However, this naïve approach is very computationally intensive, since it’d require retraining \(2^n\) models, one per possible coalition. (See a worked out example <a href="https://www.aidancooper.co.uk/how-shapley-values-work/" rel="external nofollow noopener" target="_blank">here</a>.) SHAP values get around re-training models by approximating the effect of feature subsets using conditional expectations: \(f(\mathbf{x}_S) = \mathbb{E}[f(X) \mid X_S = \mathbf{x}_S]\). In other words, we fix the features that are in \(S\) to the sample values, and average over the predictions when sampling the remaining features from the dataset.</p> <blockquote> <p><strong><em>Simplified</em> features:</strong> We use \(\mathbf{x}\) for the features in the original space \(\chi\), a vector of length \(n\). The SHAP theoretical framework uses a simplified feature vector \(\mathbf{x}' \in \{0,1\}^m\), where \(m\) is the number of simplified features (which can be different from \(n\)). \(x'_j = 1\) indicates that simplified feature \(j\) is “present” in a coalition, and \(x'_j = 0\) indicates it is “absent”. The simplified features are more useful for interpretation. For instance, if \(\mathbf{x}\) represented the individual pixels of an image, \(\mathbf{x}'\) could represent the presence of the “super pixels” that form a cat, grass or the sky. A mapping function \(h_\mathbf{x}: \{0,1\}^m \rightarrow \chi\) links the simplified representation back to the original feature space. For \(\mathbf{x}' = \mathbf{1}\) (all simplified features present), \(h_\mathbf{x}(\mathbf{1}) = \mathbf{x}\). For other \(\mathbf{x}'\), \(h_\mathbf{x}(\mathbf{x}')\) represents the original instance with features corresponding to \(x'_j=0\) appropriately handled (e.g., replaced by baseline values). Note that \(h_\mathbf{x}\) is specific to the instance \(\mathbf{x}\) being explained.</p> </blockquote> <p>That covers the <em>Shapley</em> part of SHAP; let’s now focus on the <em>Additive exPlanation</em> bit. The goal of SHAP is to obtain a local, additive explanation model \(g\) for each prediction \(f(\mathbf{x})\) using the simplified features \(\mathbf{x}'\):</p> \[g(\mathbf{x}') = \phi_0 + \sum_{j = 1}^m \phi_j \mathbf{x}'_j\] <p>where \(\phi_0\) is the expectation over all training examples \(\mathbb{E}[f(X)]\). \(g(\mathbf{x}')\) is a very easy to interpret function that we’ll use to explain \(f(\mathbf{x})\).</p> <p>Since SHAP values are Shapley values, they meet all the properties specified above. But they also satisfy three additional properties that are desirable for model explainers.</p> <h2 id="local-accuracy">Local accuracy</h2> <p>When all simplified features are present (\(\mathbf{x}' = \mathbf{1}\)), the explanation model \(g\) must equal the prediction \(f(\mathbf{x})\):</p> \[f(\mathbf{x}) = g(\mathbf{1}) = \phi_0 + \sum_{j = 1}^m \phi_j\] <h2 id="missingness">Missingness</h2> <p>If a feature is missing, it deserves 0 attribution:</p> \[\mathbf{x}'_j = 0 \implies \phi_j = 0.\] <p>This is a required property to ensure that local accuracy has a unique solution.</p> <h2 id="consistency">Consistency</h2> <p>The consistency ensures that if a model \(f\) changes into another model \(f'\), such that a feature’s contribution doesn’t decrease, the SHAP values do not decrease either. Formally, if</p> \[f'(S) - f'(S \setminus \{i\}) \geq f(S) - f(S \setminus \{i\})\] <p>for all \(S \in F\), then \(\phi_i(f', \mathbf{x}) \geq \phi_i(f, \mathbf{x})\).</p> <h1 id="a-visual-example">A visual example</h1> <p>Let’s understand SHAP values better by looking at an example. I trained a model that uses 10 clinical features (body mass index, cholesterol, age, and a few others) to predict a continuous measure of disease progression one year after baseline. For the purposes of this example, the model is treated as a black box whose input are the 10 features, and the output a real number.</p> <table> <thead> <tr> <th>Age</th> <th>Sex</th> <th>BMI</th> <th>Blood pressure</th> <th>…</th> <th>Target</th> </tr> </thead> <tbody> <tr> <td>0.0380759</td> <td>0.0506801</td> <td>0.0616962</td> <td>0.0218724</td> <td>…</td> <td>151</td> </tr> <tr> <td>-0.00188202</td> <td>-0.0446416</td> <td>-0.0514741</td> <td>-0.0263275</td> <td>…</td> <td>75</td> </tr> <tr> <td>0.0852989</td> <td>0.0506801</td> <td>0.0444512</td> <td>-0.00567042</td> <td>…</td> <td>141</td> </tr> <tr> <td>…</td> <td>…</td> <td>…</td> <td>…</td> <td>…</td> <td>…</td> </tr> </tbody> </table> <blockquote> <p>SHAP values can be computed on the dataset used to train the model (train set) or on a holdout set. Using a larger dataset like the train set might provide a more stable picture of overall feature contributions learned by the model. However, if the train and test data come from different distributions, computing SHAP on the respective sets will likely yield different results.</p> </blockquote> <details><summary>The <code class="language-plaintext highlighter-rouge">shap</code> package</summary> <p>SHAP values are implemented in Python via the <a href="https://shap.readthedocs.io/en/latest/index.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">shap</code></a> package. While I won’t be showing any code here, you can see the code that generated the figures <a href="/assets/python/2025-04-01-shapley/main.py">here</a>.</p> </details> <p>SHAP values provide <strong>local</strong> explanations, showing the contribution of each feature to a particular prediction. I computed the SHAP values describing the importance of each of the 10 variables for each of the 442 patients. These values represent the estimated impact of each feature on a prediction, relative to the average prediction. We can start by looking at the SHAP values for one patient, using a <em>waterfall</em> plot:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/python/2025-04-01-shapley/img/waterfall_diabetes-480.webp 480w,/assets/python/2025-04-01-shapley/img/waterfall_diabetes-800.webp 800w,/assets/python/2025-04-01-shapley/img/waterfall_diabetes-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/python/2025-04-01-shapley/img/waterfall_diabetes.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The waterfall plot shows how the prediction for this patient (186.53) departs from the average prediction over the training set (152.132). The difference (34.398) is the total change attributed by the model. As per the local accuracy property, the SHAP values for this instance sum up to this difference. The features are sorted by the absolute magnitude of their SHAP value. Features colored in pink push the prediction toward higher values, and features in blue toward lower values. We can see that, for this patient, the body mass index was the most important feature, contributing positively by 22.25.</p> <p>We can visualize SHAP values for all 442 patients using a <em>swarmplot</em>:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/python/2025-04-01-shapley/img/beeswarm_diabetes-480.webp 480w,/assets/python/2025-04-01-shapley/img/beeswarm_diabetes-800.webp 800w,/assets/python/2025-04-01-shapley/img/beeswarm_diabetes-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/python/2025-04-01-shapley/img/beeswarm_diabetes.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>In the swarmplot, each point represents the SHAP value for a patient for a specific feature. Features are shown on the y-axis, and their corresponding SHAP values on the x-axis. As in the waterfall plot, features are sorted by their overall importance; and the color of each point indicates the feature value for that patient (pink for high, blue for low).</p> <p>Global explanations can be derived by aggregating the local SHAP values over a dataset. A common global measure is the average absolute SHAP value for each feature:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/python/2025-04-01-shapley/img/global_diabetes-480.webp 480w,/assets/python/2025-04-01-shapley/img/global_diabetes-800.webp 800w,/assets/python/2025-04-01-shapley/img/global_diabetes-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/python/2025-04-01-shapley/img/global_diabetes.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Plotting these averages shows which features have the largest impact on the model’s predictions <em>on average</em> across the dataset, providing a global measure of feature importance.</p> <p>Lastly, SHAP values can be used for clustering. While traditional clustering groups data points based on their original feature values, clustering in SHAP space groups points based on how features <em>contribute to the model’s prediction</em>. This can be seen as a form of <em>supervised</em> clustering, as it leverages the model’s output (and indirectly the outcome it was trained on). Clustering SHAP values can reveal groups of instances where different sets of features drive the prediction.</p> <style>.colored-slider{--divider-color:rgba(0,0,0,0.5);--default-handle-color:rgba(0,0,0,0.5);--default-handle-width:clamp(40px,10vw,200px)}</style> <img-comparison-slider class="colored-slider"> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/python/2025-04-01-shapley/img/supervised_pca-480.webp 480w,/assets/python/2025-04-01-shapley/img/supervised_pca-800.webp 800w,/assets/python/2025-04-01-shapley/img/supervised_pca-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/python/2025-04-01-shapley/img/supervised_pca.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/python/2025-04-01-shapley/img/unsupervised_pca-480.webp 480w,/assets/python/2025-04-01-shapley/img/unsupervised_pca-800.webp 800w,/assets/python/2025-04-01-shapley/img/unsupervised_pca-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/python/2025-04-01-shapley/img/unsupervised_pca.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </img-comparison-slider> <p>Applying PCA to the SHAP values (“supervised PCA”) and the original features (“unsupervised PCA”) for this dataset, we can visualize how instances are grouped.</p> <h1 id="limitations">Limitations</h1> <p>One key limitation of interpreting SHAP values is their behavior with <strong>highly correlated features</strong>. When features are strongly correlated, the model might arbitrarily use one over the others, or distribute importance among them. Consequently, the SHAP values for individual correlated features can become unstable or misleading, making it hard to disentangle their individual contributions.</p> <p>Another point of consideration is <strong>feature interactions</strong>. While the fundamental Shapley value calculation inherently accounts for interactions (by averaging marginal contributions over different coalitions), the basic additive SHAP explanation model \(g(\mathbf{x}') = \phi_0 + \sum \phi_j \mathbf{x}'_j\) does not explicitly separate main effects from interaction effects. The \(\phi_j\) values represent the <em>average</em> contribution of feature \(j\), including its interactive effects, making their interpretation as pure “main effects” challenging when interactions are significant. However, SHAP <a href="https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/tree_based_models/Basic%20SHAP%20Interaction%20Value%20Example%20in%20XGBoost.html" rel="external nofollow noopener" target="_blank"><em>can</em> be extended</a> to compute pairwise SHAP interaction values (\(\phi_{ij}\)) which explicitly quantify the interaction between features \(i\) and \(j\).</p> <p>Finally, it’s important to remember that SHAP values explain <em>how the model makes a prediction</em>, not whether the prediction itself is correct. If the model is biased, overfit, or simply wrong for a given instance, the SHAP values will faithfully explain the mechanism behind that incorrect prediction. Measures like permutation feature importance, which rely on model performance metrics after feature perturbation, inherently account for the model’s correctness in their explanation.</p> <h1 id="flavors-of-shap-the-permutation-approximation">Flavors of SHAP: the permutation approximation</h1> <p><a href="#shap-values">Above</a> I described the general approach to compute SHAP values. Unfortunately, it is very computationally intensive: exploring all possible coalitions is equivalent to exploring all \(2^m\) subsets of features. For that reason, different flavors of SHAP values have been proposed to make computations more efficient. I describe below the <strong>permutation approximation</strong>, a model-agnostic method to compute SHAP values \(\phi_i\). However, there are others specialized in specific model types, like <a href="https://shap.readthedocs.io/en/latest/generated/shap.TreeExplainer.html" rel="external nofollow noopener" target="_blank">Tree SHAP</a> for tree-based models (e.g., random forests, gradient boosting) and <a href="https://shap.readthedocs.io/en/latest/generated/shap.DeepExplainer.html" rel="external nofollow noopener" target="_blank">Deep SHAP</a> for deep learning models.</p> <p>The permutation approximation approximates SHAP values by estimating the expected marginal contribution of each feature over many random permutations of the features. Let’s study the permutation approximation through a worked out example. We aim to explain a prediction \(f(\mathbf{x}_0)\). We have at our disposal a background dataset \(X_\text{bg}\), e.g., the whole training set, which we will use for sampling. For the sake of the example, our model only considers four (simplified) features: \(\mathbf{x}_0 = [x_{\text{age}}, x_\text{sex}, x_\text{BMI}, x_\text{BP}]\).</p> <p>For each feature \(i\) that we want to explain:</p> <ol> <li>Initializing a list to store the marginal contribution of each feature. In this example, I will focus on the contribution of the first feature, \(x_\text{age}\), so I will call this list just \(\text{list}_\text{age}\).</li> <li>For \(K\) iterations: <ol> <li>A random ordering of the features is produced, e.g., \((\text{BP}, \text{age}, \text{BMI}, \text{sex})\), and a random sample \(\mathbf{z}\) is sampled from the background dataset \(X_\text{bg}\).</li> <li>Create two synthetic examples: - \(\mathbf{x}_1 = (x_\text{BP}, x_\text{age}, z_\text{BMI}, z_\text{sex})\) - \(\mathbf{x}_2 = (x_\text{BP}, z_\text{age}, z_\text{BMI}, z_\text{sex})\) Note that the only difference between the two examples is the value of the age feature.</li> <li>Compute the marginal contribution of the age feature as \(\delta = f(\mathbf{x}_1) - f(\mathbf{x}_2)\).</li> <li>Append the marginal contribution to \(\text{list}_\text{age}\).</li> </ol> </li> <li>Approximate the SHAP value as the average marginal contribution: \(\phi_\text{age} \cong \frac{1}{K} \sum_i \delta_{i}.\)</li> </ol> <h1 id="further-reading">Further reading</h1> <ul> <li><a href="https://christophm.github.io/interpretable-ml-book/shapley.html" rel="external nofollow noopener" target="_blank">Interpretable Machine Learning: Shapley values</a></li> <li><a href="https://christophm.github.io/interpretable-ml-book/shap.html" rel="external nofollow noopener" target="_blank">Interpretable Machine Learning: SHAP</a></li> <li><a href="https://shap.readthedocs.io" rel="external nofollow noopener" target="_blank">Python’s <code class="language-plaintext highlighter-rouge">shap</code> documentation</a></li> <li><a href="https://www.aidancooper.co.uk/supervised-clustering-shap-values/" rel="external nofollow noopener" target="_blank">Supervised Clustering: How to Use SHAP Values for Better Cluster Analysis</a></li> <li><a href="https://davidrosenberg.github.io/ttml2021fall/interpretable-ml/5.Shapley-LIME-SHAP.pdf" rel="external nofollow noopener" target="_blank">Shapley Values, LIME, and SHAP</a></li> </ul> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'hclimente/hclimente.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Héctor Climente-González. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/img-comparison-slider@8.0.6/dist/index.min.js" integrity="sha256-EXHg3x1K4oIWdyohPeKX2ZS++Wxt/FRPH7Nl01nat1o=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-element-bundle.min.js" integrity="sha256-BPrwikijIybg9OQC5SYFFqhBjERYOn97tCureFgYH1E=" crossorigin="anonymous"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>