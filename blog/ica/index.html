<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Independent Component Analysis | Héctor Climente-González </title> <meta name="author" content="Héctor Climente-González"> <meta name="description" content="Cocktail parties make me anxious"> <meta name="keywords" content="machine-learning, genetics, pharma, drug-discovery, climente, climente-gonzalez"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%AA%A2&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hclimente.github.io/blog/ica/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Independent Component Analysis",
            "description": "Cocktail parties make me anxious",
            "published": "October 10, 2025",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Héctor</span> Climente-González </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Independent Component Analysis</h1> <p>Cocktail parties make me anxious</p> </d-title> <d-article> <p>Among spies, hackers and tabloids, <strong>independent component analysis (ICA)</strong> immediately brings to mind the cocktail party problem, the notorious signal processing problem it solves.</p> <p>The problem reads as follows. A spy wants to snoop on all the conversations happening in a cocktail party. To that end, they strategically place multiple microphones across the room.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/python/2025-09-22-ica/img/cocktail_source_signals.webp" sizes="95vw"></source> <img src="/assets/python/2025-09-22-ica/img/cocktail_source_signals.webp" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>However, the party is quite busy: many conversations are ongoing, a piano plays in the background, guests gulp, waiters trot, glasses clink. Each microphone only captures a mixture of all these signals, which results in mostly incomprehensible gibberish.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/python/2025-09-22-ica/img/cocktail_microphone_signals.webp" sizes="95vw"></source> <img src="/assets/python/2025-09-22-ica/img/cocktail_microphone_signals.webp" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Other spies would get deterred, and move on to more persuasive ways of getting the information. But this is no ordinary spy. Armed with a secret weapon, ICA, they surgically extract the original conversations and save the world:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/python/2025-09-22-ica/img/cocktail_ica_signals.webp" sizes="95vw"></source> <img src="/assets/python/2025-09-22-ica/img/cocktail_ica_signals.webp" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h1 id="the-geometry-of-ica">The geometry of ICA</h1> <p>The cocktail party problem is a good analogy for a common challenge in data science: untangling complex signals. Multidimensional datasets are somewhat redundant, consisting of complex mixtures of a relatively small number of latent factors. Distilling our dataset into that reduced number of dimensions can really help understand it better.</p> <p>One way to go about it is <strong>matrix factorization</strong>, which decomposes our original matrix \(X\) into a product of matrices. Often the matrix is decomposed into <em>two</em> matrices: \(X = A S\). \(A\), the mixing matrix, represents a new vector basis. Since it’s a good descriptor of the geometry of the data, studying \(A\) can teach us something about it. \(S\), the score matrix, represents the coordinates of each datapoint in that new basis.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-09-22-ica/change_of_basis.webp" sizes="95vw"></source> <img src="/assets/img/posts/2025-09-22-ica/change_of_basis.webp" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Illustration of a change of basis. The data itself hasn't moved, but the new basis makes it trivial to identify which points belong to which subgroup. </div> <p>As you would have guessed, what “a good descriptor of the geometry of the data” is, exactly, is up for debate. But, in general, it should help with interpretability, data compression, or parsimony.</p> <p>Unsurprisingly, many matrix factorizations have been proposed. A well-known one is the principal component analysis, or PCA. Geometrically speaking, PCA fits an ellipsoid to the data, and uses the axes of this ellipsoid as the basis for the new space. This is equivalent to PCA’s well-known interpretation: capturing the main axes of variation in the data. However, when our data has substructures that are not captured by the variance, PCA performs pretty poorly.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-09-22-ica/pca_overview-480.webp 480w,/assets/img/posts/2025-09-22-ica/pca_overview-800.webp 800w,/assets/img/posts/2025-09-22-ica/pca_overview-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/2025-09-22-ica/pca_overview.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Illustration of the principal component analysis on well-behaved data (A) and on data with substructures (B). Note that in the latter case, the new basis is not particularly well-suited to represent the data. </div> <p>And this is where ICA shines:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/python/2025-09-22-ica/img/ica_pca_sklearn_example.webp" sizes="95vw"></source> <img src="/assets/python/2025-09-22-ica/img/ica_pca_sklearn_example.webp" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> ICA and PCA applied to a synthetic dataset. The underlying, independent signals (A) are combined into the observed signal (B) as \(x = c_1 + c_2\), \(y = 2 c_2\). While ICA finds the best basis (B) and recovers the true components (C), PCA cannot (D). Adapted from the <a href="https://scikit-learn.org/stable/auto_examples/decomposition/plot_ica_vs_pca.html#sphx-glr-auto-examples-decomposition-plot-ica-vs-pca-py" rel="external nofollow noopener" target="_blank">scikit-learn documentation</a>. </div> <h1 id="how-does-ica-work">How does ICA work?</h1> <p>PCA seeks a basis of uncorrelated components, since the axes of the ellipsoid need to be orthogonal. ICA goes one step further: it seeks a <strong>statistically independent</strong> basis.</p> <p>More formally, ICA assumes the observed data is a mixture of the individual components. For instance, in the cocktail party, each component (e.g., the piano playing) is a vector indexed by time (\(s_j(t)\)). The components must meet two assumptions: <em>non-Gaussianity</em> and <em>statistical independence</em>. There is also an assumption that they are combined <em>linearly</em> to generate the observed data:</p> \[x_i = \sum_j a_{ij} s_j\] <p>where \(a_{ij}\) is a constant, representing the relative contribution of each component.</p> <p>We can arrange the observation vectors into a matrix, and think of it as a matrix factorization problem:</p> \[X = AS\] <p>where \(A\) is called the mixing matrix and \(S\) the independent component matrix. The goal is to estimate both \(A\) and \(S\) from \(X\). To that end, ICA seeks a de-mixing matrix \(W\) such that:</p> \[S = W X\] <p>The linearity assumption is key, since the central limit theorem tells us that the sum of independent random variables converges in distribution to a Gaussian. The magic lies here: our data needs to be <em>more</em> Gaussian than any of the individual components, and we will unmix them by seeking highly non-Gaussian components.</p> <p>Specifically, we want \(W\) to maximize the non-Gaussianity of the recovered signals in \(S\). Once we have \(W\), computing \(A\) is straightforward, since \(W = A^{-1}\).</p> <details><summary>Quantifying non-Gaussianity</summary> <p>There are two popular ways of quantifying the non-Gaussianity of a distribution.</p> <p>The first way consists in comparing the properties of the distribution called the <a href="https://gregorygundersen.com/blog/2020/04/11/moments/" rel="external nofollow noopener" target="_blank"><strong>moments</strong></a>. The first two moments, the mean and the variance, are not useful, since the data is usually standardized to have 0 mean and 1 variance. But the third and fourth moments (skewness and kurtosis, respectively) are frequently used:</p> <ul> <li>The <strong>skewness</strong> measures the asymmetry of the distribution. Gaussian distributions are famously symmetrical, and have a skewness of 0.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-09-22-ica/skewness.webp" sizes="95vw"></source> <img src="/assets/img/posts/2025-09-22-ica/skewness.webp" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>The <strong>kurtosis</strong> measures the tailedness of the distribution. By comparing the kurtosis of our distribution relative to that of a normal distribution (<em>excess kurtosis</em>), we can estimate how non-Gaussian it is. The kurtosis is popular because it can be easily estimated from the data.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-09-22-ica/kurtosis.webp" sizes="95vw"></source> <img src="/assets/img/posts/2025-09-22-ica/kurtosis.webp" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The second family involves using the <strong>entropy</strong> of the distribution. A fundamental theorem in information theory states that a Gaussian variable has the largest entropy among all random variables of equal variance. By computing the difference between the entropy of our distribution and that of a gaussian distribution of equal variance, we can estimate how non-Gaussian it is. This difference is called the <strong>negentropy</strong>.</p> </details> <h2 id="limitations">Limitations</h2> <p>Before we continue, it’s important to acknowledge that ICA has some limitations.</p> <p>First, it makes some strong assumptions about the data (linear mixing, non-Gaussianity, statistical independence) that may not hold in practice. If these assumptions are violated, the results of ICA may be misleading. For instance, if two or more components are Gaussian, ICA won’t be able to separate them.</p> <p>Second, the solution matrices, \(A\) and \(S\) are not unique. Both the scale and the order of the components are arbitrary. In other words, scaling \(A\) by a constant and scaling \(S\) by the inverse of that constant yields the same product. Similarly, permuting the rows of \(A\) and the columns of \(S\) also yields the same product. Hence, two runs of ICA on the same data may yield different results, albeit they should be equivalent up to permutation and scaling.</p> <h2 id="implementation">Implementation</h2> <p>Let’s put the above into practice by aiming for the simplest, possibly most inefficient, implementation of ICA. We will work on an \(m \times n\) matrix \(X\), where \(m\) is the number of features and \(n\) the number of samples. Before we can apply ICA, we need to apply two preprocessing steps to \(X\):</p> <ul> <li> <strong>Centering</strong>, ensuring that each feature has a zero mean</li> <li> <strong>Whitening</strong>, ensuring that the features are uncorrelated and have unit variance. This vastly simplifies the ICA problem since it restricts the space of possible de-mixing matrices \(W\) to only <em>orthogonal</em> ones.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_centered</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">pca</span> <span class="o">=</span> <span class="nc">PCA</span><span class="p">(</span><span class="n">whiten</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">X_whitened</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X_centered</span><span class="p">)</span>
</code></pre></div></div> <p>Now we will seek an \(m\)-dimensional vector \(\mathbf{w}_1\) that maximizes the non-Gaussianity of the independent component \(\mathbf{s}_{(1)} = X_{whitened} \mathbf{w}_1\). We will quantify non-Gaussianity using the excess kurtosis:</p> \[\operatorname{Kurtosis}(Y) = E\left[\left(\frac{Y - \mu}{\sigma}\right)^4\right] - 3.\] <p>Since our data is centered and whitened, this simplifies to \(\operatorname{Kurtosis}(Y) = E\left[\left(Y\right)^4\right] - 3\).</p> <p>We will use a biased sample estimator of the excess kurtosis:</p> \[\hat{K}(\mathbf{s}_{(1)}) = \frac 1 n \sum_{i=1}^n s_{(1)i}^4 - 3 = \frac 1 n \sum_{i=1}^n (\mathbf{x_{i*}}^\intercal \mathbf{w})^4 - 3.\] <p>We will seek the \(\mathbf{w}_1\) that maximizes the kurtosis by gradient ascent. Hence, we need the first partial derivative of the kurtosis:</p> \[\frac { d \hat{K}(\mathbf{s}_{(1)}) } { d \mathbf{w}_1 } = \frac 1 n \sum_{i=1}^n 4 (\mathbf{x_{i*}}^\intercal \mathbf{w})^3 \mathbf{x_{i*}} = \frac 4 n \sum_{i=1}^n s_{(1)i}^3 \mathbf{x_{i*}}.\] <p>Let’s implement this in Python:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">STEP_SIZE</span> <span class="o">=</span> <span class="mf">1e-3</span>
<span class="n">N_ITERATIONS</span> <span class="o">=</span> <span class="mi">50</span>

<span class="c1"># n = number of samples, m = number of features
</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="n">X_whitened</span><span class="p">.</span><span class="n">shape</span>

<span class="c1"># Random initial weight vector
</span><span class="n">w1</span> <span class="o">=</span> <span class="n">rng</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="n">w1</span> <span class="o">/=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">w1</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-10</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">N_ITERATIONS</span><span class="p">):</span>
    <span class="c1"># Project data onto weight vector
</span>    <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">X_whitened</span><span class="p">,</span> <span class="n">w1</span><span class="p">)</span>

    <span class="c1"># Compute the gradient
</span>    <span class="n">gradient</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">/</span> <span class="n">n</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">pow</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">X_whitened</span><span class="p">)</span>

    <span class="c1"># Update the weight vector
</span>    <span class="n">w1</span> <span class="o">+=</span> <span class="n">STEP_SIZE</span> <span class="o">*</span> <span class="n">gradient</span>

    <span class="c1"># Normalize the weight vector
</span>    <span class="n">w1</span> <span class="o">/=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">w1</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-10</span>
</code></pre></div></div> <p>The algorithm converges pretty quickly:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/python/2025-09-22-ica/img/ica_kurtosis_gd_component_1.webp" sizes="95vw"></source> <img src="/assets/python/2025-09-22-ica/img/ica_kurtosis_gd_component_1.webp" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Before repeating the process to find the second component, we need to ensure that it is statistically independent from the first component. We can achieve this through a <strong>deflation</strong> step, which removes the influence of already-found components from the data before searching for the next component. This process is conceptually similar to the <a href="https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process" rel="external nofollow noopener" target="_blank">Gram-Schmidt algorithm</a> for creating orthogonal vectors.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_deflated</span> <span class="o">=</span> <span class="n">X_whitened</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">X_whitened</span><span class="p">,</span> <span class="n">w1</span><span class="p">),</span> <span class="n">w1</span><span class="p">)</span>
</code></pre></div></div> <blockquote> <p>Deflation is good for illustration purposes, but too numerically unstable for practical use. A more robust approach is to use the <a href="https://support.sas.com/resources/papers/proceedings19/2997-2019.pdf" rel="external nofollow noopener" target="_blank">symmetric decorrelation</a> method, which ensures that keeps the data unchanged, and optimizes all components at once while keeping them orthogonal.</p> </blockquote> <p>Then, we repeat:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Random initial weight vector
</span><span class="n">w2</span> <span class="o">=</span> <span class="n">rng</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="n">w2</span> <span class="o">/=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">w2</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-10</span>

<span class="c1"># Repeat gradient ascent to find second component
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">N_ITERATIONS</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">X_deflated</span><span class="p">,</span> <span class="n">w2</span><span class="p">)</span>
    <span class="n">gradient</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">/</span> <span class="n">n</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">pow</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">X_deflated</span><span class="p">)</span>
    <span class="n">w2</span> <span class="o">+=</span> <span class="n">STEP_SIZE</span> <span class="o">*</span> <span class="n">gradient</span>
    <span class="n">w2</span> <span class="o">/=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">w2</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-10</span>
</code></pre></div></div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/python/2025-09-22-ica/img/ica_kurtosis_gd_component_2.webp" sizes="95vw"></source> <img src="/assets/python/2025-09-22-ica/img/ica_kurtosis_gd_component_2.webp" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>We could keep going by removing the influence of both the first and second components using the deflation steps, and running gradient ascent again to find the third component.</p> <blockquote> <p>Our simple ICA gets the job done, but it is unstable and the results are not optimal, as we can see by comparing them with the results obtained in the <a href="#the-geometry-of-ica">ICA vs PCA comparison</a>. If you want to play with ICA, <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html" rel="external nofollow noopener" target="_blank">FastICA</a> is the way to go. It offers multiple improvements over my toy implementation, including a fast optimization process (fixed-point algorithm), solves all the components at once, and will even whiten the data for you.</p> </blockquote> <h1 id="application-gene-expression-analysis">Application: gene expression analysis</h1> <p>Ultimately, the reason I went down the ICA rabbit hole was not to eavesdrop in parties, but because of its applications in computational biology. Specifically, ICA is used to factorize gene expression matrices into a matrix containing groups of genes that work together (\(A\), the <em>metagenes</em>) and a matrix containing groups of similar samples (\(S\), the <em>metasamples</em>). Both can provide insights into the underlying biology:</p> <ul> <li>The metagenes are particularly useful when we can annotate them with the biological process they capture. We can do that by studying their overlap with known signatures or pathways, or finding commonalities among the genes’ properties.</li> <li>The metasamples can be helpful if we have other information about the samples (e.g., clinical records) that help us understand what each set of samples have in common.</li> </ul> <p><a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.0030161" rel="external nofollow noopener" target="_blank">Teschendorf et al. (2007)</a> provides an example for both. While other matrix factorization algorithms are common, <a href="https://www.nature.com/articles/s41467-018-03424-4" rel="external nofollow noopener" target="_blank">ICA is one of the best</a>, making it an essential tool in the computational biologist’s toolbox.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <d-article> <br> <br> <div id="giscus_thread"> <script defer src="/assets/js/giscus-setup.js"></script> <noscript> Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </d-article> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Héctor Climente-González. All content and views expressed here are strictly personal and do not reflect the opinions, policies, or practices of my current or former employer. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>