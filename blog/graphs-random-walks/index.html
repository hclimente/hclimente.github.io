<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Random walks and Markov chains | Héctor Climente-González </title> <meta name="author" content="Héctor Climente-González"> <meta name="description" content="PageRank, MCMC, and others"> <meta name="keywords" content="machine-learning, genetics, pharma, drug-discovery, climente, climente-gonzalez"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%AA%A2&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hclimente.github.io/blog/graphs-random-walks/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Héctor</span> Climente-González </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Random walks and Markov chains</h1> <p class="post-meta"> Created in January 27, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/graphs"> <i class="fa-solid fa-hashtag fa-sm"></i> graphs</a>   <a href="/blog/tag/random-walks"> <i class="fa-solid fa-hashtag fa-sm"></i> random_walks</a>   <a href="/blog/tag/linear-algebra"> <i class="fa-solid fa-hashtag fa-sm"></i> linear_algebra</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="random-walk">Random walk</h1> <p>A <strong>random walk (RW)</strong> is a <a href="https://en.wikipedia.org/wiki/Stochastic_process" rel="external nofollow noopener" target="_blank">stochastic</a>, discrete process. At a given time step, the walker, located in one of the graph’s vertices, picks one of its neighbors at random and moves to it. Often the transition probability between vertices is represented by the <strong>transition</strong> matrix \(P\), a normalized version of the <a href="/blog/graphs-linear-algebra/#adjacency-matrix">adjacency</a> in which the weights of all outbound edges add up to 1:</p> \[P = D^{-1} A\] <p>The outcome of a single random walk is a <a href="/blog/graphs-glossary/#walk">walk</a> of length \(t\), where \(t\) is the number of steps.</p> <p>We can consider the outcome of infinitely many random walkers departing from a given vertex. In that case, the outcome at time \(t\) is an \(n\)-dimensional column vector \(\mathbf{\pi}_t\) in which \(\pi_{ti}\) represents the probability of the walker starting at a given vertex and being on vertex $i$ at time $t$. The probability distribution at step \(t+1\) is computed as:</p> \[\mathbf{\pi}_{t+1} = \mathbf{\pi}_t P\] <p>If we allow the RW to run indefinitely, the probability of ending up at each vertex reaches a stationary distribution \(\pi\) such that the probability of being in vertex \(i\)</p> \[\pi_i = \frac {d_i} {\sum_j d_j}.\] <p>That is, high <a href="/blog/graph-properties/#regular">degree</a> vertices are more likely to be visited. If the graph is <a href="/blog/graphs-glossary/#regular">regular</a>, the stationary distribution is uniform. Because of this property, the initial vertex is not important in the long run: if we allow the RW to run indefinitely, the probability of being at any given vertex is uniform.</p> <blockquote> <p><strong><em>Lazy</em> random walks:</strong> A stationary distribution does not always exists. For instance, consider the case of a random walk on a <a href="/blog/graphs-glossary/#bipartite">bipartite</a> graph: at step \(t\) the walker will be on one side or another, depending on the initial vertex and the parity of \(t\). Such cases have a stationary distribution under the <a href="https://ocw.mit.edu/courses/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/100377025e8520aab9f61d8585e71cc5_MIT18_409F09_scribe4.pdf" rel="external nofollow noopener" target="_blank"><em>lazy</em> random walk</a>, in which the walker has a probability \(\frac 1 2\) of remaining at the current vertex, and a probability \(\frac 1 2\) of leaving it.</p> </blockquote> <details><summary>Proof</summary> <p>To understand why this happens, let’s expand what happens at each step of a random walk starting at vertex \(i\):</p> <ul> <li>At step 0, \(\mathbf{\pi}_0 = (0, 0, \cdots, 1, \cdots, 0)^\intercal\), i.e., a \(0\)-vector almost everywhere, with a \(1\) at position \(i\).</li> <li>At step 1, \(\mathbf{\pi}_{1} = \mathbf{\pi}_0 P\)</li> <li>At step 2, \(\mathbf{\pi}_{2} = \mathbf{\pi}_1 P = (\mathbf{\pi}_0 P) P = \mathbf{\pi}_0 P^2\)</li> <li>At step 3, \(\mathbf{\pi}_{3} = \mathbf{\pi}_2 P = (\mathbf{\pi}_0 P^2) P = \mathbf{\pi}_0 P^3\)</li> <li>…</li> <li>At step \(t\), \(\mathbf{\pi}_{t} = \mathbf{\pi}_0 P^t\)</li> </ul> <p>When taking powers of a matrix, it is useful to use its <a href="https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix" rel="external nofollow noopener" target="_blank">eigendecomposition</a>. After computing the eigenvectors (\(\mathbf{u}_1, \cdots, \mathbf{u}_n\)) and the eigenvalues (\(\lambda_1, \cdots, \lambda_n\)) of \(P\), we first expand \(\mathbf{\pi}_0\) in the eigenbasis:</p> \[\mathbf{\pi}_0 = c_1 \mathbf{u}_1 + c_2 \mathbf{u}_2 + \cdots + c_n \mathbf{u}_n\] <p>Then, for an arbitrary step \(t\):</p> \[\begin{multline*} \mathbf{\pi}_{t} = \mathbf{\pi}_0 P^t \\ = (c_1 \mathbf{u}_1 + \cdots + c_n \mathbf{u}_n) P^t \\ = c_1 \mathbf{u}_1 P^t + \cdots + c_n \mathbf{u}_n P^t \\ = c_1 \lambda^t_1 \mathbf{u}_1 + \cdots + c_n \lambda^t_n \mathbf{u}_n \end{multline*}\] <p>Note that the <a href="/blog/graphs-linear-algebra/#normalized-laplacian-matrices">Laplacian</a> and the transition matrices are deeply related:</p> \[L_{rw} = D^{-1}L = D^{-1}(D - A) = I - P\] <p>In fact, their eigenvectors and eigenvalues are connected. If \(\mathbf{u}\) is an eigenvector of \(P\), with eigenvalue \(\lambda\):</p> \[\mathbf{u} L_{rw} = \mathbf{u} (I - P) = \mathbf{u} - \mathbf{u} P = (1 - \lambda) \mathbf{u}\] <p>That is, \(P\) and \(L_{rw}\) have the same eigenvectors, and the eigenvalues are related as \(\lambda_i(L_{rw}) = 1 - \lambda_i(P)\). Since the <a href="/blog/graphs-linear-algebra/#connectivity-of-the-graph">smallest eigenvalue of \(L_{rw}\) is 0</a>, the largest eigenvalue of \(P\) is \(1\). The corresponding eigenvector \(\pi\) is one in which</p> \[\pi P = \pi.\] <p>Hence</p> \[\pi_i = \sum_j P_{ij} \pi_j = \sum_j \frac {A_ij} {d_j} \pi_j.\] <p>, corresponding to \(\mathbf{u}_1 = \left(\frac 1 n, \cdots, \frac 1 n \right)\). Similarly, the remaining eigenvalues are positive and strictly less than 1. Hence,</p> \[\lim_{t \to \infty} \mathbf{\pi}_{t} = \lim_{t \to \infty} c_1 \lambda^t_1 \mathbf{u}_1 + \cdots + c_n \lambda^t_n \mathbf{u}_n = c_1 \mathbf{u}_1 = \left(\frac 1 n, \cdots, \frac 1 n \right) \blacksquare\] <p>There are several remarks we can do:</p> <ul> <li>This result holds regardless of what the starting vertex is. In fact, \(\pi_0\) could be a probability distribution over the vertices.</li> <li>The <em>speed</em> at which the distribution converges depends on the eigenvalues of \(P\). Specifically, if \(\lambda_2\) is close to 1, the convergence will be slow.</li> </ul> </details> <h1 id="random-walk-with-restart">Random walk with restart</h1> <p>In the <strong>random walk with restart (RWR)</strong>, the walker can return to its root vertex with a restart probability \(r \in [0, 1]\):</p> \[\mathbf{\pi}_{t+1} = r \mathbf{\pi}_0 + (1 - r) \mathbf{\pi}_t P\] <p>where \(\mathbf{\pi}_0\) represents the probability of starting at each vertex. If \(r = 0\), the walker will never be teleported back to the root, and a RW is equivalent to a RWR. If \(r = 1\), the walker will not be allowed to move out of the root, and \(\mathbf{\pi}_t = \mathbf{\pi}_0\). However, for certain values of $r$, the walker is allowed to explore the root’s neighborhood before teleporting back. If the root is part of a <a href="/blog/graphs-glossary/#module">module</a>, the walk will mostly happen within that module. If the root is very central, the walker will explore many parts of the network.</p> <p>Importantly, the RWR has a stationary distribution \(\pi\) which is not necessarily uniform:</p> \[\lim_{t \to \infty} \mathbf{\pi}_{t} = \pi\] <h1 id="markov-chains">Markov chains</h1> <p>A <strong>Markov chain</strong> is a sequence of events in which the probability of each event only depends on the state attained in the previous event. A random walk is a Markov chain: the probability of visiting a vertex only depends on what the neighbors of the current vertex are, and what is the probability of visiting each of them. We can describe some of the properties of a Markov chain by describing the underlying graph:</p> <ul> <li><em>Time reversibility</em></li> <li> <em>Symmetry</em>: a Markov chain is symmetric when the underlying graph is <a href="/blog/graphs-glossary/#regular">regular</a>.</li> </ul> <p>In the context of Markov chains, the transition matrix \(P\) is known as the <strong>right stochastic matrix</strong>.</p> <details><summary>Types of stochastic matrices</summary> <ul> <li> <strong><em>Row/right</em> stochastic matrix</strong>: square matrix with non-negative entries where each row sums to \(1\).</li> <li> <strong><em>Column/left</em> stochastic matrix</strong>: square matrix with non-negative entries where each column sums to \(1\).</li> <li> <strong><em>Doubly</em> stochastic matrix</strong>: square matrix with non-negative entries where each row and column sum to \(1\).</li> </ul> </details> <h1 id="further-reading">Further reading</h1> <ul> <li><a href="https://www.youtube.com/watch?v=8XJes6XFjxM" rel="external nofollow noopener" target="_blank">Full title: The Unreasonable Effectiveness of Spectral Graph Theory: A Confluence of Algorithms, Geometry, and Physics</a></li> </ul> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'hclimente/hclimente.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Héctor Climente-González. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>