<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Covariance and precision | Héctor Climente-González </title> <meta name="author" content="Héctor Climente-González"> <meta name="description" content="Learning the hidden structure of data"> <meta name="keywords" content="machine-learning, genetics, pharma, drug-discovery, climente, climente-gonzalez"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%AA%A2&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hclimente.github.io/blog/precision-matrix/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Héctor</span> Climente-González </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Covariance and precision</h1> <p class="post-meta"> Created on July 16, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/linear-algebra"> <i class="fa-solid fa-hashtag fa-sm"></i> linear_algebra</a>   <a href="/blog/tag/graphs"> <i class="fa-solid fa-hashtag fa-sm"></i> graphs</a>   <a href="/blog/tag/statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> statistics</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Imagine we have a set of 3 variables (\(U\), \(X\), and \(Y\)), with one of them being upstream of the other two (\(X \leftarrow U \rightarrow Y\)):</p> \[U \sim N(0, 1)\] \[X = U + \varepsilon_X\] \[Y = U + \varepsilon_Y\] \[\varepsilon_X, \varepsilon_Y \sim N(0, 0.25)\] <p>We want to discover this structure from observational data. Since both \(X\) and \(Y\) are caused by \(U\), a correlation is not very enlightening and will just return the fully connected graph:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/python/2025-05-29-precision-matrix/img/correlations.webp" sizes="95vw"></source> <img src="/assets/python/2025-05-29-precision-matrix/img/correlations.webp" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Scatter plots of each pair of variables on 200 observations, each with the correlation between the variables and the associated P-value indicated above. </div> <p>A sensible way of going about it is to study the correlation between each pair of variables after adjusting for the remaining variable. If we assume all relationships are linear, these are called <strong>partial correlations</strong>. Partial correlations are designed to reveal direct relationships by removing the influence of confounding variables. Here is a naive implementation:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pcorr_residuals</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Compute the matrix of partial correlations from the residuals of linear regression models.

    Parameters
    ----------
    X : np.ndarray
        The input data matrix.

    Returns
    -------
    np.ndarray
        The matrix of partial correlations.
    </span><span class="sh">"""</span>

    <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span>

    <span class="n">residuals</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">empty</span><span class="p">((</span><span class="n">p</span><span class="p">,</span> <span class="n">p</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
            <span class="n">covariates_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">if</span> <span class="n">k</span> <span class="o">!=</span> <span class="n">i</span> <span class="ow">and</span> <span class="n">k</span> <span class="o">!=</span> <span class="n">j</span><span class="p">]</span>
            <span class="n">X_covars</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">covariates_indices</span><span class="p">]</span>

            <span class="k">for</span> <span class="n">target</span><span class="p">,</span> <span class="n">excluded</span> <span class="ow">in</span> <span class="p">[(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">),</span> <span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">)]:</span>

                <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">target</span><span class="p">]</span>

                <span class="c1"># fit a linear model
</span>                <span class="n">beta</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">lstsq</span><span class="p">(</span><span class="n">X_covars</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">rcond</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
                <span class="n">y_pred</span> <span class="o">=</span> <span class="n">X_covars</span> <span class="o">@</span> <span class="n">beta</span>

                <span class="c1"># compute and center the residuals
</span>                <span class="n">r</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y_pred</span>
                <span class="n">residuals</span><span class="p">[(</span><span class="n">target</span><span class="p">,</span> <span class="n">excluded</span><span class="p">)]</span> <span class="o">=</span> <span class="n">r</span> <span class="o">-</span> <span class="n">r</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span>

    <span class="n">pcorr</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>

            <span class="n">res_1</span> <span class="o">=</span> <span class="n">residuals</span><span class="p">[(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)]</span>
            <span class="n">res_2</span> <span class="o">=</span> <span class="n">residuals</span><span class="p">[(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">)]</span>
            <span class="n">corr</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">res_1</span><span class="p">,</span> <span class="n">res_2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">res_1</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">res_2</span><span class="p">))</span>

            <span class="n">pcorr</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">pcorr</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">corr</span>

    <span class="k">return</span> <span class="n">pcorr</span>
</code></pre></div></div> <p>Partial correlations correctly identify that \(U\) is correlated with both \(X\) and \(Y\), and in turn that those are not correlated once we account for the effect of \(U\):</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/python/2025-05-29-precision-matrix/img/partial_correlations.webp" sizes="95vw"></source> <img src="/assets/python/2025-05-29-precision-matrix/img/partial_correlations.webp" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Scatter plots of the residuals of each pair of variables on 200 observations, each with the <em>partial correlation</em> between the variables and its associated P-value indicated above. </div> <p>Note that while \(\hat{\rho}_{X, U} \approx \rho_{X, U} = 0.8944\), \(\hat{\rho}_{X, U \mid Y} \neq \rho_{X, U}\). This is because \(Y\) contains an additional noise term that makes the adjustment imperfect. Also note that we have identified the <strong>structure</strong> of the data (\(X - U - Y\)), but not its <strong>causal</strong> structure (\(X \rightarrow U \rightarrow Y\), \(X \leftarrow U \leftarrow Y\) or \(X \rightarrow U \rightarrow Y\)).</p> <p>A downside of this approach is its computational complexity. For an \(n \times p\) input matrix:</p> <ul> <li>Memory complexity: \(\mathcal{O}(np^2)\), dominated by storing \({p \choose 2} = \mathcal{O}(p^2)\) residuals, each of length \(n\).</li> <li>Time complexity: \(\mathcal{O}(np^4)\), dominated by computing \({p \choose 2} = \mathcal{O}(p^2)\) least squares problems, each of complexity \(\mathcal{O}(np^2)\).</li> </ul> <p>This is quite computational intensive, which will become a problem in real-world problems. Can we do better? Enter the <strong>precision matrix</strong>, a nice mathematical object to do this at scale.</p> <h1 id="the-precision-matrix">The precision matrix</h1> <details><summary>Need to dust off the basics? Variance, covariance and correlation</summary> <p>The <strong>variance</strong> of a random variable \(X\) is defined as</p> \[\sigma_X^2 = \mathbf{E}(X - \mathbf{E}(X))^2\] <p>The variance takes values in \([0, \infty)\), and measures how disperse the outcomes of the RV are from its mean. Notably, the <strong>(scalar) precision</strong> is defined as \(\frac 1 \sigma_X^2\), so high variance equals low precision and vice versa.</p> <p>The <strong>covariance</strong> between two random variables, \(X_1\) and \(X_2\), is defined as:</p> \[\text{Cov}(X_1, X_2) = \mathbf{E}((X_1 - \mathbf{E}(X_1))(X_2 - \mathbf{E}(X_2))).\] <p>Observe that if \(X_1 = X_2\), \(\sigma_{X_1}^2 = \sigma_{X_2}^2 = \text{Cov}(X_1, X_2)\).</p> <p>The covariance takes values in \((-\sigma_{X_1} \sigma_{X_2}, \sigma_{X_1} \sigma_{X_2})\), and measures the degree to which two random variables are linearly related. The <strong>correlation</strong> \(\rho\) normalizes the covariance, rescaling it to the \([-1, 1]\) range:</p> \[\rho_{X_1, X_2} = \frac {\text{Cov}(X_1, X_2)} {\sigma_{X_1} \sigma_{X_2}}\] </details> <p>The <strong>covariance matrix</strong> of a set of random variables ties the variance and the covariance together. If \(\mathbf{X}\) is a column vector such that</p> \[\mathbf{X} = \begin{pmatrix} X_1 \\ X_2 \\ \vdots \\ X_n \end{pmatrix}\] <p>then its associated covariance matrix \(\mathbf{\Sigma}\) is</p> \[\mathbf{\Sigma} = \begin{pmatrix} \sigma_{X_1}^2 &amp; \text{Cov}(X_1, X_2) &amp; \cdots &amp; \text{Cov}(X_1, X_n) \\ \text{Cov}(X_2, X_1) &amp; \sigma_{X_2}^2 &amp; \cdots &amp; \text{Cov}(X_2, X_n) \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \text{Cov}(X_n, X_1) &amp; \text{Cov}(X_n, X_2) &amp; \cdots &amp; \sigma_{X_n}^2 \end{pmatrix}\] <p>Since always \(\text{Cov}(X_i, X_j) = \text{Cov}(X_j, X_i)\), \(\mathbf{\Sigma}\) is <em>symmetric</em>. It is, in fact, <em>positive semi-definite</em> (<a href="https://statproofbook.github.io/P/covmat-psd.html" rel="external nofollow noopener" target="_blank">proof</a>).</p> <p>By normalizing the covariance matrix by dividing each item \(\mathbf{\Sigma}_{ij}\) by \(\sigma_{X_i} \sigma_{X_j}\), we obtain the <strong>correlation matrix</strong>:</p> \[P = \begin{pmatrix} 1 &amp; \rho_{X_1, X_2} &amp; \cdots &amp; \rho_{X_1, X_n} \\ \rho_{X_2, X_1} &amp; 1 &amp; \cdots &amp; \rho_{X_1, X_n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \rho_{X_n, X_1} &amp; \rho_{X_n, X_2} &amp; \cdots &amp; 1 \end{pmatrix}\] <p>Finally, the <strong>precision matrix</strong> \(\mathbf{\Sigma}^{-1}\) is the inverse of the covariance matrix, i.e., \(\mathbf{\Sigma} \mathbf{\Sigma}^{-1} = \mathbf{I}\). \(\mathbf{\Sigma}\) is not guaranteed to be invertible, and hence \(\mathbf{\Sigma}^{-1}\) may not exist. Let’s ignore this case for now, and jump to where things start getting interesting. \(\mathbf{\Sigma}^{-1}\) can be decomposed as follows:</p> \[\mathbf{\Sigma}^{-1} = D \begin{pmatrix} 1 &amp; -\rho_{X_1, X_2 \mid X_3, \dots, X_n} &amp; \cdots &amp; -\rho_{X_1, X_n \mid X_2, \cdots, X_{n-1}} \\ -\rho_{X_2, X_1 \mid X_3, \cdots, X_n} &amp; 1 &amp; \cdots &amp; -\rho_{X_2, X_n \mid X_1, X_3, \cdots, X_{n-1}} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ -\rho_{X_n, X_1 \mid X_2, \cdots, X_{n-1}} &amp; -\rho_{X_n, X_2 \mid X_1, X_3 \cdots, X_{n-1}} &amp; \cdots &amp; 1 \end{pmatrix} D\] <p>where \(D\) is a normalization matrix:</p> \[D = \begin{pmatrix} \frac 1 {\sigma_{X_1 \mid X_2, \cdots, X_n}} &amp; &amp; &amp; 0 \\ &amp; \frac 1 {\sigma_{X_2 \mid X_1, X_3, \cdots, X_n}} &amp; &amp; \\ &amp; &amp; \ddots &amp; \\ 0 &amp; &amp; &amp; \frac 1 {\sigma_{X_n \mid X_1, \cdots, X_{n-1}}} \end{pmatrix}\] <p>The entries \(\rho_{X_., X_. \mid \dots}\) in the middle matrix are our precious <strong>partial correlations</strong>.</p> <h1 id="estimating-the-precision-matrix">Estimating the precision matrix</h1> <p>Let’s revisit our motivating example equipped with our newfound knowledge: instead of fitting \(\mathcal{O}(p^2)\) linear models, let’s reach the same result using linear algebra. First, we will estimate the covariance matrix using the maximum likelihood approach. Then, we will invert it to obtain the precision matrix.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pcorr_linalg</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">:</span> <span class="c1"># (n, p) -&gt; (p, p)
</span>    <span class="sh">"""</span><span class="s">
    Compute the matrix of partial correlations from the covariance matrix.

    Parameters
    ----------
    X : np.ndarray
        The input data matrix.

    Returns
    -------
    np.ndarray
        The matrix of partial correlations.
    </span><span class="sh">"""</span>

    <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span>

    <span class="c1"># showing how the sausage is made
</span>    <span class="c1"># but could be replaced by covariance = np.cov(X, rowvar=False)
</span>    <span class="n">centered_X</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">X</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">covariance</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">centered_X</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">centered_X</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">precision</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">inv</span><span class="p">(</span><span class="n">covariance</span><span class="p">)</span>

    <span class="n">normalization_factors</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">diag</span><span class="p">(</span><span class="n">precision</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">diag</span><span class="p">(</span><span class="n">precision</span><span class="p">)))</span>
    <span class="n">partial_correlations</span> <span class="o">=</span> <span class="o">-</span> <span class="n">precision</span> <span class="o">/</span> <span class="n">normalization_factors</span>

    <span class="k">return</span> <span class="n">partial_correlations</span>
</code></pre></div></div> <p>This implementation is not only more compact, but has a more favorable computational complexity:</p> <ul> <li>Memory complexity: \(\mathcal{O}(p^2)\), dominated by the intermediate matrices.</li> <li>Time complexity: \(\mathcal{O}(np^2 + p^3)\), dominated by the computation of the covariance matrix and by the matrix inversion.</li> </ul> <p>Furthermore, this implementation is <a href="/blog/python-vectors/">vectorized</a> which further boosts performance. As a quick benchmark, on a random \(1000 \times 100\) matrix, the original <code class="language-plaintext highlighter-rouge">pcorr_residuals</code> took 40.85 seconds; the updated <code class="language-plaintext highlighter-rouge">pcorr_linalg</code> took only 0.0007 seconds.</p> <p>As with many elegant results in linear algebra, things start breaking down when our covariance matrix is <a href="https://en.wikipedia.org/wiki/Condition_number" rel="external nofollow noopener" target="_blank">ill-conditioned</a> or outright <a href="https://en.wikipedia.org/wiki/Singular_matrix" rel="external nofollow noopener" target="_blank">non-invertible</a>. In <a href="https://en.wikipedia.org/wiki/High-dimensional_statistics" rel="external nofollow noopener" target="_blank">high-dimensional problems</a>, \(\Sigma\) is non-invertible (and hard to estimate in the first place). In such cases, we could use <a href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse" rel="external nofollow noopener" target="_blank">the pseudoinverse matrix</a> instead of the inverse. But that’s just a patch: we will get results, but we are outside of the theory and interpreting the results is not as straightforward. However, when the matrix is ill-conditioned, there is a potential path to salvation: <a href="https://scikit-learn.org/stable/modules/covariance.html" rel="external nofollow noopener" target="_blank">regularization</a>.</p> <h1 id="regularized-estimation">Regularized estimation</h1> <p>Adding a regularization step to the covariance matrix estimation will result in a better conditioned matrix. A common approach is <em>shrinking</em> our empirical covariance towards another matrix, the <em>target</em>:</p> \[\hat{\mathbf{\Sigma}} = (1 - \alpha) \hat{\mathbf{\Sigma}}_\text{MLE} + \alpha T\] <p>where \(\alpha \in [0, 1]\) is a parameter and \(T\) is the target matrix, a highly structured matrix that encodes our assumption about what a <em>true</em> covariance matrix should look like. A possible and aggressive target matrix is a diagonal matrix, which encodes the assumption of zero covariance between variables. By upweighting the diagonal elements and downweighting the off-diagonal elements, this matrix will have a better condition than \(\Sigma_\text{MLE}\).</p> <p>The problem becomes then tuning \(\alpha\). A common way to compute the \(\alpha\) is the <a href="https://web.archive.org/web/20141205061842/http://www.econ.uzh.ch/faculty/ledoit/publications/honey.pdf" rel="external nofollow noopener" target="_blank">Ledoit-Wolf shrinkage method</a>, which finds the \(\alpha\) that minimizes the mean squared error between the real and the estimated matrix. Its <a href="https://github.com/scikit-learn/scikit-learn/blob/68483539614102ba8e083277ed7123e6a9fece53/sklearn/covariance/_shrunk_covariance.py#L25" rel="external nofollow noopener" target="_blank">scikit-learn implementation</a> assumes that the target matrix is \(T = \mu I\), where \(\mu\) is the average variance.</p> <p>Alternatively, we can use graphical lasso to estimate a sparse precision matrix. Conceptually, this is a bit easier to swallow: in many situations, most variables being conditionally uncorrelated is a valid assumption. The <a href="https://en.wikipedia.org/wiki/Graphical_lasso" rel="external nofollow noopener" target="_blank">graphical lasso</a> does just that; it is a penalized estimator of the precision matrix.</p> \[\hat{\mathbf{\Sigma}}^{-1} = \operatorname{argmin}_{\mathbf{\Sigma}^{-1} \succ 0} \left(\operatorname{tr}(\mathbf{\Sigma} \mathbf{\Sigma}^{-1}) - \log \det \mathbf{\Sigma}^{-1} - \lambda \|\mathbf{\Sigma}^{-1}\|_1 \right).\] <p>The \(- \lambda \|\mathbf{\Sigma}^{-1}\|_1\) term will favor sparse matrices, with a strength proportional to the magnitude of \(\lambda\). While tuning \(\lambda\) is in itself a challenge, a common approach is using <a href="https://scikit-learn.org/stable/modules/generated/sklearn.covariance.GraphicalLassoCV.html" rel="external nofollow noopener" target="_blank">cross-validation</a>.</p> <p>Let’s bring this point home by looking at a high-dimensional example (20 samples, 20 features).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/python/2025-05-29-precision-matrix/img/high_dimensional_experiments.webp" sizes="95vw"></source> <img src="/assets/python/2025-05-29-precision-matrix/img/high_dimensional_experiments.webp" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Ground truth and estimated covariance matrix, precision matrix and structure of a high-dimensional example. The data generation process involved 20 samples, with 20 features each, sampled from a 0-mean multivariate Normal distribution. The estimated structure using the Ledoit-Wolf used a soft threshold (abs(correlation) &gt; 0.1); otherwise, the fully connected graph would be shown. </div> <p>As we can see, <strong>maximum likelihood estimation</strong> absolutely fails. Due to the extremely ill-conditioned covariance matrix, the precision matrix is completely off scale, with values ranging from -1.8e+15 to 1.0e+15. <strong>Ledoit-Wolf</strong> succeeds at computing a sensible-looking precision matrix. But recovering a structure, e.g., by thresholding it, is quite a hard task. Last, <strong>graphical lasso</strong> is able to find a relatively sparse structure. While it is still far from the ground truth, it prunes away most of the spurious correlations and keeps most of the true links. <a href="https://scikit-learn.org/stable/modules/covariance.html#sparse-inverse-covariance" rel="external nofollow noopener" target="_blank">As expected</a>, most of the true links are larger in absolute value, and further pruning it would return something close to the true structure.</p> <p>More than anything, this little exercise shows how hard this endeavour is, and serves as a good caution to high-dimensional statistics. Beware!</p> <h2 id="conclusions">Conclusions</h2> <p>Under certain assumptions, the precision matrix helps us discover the internal structure of the data. When should we use what to estimate it?</p> <ol> <li> <strong>Empirical inverse (MLE):</strong> fast and exact, but blows up if \(p\) approaches \(n\) or \(\hat Σ\) is singular. Use it when \(n \gg p\) and \(\hat Σ\) is well‑conditioned.</li> <li> <strong>Shrinkage (Ledoit-Wolf):</strong> automatically picks \(\alpha\) to stabilize \(\hat Σ\), yielding a dense but well‑behaved precision. Use it when \(\frac p n\) is moderate.</li> <li> <strong>Graphical Lasso (cross‑validated \(\lambda\)):</strong> trades off likelihood vs. sparsity to prune weak edges and reveal a parsimonious conditional‑independence network. Use it in high‑dimensional settings.</li> </ol> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script defer src="/assets/js/giscus-setup.js"></script> <noscript> Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Héctor Climente-González. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>